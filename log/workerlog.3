[33m[2023-11-10 08:01:33,655] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 08:01:34,732] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I1110 08:01:34.733554 45213 tcp_utils.cc:130] Successfully connected to 172.31.1.102:44250
Traceback (most recent call last):
  File "run_mrc.py", line 243, in <module>
    main()
  File "run_mrc.py", line 35, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/argparser.py", line 223, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 83, in __init__
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/training_args.py", line 942, in __post_init__
    paddle.distributed.init_parallel_env()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/distributed/parallel.py", line 1100, in init_parallel_env
    paddle.distributed.barrier(group=group)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/distributed/communication/group.py", line 328, in barrier
    task = group.process_group.barrier(device_id)
ValueError: (InvalidArgument) TCP receive error. Details: Success.
  [Hint: Expected byte_received > 0, but received byte_received:0 <= 0:0.] (at ../paddle/phi/core/distributed/store/tcp_utils.h:107)

[33m[2023-11-10 08:29:08,558] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
usage: run_mrc.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                  [--config_name CONFIG_NAME]
                  [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                  [--task_name TASK_NAME] [--dataset_name DATASET_NAME]
                  [--dataset_config_name DATASET_CONFIG_NAME]
                  [--overwrite_cache [OVERWRITE_CACHE]]
                  [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]
                  [--max_seq_length MAX_SEQ_LENGTH] [--doc_stride DOC_STRIDE]
                  [--target_size TARGET_SIZE]
                  [--pad_to_max_length [PAD_TO_MAX_LENGTH]]
                  [--no_pad_to_max_length]
                  [--max_train_samples MAX_TRAIN_SAMPLES]
                  [--max_val_samples MAX_VAL_SAMPLES]
                  [--max_test_samples MAX_TEST_SAMPLES]
                  [--label_all_tokens [LABEL_ALL_TOKENS]]
                  [--return_entity_level_metrics [RETURN_ENTITY_LEVEL_METRICS]]
                  [--train_log_file TRAIN_LOG_FILE]
                  [--train_nshard TRAIN_NSHARD]
                  [--use_segment_box [USE_SEGMENT_BOX]]
                  [--task_type TASK_TYPE] [--pattern PATTERN]
                  [--rst_converter RST_CONVERTER] [--lang LANG] --output_dir
                  OUTPUT_DIR [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]
                  [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]
                  [--do_predict [DO_PREDICT]] [--do_export [DO_EXPORT]]
                  [--evaluation_strategy {no,steps,epoch}]
                  [--prediction_loss_only [PREDICTION_LOSS_ONLY]]
                  [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]
                  [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]
                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                  [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]
                  [--learning_rate LEARNING_RATE]
                  [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]
                  [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]
                  [--max_grad_norm MAX_GRAD_NORM]
                  [--num_train_epochs NUM_TRAIN_EPOCHS]
                  [--max_steps MAX_STEPS]
                  [--lr_scheduler_type LR_SCHEDULER_TYPE]
                  [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]
                  [--num_cycles NUM_CYCLES] [--lr_end LR_END] [--power POWER]
                  [--log_on_each_node [LOG_ON_EACH_NODE]]
                  [--no_log_on_each_node] [--logging_dir LOGGING_DIR]
                  [--logging_strategy {no,steps,epoch}]
                  [--logging_first_step [LOGGING_FIRST_STEP]]
                  [--logging_steps LOGGING_STEPS]
                  [--save_strategy {no,steps,epoch}] [--save_steps SAVE_STEPS]
                  [--save_total_limit SAVE_TOTAL_LIMIT]
                  [--save_on_each_node [SAVE_ON_EACH_NODE]]
                  [--no_cuda [NO_CUDA]] [--seed SEED] [--bf16 [BF16]]
                  [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]
                  [--amp_master_grad [AMP_MASTER_GRAD]]
                  [--bf16_full_eval [BF16_FULL_EVAL]]
                  [--fp16_full_eval [FP16_FULL_EVAL]]
                  [--amp_custom_black_list AMP_CUSTOM_BLACK_LIST [AMP_CUSTOM_BLACK_LIST ...]]
                  [--amp_custom_white_list AMP_CUSTOM_WHITE_LIST [AMP_CUSTOM_WHITE_LIST ...]]
                  [--sharding SHARDING] [--sharding_degree SHARDING_DEGREE]
                  [--sharding_parallel_degree SHARDING_PARALLEL_DEGREE]
                  [--save_sharded_model [SAVE_SHARDED_MODEL]]
                  [--load_sharded_model [LOAD_SHARDED_MODEL]]
                  [--tensor_parallel_degree TENSOR_PARALLEL_DEGREE]
                  [--pipeline_parallel_degree PIPELINE_PARALLEL_DEGREE]
                  [--pipeline_parallel_config PIPELINE_PARALLEL_CONFIG]
                  [--sharding_parallel_config SHARDING_PARALLEL_CONFIG]
                  [--hybrid_parallel_topo_order HYBRID_PARALLEL_TOPO_ORDER]
                  [--recompute [RECOMPUTE]] [--scale_loss SCALE_LOSS]
                  [--minimum_eval_times MINIMUM_EVAL_TIMES]
                  [--local_rank LOCAL_RANK]
                  [--dataloader_drop_last [DATALOADER_DROP_LAST]]
                  [--eval_steps EVAL_STEPS]
                  [--max_evaluate_steps MAX_EVALUATE_STEPS]
                  [--dataloader_num_workers DATALOADER_NUM_WORKERS]
                  [--past_index PAST_INDEX] [--run_name RUN_NAME]
                  [--device DEVICE] [--disable_tqdm DISABLE_TQDM]
                  [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]
                  [--no_remove_unused_columns]
                  [--label_names LABEL_NAMES [LABEL_NAMES ...]]
                  [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]
                  [--metric_for_best_model METRIC_FOR_BEST_MODEL]
                  [--greater_is_better GREATER_IS_BETTER]
                  [--ignore_data_skip [IGNORE_DATA_SKIP]] [--optim OPTIM]
                  [--report_to REPORT_TO [REPORT_TO ...]]
                  [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]
                  [--skip_memory_metrics [SKIP_MEMORY_METRICS]]
                  [--no_skip_memory_metrics]
                  [--flatten_param_grads [FLATTEN_PARAM_GRADS]]
                  [--lazy_data_processing [LAZY_DATA_PROCESSING]]
                  [--no_lazy_data_processing]
                  [--skip_profile_timer [SKIP_PROFILE_TIMER]]
                  [--no_skip_profile_timer]
run_mrc.py: error: the following arguments are required: --model_name_or_path, --output_dir
[33m[2023-11-10 08:29:32,379] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 08:29:33,448] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I1110 08:29:33.449519 50894 tcp_utils.cc:107] Retry to connect to 172.31.1.102:63781 while the server is not yet listening.
I1110 08:29:36.449671 50894 tcp_utils.cc:130] Successfully connected to 172.31.1.102:63781
Traceback (most recent call last):
  File "run_mrc.py", line 243, in <module>
    main()
  File "run_mrc.py", line 35, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/argparser.py", line 223, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 83, in __init__
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/training_args.py", line 942, in __post_init__
    paddle.distributed.init_parallel_env()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/distributed/parallel.py", line 1100, in init_parallel_env
    paddle.distributed.barrier(group=group)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/distributed/communication/group.py", line 328, in barrier
    task = group.process_group.barrier(device_id)
ValueError: (InvalidArgument) TCP receive error. Details: Connection refused.
  [Hint: Expected byte_received > 0, but received byte_received:0 <= 0:0.] (at ../paddle/phi/core/distributed/store/tcp_utils.h:107)

[33m[2023-11-10 08:46:57,379] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 08:46:58,447] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I1110 08:46:58.448048 55312 tcp_utils.cc:130] Successfully connected to 172.31.1.102:39256
Traceback (most recent call last):
  File "run_mrc.py", line 243, in <module>
    main()
  File "run_mrc.py", line 35, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/argparser.py", line 223, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 83, in __init__
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/training_args.py", line 942, in __post_init__
    paddle.distributed.init_parallel_env()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/distributed/parallel.py", line 1100, in init_parallel_env
    paddle.distributed.barrier(group=group)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/distributed/communication/group.py", line 328, in barrier
    task = group.process_group.barrier(device_id)
ValueError: (InvalidArgument) TCP receive error. Details: Success.
  [Hint: Expected byte_received > 0, but received byte_received:0 <= 0:0.] (at ../paddle/phi/core/distributed/store/tcp_utils.h:107)

[33m[2023-11-10 08:50:54,027] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 08:50:55,097] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I1110 08:50:55.098239 56866 tcp_utils.cc:130] Successfully connected to 172.31.1.102:54877
W1110 08:51:00.329773 56866 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 08:51:00.336061 56866 gpu_resources.cc:149] device: 3, cuDNN Version: 8.5.
[32m[2023-11-10 08:51:00,948] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 08:51:00,948] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - model_name_or_path            : ernie-layoutx-base-uncased[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - [0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - return_entity_level_metrics   : False[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 08:51:00,951] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 08:51:00,951] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 08:51:00,951] [    INFO][0m - [0m
[32m[2023-11-10 08:51:00,989] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'>, False) to load 'ernie-layoutx-base-uncased'.[0m
[32m[2023-11-10 08:51:00,989] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/vocab.txt[0m
[32m[2023-11-10 08:51:00,989] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/sentencepiece.bpe.model[0m
[32m[2023-11-10 08:51:01,558] [    INFO][0m - tokenizer config file saved in /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/tokenizer_config.json[0m
[32m[2023-11-10 08:51:01,558] [    INFO][0m - Special tokens file saved in /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/special_tokens_map.json[0m
[32m[2023-11-10 08:51:01,559] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.modeling.ErnieLayoutForQuestionAnswering'> to load 'ernie-layoutx-base-uncased'.[0m
[32m[2023-11-10 08:51:01,559] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/model_state.pdparams[0m
[32m[2023-11-10 08:51:01,559] [    INFO][0m - Loading weights file model_state.pdparams from cache at /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/model_state.pdparams[0m
[32m[2023-11-10 08:51:02,861] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-11-10 08:51:04,306] [    INFO][0m - All model checkpoint weights were used when initializing ErnieLayoutForQuestionAnswering.
[0m
[33m[2023-11-10 08:51:04,307] [ WARNING][0m - Some weights of ErnieLayoutForQuestionAnswering were not initialized from the model checkpoint at ernie-layoutx-base-uncased and are newly initialized: ['qa_outputs.bias', 'visual.pixel_std', 'embeddings.position_ids', 'qa_outputs.weight', 'visual.pixel_mean']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-11-10 08:51:04,339] [    INFO][0m - spliting train dataset into 16 shard[0m
[33m[2023-11-10 08:55:09,434] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 08:55:10,501] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I1110 08:55:10.501928 58254 tcp_utils.cc:107] Retry to connect to 172.31.1.102:36578 while the server is not yet listening.
I1110 08:55:13.502127 58254 tcp_utils.cc:130] Successfully connected to 172.31.1.102:36578
W1110 08:55:15.857806 58254 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 08:55:15.864084 58254 gpu_resources.cc:149] device: 3, cuDNN Version: 8.5.
[32m[2023-11-10 08:55:16,801] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
Traceback (most recent call last):
  File "run_mrc.py", line 243, in <module>
    main()
  File "run_mrc.py", line 35, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/argparser.py", line 232, in parse_args_into_dataclasses
    raise ValueError(f"Some specified arguments are not used by the PdArgumentParser: {remaining_args}")
ValueError: Some specified arguments are not used by the PdArgumentParser: ['--devices', '=', '1,2,3']
[33m[2023-11-10 10:31:51,627] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 10:31:52,699] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I1110 10:31:52.700686 172466 tcp_utils.cc:107] Retry to connect to 172.31.1.102:42301 while the server is not yet listening.
I1110 10:31:55.700860 172466 tcp_utils.cc:130] Successfully connected to 172.31.1.102:42301
W1110 10:31:57.393808 172466 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 10:31:57.400020 172466 gpu_resources.cc:149] device: 3, cuDNN Version: 8.5.
[32m[2023-11-10 10:31:58,004] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 10:31:58,004] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - model_name_or_path            : doc15k[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - [0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - return_entity_level_metrics   : False[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 10:31:58,007] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 10:31:58,007] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 10:31:58,007] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 10:31:58,007] [    INFO][0m - [0m
[32m[2023-11-10 10:31:58,199] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'> to load 'doc15k'.[0m
[32m[2023-11-10 10:31:58,771] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.modeling.ErnieLayoutForQuestionAnswering'> to load 'doc15k'.[0m
[32m[2023-11-10 10:31:58,772] [    INFO][0m - Loading configuration file doc15k/config.json[0m
[32m[2023-11-10 10:31:58,773] [    INFO][0m - Loading weights file doc15k/model_state.pdparams[0m
[32m[2023-11-10 10:32:00,080] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-11-10 10:32:01,557] [    INFO][0m - All model checkpoint weights were used when initializing ErnieLayoutForQuestionAnswering.
[0m
[32m[2023-11-10 10:32:01,557] [    INFO][0m - All the weights of ErnieLayoutForQuestionAnswering were initialized from the model checkpoint at doc15k.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieLayoutForQuestionAnswering for predictions without further training.[0m
[32m[2023-11-10 10:32:01,590] [    INFO][0m - spliting train dataset into 16 shard[0m
[32m[2023-11-10 10:32:39,149] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 10:32:39,149] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-11-10 10:32:39,149] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - bf16                          : False[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - data_parallel_rank            : 3[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-11-10 10:32:39,150] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - dataset_rank                  : 3[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - device                        : gpu[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - do_eval                       : True[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - do_export                     : False[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - do_predict                    : False[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - do_train                      : True[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - eval_batch_size               : 6[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - eval_steps                    : 100[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - fp16                          : False[0m
[32m[2023-11-10 10:32:39,151] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - fp16_opt_level                : O1[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - greater_is_better             : True[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - label_names                   : ['start_positions', 'end_positions'][0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - learning_rate                 : 2e-05[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - load_sharded_model            : False[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - log_level                     : -1[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-11-10 10:32:39,152] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - logging_dir                   : ./models/fidelity_save_100/runs/Nov10_10-31-52_ip-172-31-1-102[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - logging_steps                 : 500[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - metric_for_best_model         : anls[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2023-11-10 10:32:39,153] [    INFO][0m - num_train_epochs              : 4.0[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - output_dir                    : ./models/fidelity_save_100[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - overwrite_output_dir          : True[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - past_index                    : -1[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - per_device_eval_batch_size    : 6[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - per_device_train_batch_size   : 6[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - power                         : 1.0[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - process_index                 : 3[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - recompute                     : False[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-11-10 10:32:39,154] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - run_name                      : ./models/fidelity_save_100[0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - save_sharded_model            : False[0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - save_steps                    : 100[0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - save_total_limit              : 1[0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - seed                          : 1000[0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - sharding                      : [][0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-11-10 10:32:39,155] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - should_log                    : False[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - should_save                   : False[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - should_save_model_state       : False[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - train_batch_size              : 6[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-11-10 10:32:39,156] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2023-11-10 10:32:39,157] [    INFO][0m - world_size                    : 4[0m
[32m[2023-11-10 10:32:39,157] [    INFO][0m - [0m
[32m[2023-11-10 10:32:39,157] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions. If tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:32:45,919] [    INFO][0m - ***** Running training *****[0m
[32m[2023-11-10 10:32:45,919] [    INFO][0m -   Num examples = 13,978[0m
[32m[2023-11-10 10:32:45,919] [    INFO][0m -   Num Epochs = 4[0m
[32m[2023-11-10 10:32:45,919] [    INFO][0m -   Instantaneous batch size per device = 6[0m
[32m[2023-11-10 10:32:45,919] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 24[0m
[32m[2023-11-10 10:32:45,919] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-11-10 10:32:45,919] [    INFO][0m -   Total optimization steps = 2,332[0m
[32m[2023-11-10 10:32:45,919] [    INFO][0m -   Total num train samples = 55,912[0m
[32m[2023-11-10 10:32:45,922] [    INFO][0m -   Number of trainable parameters = 281,693,122 (per device)[0m
[32m[2023-11-10 10:35:11,084] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions. If tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:35:11,533] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:35:11,533] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:35:11,533] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:35:11,533] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:35:11,533] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 10:38:09,544] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions. If tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:38:09,994] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:38:09,994] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:38:09,994] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:38:09,994] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:38:09,994] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 10:41:05,164] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions. If tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:41:05,607] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:41:05,607] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:41:05,607] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:41:05,607] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:41:05,607] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 10:44:00,900] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions. If tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:44:01,345] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:44:01,345] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:44:01,345] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:44:01,345] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:44:01,345] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 10:46:56,818] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions. If tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:46:57,264] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:46:57,264] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:46:57,264] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:46:57,264] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:46:57,264] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 10:49:52,557] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions. If tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:49:53,003] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:49:53,003] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:49:53,003] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:49:53,003] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:49:53,003] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 10:52:54,587] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions. If tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:52:55,040] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:52:55,041] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:52:55,041] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:52:55,041] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:52:55,041] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 10:55:50,907] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions. If tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:55:51,355] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:55:51,355] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:55:51,356] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:55:51,356] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:55:51,356] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 10:58:47,030] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions. If tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:58:47,481] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:58:47,481] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:58:47,481] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:58:47,481] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:58:47,481] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 11:01:43,029] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions. If tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:01:43,483] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:01:43,483] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:01:43,483] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:01:43,484] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:01:43,484] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 11:04:38,789] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions. If tokens, token_is_max_context, end_labels, token_to_orig_map, question_id, id, start_labels, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:04:39,242] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:04:39,242] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:04:39,242] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:04:39,242] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:04:39,243] [    INFO][0m -   Total Batch size = 24[0m
[33m[2023-11-10 11:06:33,794] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 11:06:34,860] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I1110 11:06:34.860872 185715 tcp_utils.cc:130] Successfully connected to 172.31.1.102:43154
W1110 11:06:39.237764 185715 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 11:06:39.243865 185715 gpu_resources.cc:149] device: 3, cuDNN Version: 8.5.
[32m[2023-11-10 11:06:39,892] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 11:06:39,893] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:06:39,893] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 11:06:39,893] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - model_name_or_path            : doc15k[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - [0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - return_entity_level_metrics   : False[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - [0m
[32m[2023-11-10 11:06:39,938] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'> to load 'doc15k'.[0m
[32m[2023-11-10 11:06:40,503] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.modeling.ErnieLayoutForQuestionAnswering'> to load 'doc15k'.[0m
[32m[2023-11-10 11:06:40,503] [    INFO][0m - Loading configuration file doc15k/config.json[0m
[32m[2023-11-10 11:06:40,504] [    INFO][0m - Loading weights file doc15k/model_state.pdparams[0m
[32m[2023-11-10 11:06:41,810] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-11-10 11:06:43,270] [    INFO][0m - All model checkpoint weights were used when initializing ErnieLayoutForQuestionAnswering.
[0m
[32m[2023-11-10 11:06:43,270] [    INFO][0m - All the weights of ErnieLayoutForQuestionAnswering were initialized from the model checkpoint at doc15k.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieLayoutForQuestionAnswering for predictions without further training.[0m
[32m[2023-11-10 11:06:43,304] [    INFO][0m - spliting train dataset into 16 shard[0m
[32m[2023-11-10 11:07:20,471] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:07:20,472] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-11-10 11:07:20,472] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:07:20,472] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-11-10 11:07:20,472] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-11-10 11:07:20,472] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-11-10 11:07:20,472] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-11-10 11:07:20,472] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2023-11-10 11:07:20,472] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2023-11-10 11:07:20,472] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-11-10 11:07:20,472] [    INFO][0m - bf16                          : False[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - data_parallel_rank            : 3[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - dataset_rank                  : 3[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - device                        : gpu[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - do_eval                       : True[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - do_export                     : False[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - do_predict                    : False[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - do_train                      : True[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-11-10 11:07:20,473] [    INFO][0m - eval_batch_size               : 6[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - eval_steps                    : 100[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - fp16                          : False[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - fp16_opt_level                : O1[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - greater_is_better             : True[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - label_names                   : ['start_positions', 'end_positions'][0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - learning_rate                 : 2e-05[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2023-11-10 11:07:20,474] [    INFO][0m - load_sharded_model            : False[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - log_level                     : -1[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - logging_dir                   : ./models/fidelity_save_100/runs/Nov10_11-06-34_ip-172-31-1-102[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - logging_steps                 : 500[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - metric_for_best_model         : eval_f1[0m
[32m[2023-11-10 11:07:20,475] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - num_train_epochs              : 4.0[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - output_dir                    : ./models/fidelity_save_100/[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - overwrite_output_dir          : True[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - past_index                    : -1[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - per_device_eval_batch_size    : 6[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - per_device_train_batch_size   : 6[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - power                         : 1.0[0m
[32m[2023-11-10 11:07:20,476] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - process_index                 : 3[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - recompute                     : False[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - run_name                      : ./models/fidelity_save_100/[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - save_sharded_model            : False[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - save_steps                    : 100[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - save_total_limit              : 1[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - seed                          : 1000[0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - sharding                      : [][0m
[32m[2023-11-10 11:07:20,477] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - should_log                    : False[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - should_save                   : False[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - should_save_model_state       : False[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - train_batch_size              : 6[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2023-11-10 11:07:20,478] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-11-10 11:07:20,479] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-11-10 11:07:20,479] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2023-11-10 11:07:20,479] [    INFO][0m - world_size                    : 4[0m
[32m[2023-11-10 11:07:20,479] [    INFO][0m - [0m
[32m[2023-11-10 11:07:20,479] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: start_labels, id, tokens, end_labels, questions, token_to_orig_map, question_id, token_is_max_context. If start_labels, id, tokens, end_labels, questions, token_to_orig_map, question_id, token_is_max_context are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:07:26,794] [    INFO][0m - ***** Running training *****[0m
[32m[2023-11-10 11:07:26,794] [    INFO][0m -   Num examples = 13,978[0m
[32m[2023-11-10 11:07:26,794] [    INFO][0m -   Num Epochs = 4[0m
[32m[2023-11-10 11:07:26,794] [    INFO][0m -   Instantaneous batch size per device = 6[0m
[32m[2023-11-10 11:07:26,794] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 24[0m
[32m[2023-11-10 11:07:26,795] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-11-10 11:07:26,795] [    INFO][0m -   Total optimization steps = 2,332[0m
[32m[2023-11-10 11:07:26,795] [    INFO][0m -   Total num train samples = 55,912[0m
[32m[2023-11-10 11:07:26,797] [    INFO][0m -   Number of trainable parameters = 281,693,122 (per device)[0m
[32m[2023-11-10 11:09:52,405] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: start_labels, id, tokens, end_labels, questions, token_to_orig_map, question_id, token_is_max_context. If start_labels, id, tokens, end_labels, questions, token_to_orig_map, question_id, token_is_max_context are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:09:52,863] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:09:52,863] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:09:52,863] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:09:52,863] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:09:52,863] [    INFO][0m -   Total Batch size = 24[0m
Traceback (most recent call last):
  File "run_mrc.py", line 243, in <module>
    main()
  File "run_mrc.py", line 211, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/trainer.py", line 883, in train
    self._maybe_log_save_evaluate(tr_loss, model, epoch, ignore_keys_for_eval, inputs=inputs)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/trainer.py", line 1060, in _maybe_log_save_evaluate
    self._save_checkpoint(model, metrics=metrics)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/trainer.py", line 1800, in _save_checkpoint
    metric_value = metrics[metric_to_check]
KeyError: 'eval_f1'
[33m[2023-11-10 11:11:33,554] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 11:11:34,613] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I1110 11:11:34.614645 187367 tcp_utils.cc:107] Retry to connect to 172.31.1.102:63714 while the server is not yet listening.
I1110 11:11:37.614944 187367 tcp_utils.cc:130] Successfully connected to 172.31.1.102:63714
W1110 11:11:39.017671 187367 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 11:11:39.023782 187367 gpu_resources.cc:149] device: 3, cuDNN Version: 8.5.
[32m[2023-11-10 11:11:39,619] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 11:11:39,620] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:11:39,620] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 11:11:39,620] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:11:39,620] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 11:11:39,620] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 11:11:39,620] [    INFO][0m - model_name_or_path            : doc15k[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - [0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - return_entity_level_metrics   : True[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - [0m
[32m[2023-11-10 11:11:39,664] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'> to load 'doc15k'.[0m
[32m[2023-11-10 11:11:40,233] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.modeling.ErnieLayoutForQuestionAnswering'> to load 'doc15k'.[0m
[32m[2023-11-10 11:11:40,233] [    INFO][0m - Loading configuration file doc15k/config.json[0m
[32m[2023-11-10 11:11:40,234] [    INFO][0m - Loading weights file doc15k/model_state.pdparams[0m
[32m[2023-11-10 11:11:41,530] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-11-10 11:11:42,983] [    INFO][0m - All model checkpoint weights were used when initializing ErnieLayoutForQuestionAnswering.
[0m
[32m[2023-11-10 11:11:42,984] [    INFO][0m - All the weights of ErnieLayoutForQuestionAnswering were initialized from the model checkpoint at doc15k.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieLayoutForQuestionAnswering for predictions without further training.[0m
[32m[2023-11-10 11:11:43,018] [    INFO][0m - spliting train dataset into 16 shard[0m
[32m[2023-11-10 11:12:20,010] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:12:20,010] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-11-10 11:12:20,010] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:12:20,010] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-11-10 11:12:20,010] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - bf16                          : False[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - data_parallel_rank            : 3[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - dataset_rank                  : 3[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2023-11-10 11:12:20,011] [    INFO][0m - device                        : gpu[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - do_eval                       : True[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - do_export                     : False[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - do_predict                    : False[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - do_train                      : True[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - eval_batch_size               : 6[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - eval_steps                    : 100[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - fp16                          : False[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - fp16_opt_level                : O1[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-11-10 11:12:20,012] [    INFO][0m - greater_is_better             : True[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - label_names                   : ['start_positions', 'end_positions'][0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - learning_rate                 : 2e-05[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - load_sharded_model            : False[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - log_level                     : -1[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - logging_dir                   : ./models/fidelity_save_100/runs/Nov10_11-11-34_ip-172-31-1-102[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-11-10 11:12:20,013] [    INFO][0m - logging_steps                 : 500[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - metric_for_best_model         : anls[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - num_train_epochs              : 4.0[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - output_dir                    : ./models/fidelity_save_100/[0m
[32m[2023-11-10 11:12:20,014] [    INFO][0m - overwrite_output_dir          : True[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - past_index                    : -1[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - per_device_eval_batch_size    : 6[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - per_device_train_batch_size   : 6[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - power                         : 1.0[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - process_index                 : 3[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - recompute                     : False[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - run_name                      : ./models/fidelity_save_100/[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-11-10 11:12:20,015] [    INFO][0m - save_sharded_model            : False[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - save_steps                    : 100[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - save_total_limit              : 1[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - seed                          : 1000[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - sharding                      : [][0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - should_log                    : False[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - should_save                   : False[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - should_save_model_state       : False[0m
[32m[2023-11-10 11:12:20,016] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2023-11-10 11:12:20,017] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-11-10 11:12:20,017] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2023-11-10 11:12:20,017] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-11-10 11:12:20,017] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-11-10 11:12:20,017] [    INFO][0m - train_batch_size              : 6[0m
[32m[2023-11-10 11:12:20,017] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2023-11-10 11:12:20,017] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2023-11-10 11:12:20,017] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-11-10 11:12:20,017] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-11-10 11:12:20,017] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2023-11-10 11:12:20,017] [    INFO][0m - world_size                    : 4[0m
[32m[2023-11-10 11:12:20,017] [    INFO][0m - [0m
[32m[2023-11-10 11:12:20,018] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: start_labels, token_is_max_context, end_labels, id, tokens, question_id, token_to_orig_map, questions. If start_labels, token_is_max_context, end_labels, id, tokens, question_id, token_to_orig_map, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:12:26,230] [    INFO][0m - ***** Running training *****[0m
[32m[2023-11-10 11:12:26,231] [    INFO][0m -   Num examples = 13,978[0m
[32m[2023-11-10 11:12:26,231] [    INFO][0m -   Num Epochs = 4[0m
[32m[2023-11-10 11:12:26,231] [    INFO][0m -   Instantaneous batch size per device = 6[0m
[32m[2023-11-10 11:12:26,231] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 24[0m
[32m[2023-11-10 11:12:26,231] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-11-10 11:12:26,231] [    INFO][0m -   Total optimization steps = 2,332[0m
[32m[2023-11-10 11:12:26,231] [    INFO][0m -   Total num train samples = 55,912[0m
[32m[2023-11-10 11:12:26,233] [    INFO][0m -   Number of trainable parameters = 281,693,122 (per device)[0m
[32m[2023-11-10 11:14:51,296] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: start_labels, token_is_max_context, end_labels, id, tokens, question_id, token_to_orig_map, questions. If start_labels, token_is_max_context, end_labels, id, tokens, question_id, token_to_orig_map, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:14:51,746] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:14:51,746] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:14:51,747] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:14:51,747] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:14:51,747] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 11:17:53,351] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: start_labels, token_is_max_context, end_labels, id, tokens, question_id, token_to_orig_map, questions. If start_labels, token_is_max_context, end_labels, id, tokens, question_id, token_to_orig_map, questions are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:17:53,806] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:17:53,806] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:17:53,806] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:17:53,806] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:17:53,806] [    INFO][0m -   Total Batch size = 24[0m
[33m[2023-11-10 11:18:58,889] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 11:18:59,958] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I1110 11:18:59.958822 190093 tcp_utils.cc:130] Successfully connected to 172.31.1.102:48747
W1110 11:19:04.273748 190093 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 11:19:04.279934 190093 gpu_resources.cc:149] device: 3, cuDNN Version: 8.5.
[32m[2023-11-10 11:19:04,916] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 11:19:04,917] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:19:04,917] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 11:19:04,917] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:19:04,917] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - model_name_or_path            : ernie-layoutx-base-uncased[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - [0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - return_entity_level_metrics   : True[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - [0m
[32m[2023-11-10 11:19:05,110] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'>, False) to load 'ernie-layoutx-base-uncased'.[0m
[32m[2023-11-10 11:19:05,110] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/vocab.txt[0m
[32m[2023-11-10 11:19:05,111] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/sentencepiece.bpe.model[0m
[32m[2023-11-10 11:19:05,677] [    INFO][0m - tokenizer config file saved in /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/tokenizer_config.json[0m
[32m[2023-11-10 11:19:05,678] [    INFO][0m - Special tokens file saved in /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/special_tokens_map.json[0m
[32m[2023-11-10 11:19:05,678] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.modeling.ErnieLayoutForQuestionAnswering'> to load 'ernie-layoutx-base-uncased'.[0m
[32m[2023-11-10 11:19:05,679] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/model_state.pdparams[0m
[32m[2023-11-10 11:19:05,679] [    INFO][0m - Loading weights file model_state.pdparams from cache at /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/model_state.pdparams[0m
[32m[2023-11-10 11:19:06,971] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-11-10 11:19:08,425] [    INFO][0m - All model checkpoint weights were used when initializing ErnieLayoutForQuestionAnswering.
[0m
[33m[2023-11-10 11:19:08,425] [ WARNING][0m - Some weights of ErnieLayoutForQuestionAnswering were not initialized from the model checkpoint at ernie-layoutx-base-uncased and are newly initialized: ['embeddings.position_ids', 'qa_outputs.weight', 'visual.pixel_std', 'qa_outputs.bias', 'visual.pixel_mean']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-11-10 11:19:08,459] [    INFO][0m - spliting train dataset into 16 shard[0m
[32m[2023-11-10 11:19:45,237] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:19:45,237] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-11-10 11:19:45,238] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:19:45,238] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-11-10 11:19:45,238] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-11-10 11:19:45,238] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-11-10 11:19:45,238] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-11-10 11:19:45,238] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2023-11-10 11:19:45,238] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2023-11-10 11:19:45,238] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-11-10 11:19:45,238] [    INFO][0m - bf16                          : False[0m
[32m[2023-11-10 11:19:45,238] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-11-10 11:19:45,238] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-11-10 11:19:45,238] [    INFO][0m - data_parallel_rank            : 3[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - dataset_rank                  : 3[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - device                        : gpu[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - do_eval                       : True[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - do_export                     : False[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - do_predict                    : False[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - do_train                      : True[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - eval_batch_size               : 6[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - eval_steps                    : 100[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:19:45,239] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - fp16                          : False[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - fp16_opt_level                : O1[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - greater_is_better             : True[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - label_names                   : ['start_positions', 'end_positions'][0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - learning_rate                 : 2e-05[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - load_sharded_model            : False[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-11-10 11:19:45,240] [    INFO][0m - log_level                     : -1[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - logging_dir                   : ./models/fidelity_save_100/runs/Nov10_11-18-59_ip-172-31-1-102[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - logging_steps                 : 500[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - metric_for_best_model         : anls[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-11-10 11:19:45,241] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - num_train_epochs              : 4.0[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - output_dir                    : ./models/fidelity_save_100/[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - overwrite_output_dir          : True[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - past_index                    : -1[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - per_device_eval_batch_size    : 6[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - per_device_train_batch_size   : 6[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - power                         : 1.0[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - process_index                 : 3[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - recompute                     : False[0m
[32m[2023-11-10 11:19:45,242] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - run_name                      : ./models/fidelity_save_100/[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - save_sharded_model            : False[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - save_steps                    : 100[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - save_total_limit              : 1[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - seed                          : 1000[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - sharding                      : [][0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-11-10 11:19:45,243] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - should_log                    : False[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - should_save                   : False[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - should_save_model_state       : False[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - train_batch_size              : 6[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - weight_decay                  : 0.05[0m
[32m[2023-11-10 11:19:45,244] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2023-11-10 11:19:45,245] [    INFO][0m - world_size                    : 4[0m
[32m[2023-11-10 11:19:45,245] [    INFO][0m - [0m
[32m[2023-11-10 11:19:45,245] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id. If end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:19:51,424] [    INFO][0m - ***** Running training *****[0m
[32m[2023-11-10 11:19:51,424] [    INFO][0m -   Num examples = 13,978[0m
[32m[2023-11-10 11:19:51,425] [    INFO][0m -   Num Epochs = 4[0m
[32m[2023-11-10 11:19:51,425] [    INFO][0m -   Instantaneous batch size per device = 6[0m
[32m[2023-11-10 11:19:51,425] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 24[0m
[32m[2023-11-10 11:19:51,425] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-11-10 11:19:51,425] [    INFO][0m -   Total optimization steps = 2,332[0m
[32m[2023-11-10 11:19:51,425] [    INFO][0m -   Total num train samples = 55,912[0m
[32m[2023-11-10 11:19:51,427] [    INFO][0m -   Number of trainable parameters = 281,693,122 (per device)[0m
[32m[2023-11-10 11:22:19,702] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id. If end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:22:20,150] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:22:20,151] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:22:20,151] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:22:20,151] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:22:20,151] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 11:25:33,209] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id. If end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:25:33,659] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:25:33,659] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:25:33,659] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:25:33,659] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:25:33,659] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 11:28:43,934] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id. If end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:28:44,414] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:28:44,414] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:28:44,414] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:28:44,414] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:28:44,414] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 11:31:43,866] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id. If end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:31:44,306] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:31:44,307] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:31:44,307] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:31:44,307] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:31:44,307] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 11:34:44,236] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id. If end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:34:44,677] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:34:44,677] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:34:44,677] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:34:44,677] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:34:44,677] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 11:37:43,820] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id. If end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:37:44,260] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:37:44,261] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:37:44,261] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:37:44,261] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:37:44,261] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 11:40:43,887] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id. If end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:40:44,336] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:40:44,336] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:40:44,336] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:40:44,336] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:40:44,336] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 11:43:43,919] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id. If end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:43:44,360] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:43:44,360] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:43:44,360] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:43:44,360] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:43:44,360] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 11:46:43,783] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id. If end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:46:44,225] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:46:44,225] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:46:44,225] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:46:44,225] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:46:44,225] [    INFO][0m -   Total Batch size = 24[0m
[32m[2023-11-10 11:49:43,906] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id. If end_labels, questions, token_is_max_context, id, start_labels, token_to_orig_map, tokens, question_id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:49:44,357] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:49:44,357] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:49:44,357] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:49:44,357] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:49:44,357] [    INFO][0m -   Total Batch size = 24[0m
