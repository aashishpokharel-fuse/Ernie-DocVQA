[33m[2023-11-10 08:01:33,615] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 08:01:34,684] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')
=======================================================================
I1110 08:01:34.685040 45207 tcp_utils.cc:181] The server starts to listen on IP_ANY:44250
I1110 08:01:34.685182 45207 tcp_utils.cc:130] Successfully connected to 172.31.1.102:44250
W1110 08:01:37.110781 45207 dynamic_loader.cc:274] You may need to install 'nccl2' from NVIDIA official website: https://developer.nvidia.com/nccl/nccl-download before install PaddlePaddle.
Traceback (most recent call last):
  File "run_mrc.py", line 243, in <module>
    main()
  File "run_mrc.py", line 35, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/argparser.py", line 223, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 83, in __init__
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/training_args.py", line 942, in __post_init__
    paddle.distributed.init_parallel_env()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/distributed/parallel.py", line 1100, in init_parallel_env
    paddle.distributed.barrier(group=group)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/distributed/communication/group.py", line 328, in barrier
    task = group.process_group.barrier(device_id)
RuntimeError: (PreconditionNotMet) The third-party dynamic library (libnccl.so) that Paddle depends on is not configured correctly. (error code is libnccl.so: cannot open shared object file: No such file or directory)
  Suggestions:
  1. Check if the third-party dynamic library (e.g. CUDA, CUDNN) is installed correctly and its version is matched with paddlepaddle you installed.
  2. Configure third-party dynamic library environment variables as follows:
  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`
  - Windows: set PATH by `set PATH=XXX; (at ../paddle/phi/backends/dynload/dynamic_loader.cc:301)

I1110 08:01:37.435070 45402 tcp_store.cc:273] receive shutdown event and so quit from MasterDaemon run loop
[33m[2023-11-10 08:29:08,560] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
usage: run_mrc.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                  [--config_name CONFIG_NAME]
                  [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                  [--task_name TASK_NAME] [--dataset_name DATASET_NAME]
                  [--dataset_config_name DATASET_CONFIG_NAME]
                  [--overwrite_cache [OVERWRITE_CACHE]]
                  [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]
                  [--max_seq_length MAX_SEQ_LENGTH] [--doc_stride DOC_STRIDE]
                  [--target_size TARGET_SIZE]
                  [--pad_to_max_length [PAD_TO_MAX_LENGTH]]
                  [--no_pad_to_max_length]
                  [--max_train_samples MAX_TRAIN_SAMPLES]
                  [--max_val_samples MAX_VAL_SAMPLES]
                  [--max_test_samples MAX_TEST_SAMPLES]
                  [--label_all_tokens [LABEL_ALL_TOKENS]]
                  [--return_entity_level_metrics [RETURN_ENTITY_LEVEL_METRICS]]
                  [--train_log_file TRAIN_LOG_FILE]
                  [--train_nshard TRAIN_NSHARD]
                  [--use_segment_box [USE_SEGMENT_BOX]]
                  [--task_type TASK_TYPE] [--pattern PATTERN]
                  [--rst_converter RST_CONVERTER] [--lang LANG] --output_dir
                  OUTPUT_DIR [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]
                  [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]
                  [--do_predict [DO_PREDICT]] [--do_export [DO_EXPORT]]
                  [--evaluation_strategy {no,steps,epoch}]
                  [--prediction_loss_only [PREDICTION_LOSS_ONLY]]
                  [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]
                  [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]
                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                  [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]
                  [--learning_rate LEARNING_RATE]
                  [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]
                  [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]
                  [--max_grad_norm MAX_GRAD_NORM]
                  [--num_train_epochs NUM_TRAIN_EPOCHS]
                  [--max_steps MAX_STEPS]
                  [--lr_scheduler_type LR_SCHEDULER_TYPE]
                  [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]
                  [--num_cycles NUM_CYCLES] [--lr_end LR_END] [--power POWER]
                  [--log_on_each_node [LOG_ON_EACH_NODE]]
                  [--no_log_on_each_node] [--logging_dir LOGGING_DIR]
                  [--logging_strategy {no,steps,epoch}]
                  [--logging_first_step [LOGGING_FIRST_STEP]]
                  [--logging_steps LOGGING_STEPS]
                  [--save_strategy {no,steps,epoch}] [--save_steps SAVE_STEPS]
                  [--save_total_limit SAVE_TOTAL_LIMIT]
                  [--save_on_each_node [SAVE_ON_EACH_NODE]]
                  [--no_cuda [NO_CUDA]] [--seed SEED] [--bf16 [BF16]]
                  [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]
                  [--amp_master_grad [AMP_MASTER_GRAD]]
                  [--bf16_full_eval [BF16_FULL_EVAL]]
                  [--fp16_full_eval [FP16_FULL_EVAL]]
                  [--amp_custom_black_list AMP_CUSTOM_BLACK_LIST [AMP_CUSTOM_BLACK_LIST ...]]
                  [--amp_custom_white_list AMP_CUSTOM_WHITE_LIST [AMP_CUSTOM_WHITE_LIST ...]]
                  [--sharding SHARDING] [--sharding_degree SHARDING_DEGREE]
                  [--sharding_parallel_degree SHARDING_PARALLEL_DEGREE]
                  [--save_sharded_model [SAVE_SHARDED_MODEL]]
                  [--load_sharded_model [LOAD_SHARDED_MODEL]]
                  [--tensor_parallel_degree TENSOR_PARALLEL_DEGREE]
                  [--pipeline_parallel_degree PIPELINE_PARALLEL_DEGREE]
                  [--pipeline_parallel_config PIPELINE_PARALLEL_CONFIG]
                  [--sharding_parallel_config SHARDING_PARALLEL_CONFIG]
                  [--hybrid_parallel_topo_order HYBRID_PARALLEL_TOPO_ORDER]
                  [--recompute [RECOMPUTE]] [--scale_loss SCALE_LOSS]
                  [--minimum_eval_times MINIMUM_EVAL_TIMES]
                  [--local_rank LOCAL_RANK]
                  [--dataloader_drop_last [DATALOADER_DROP_LAST]]
                  [--eval_steps EVAL_STEPS]
                  [--max_evaluate_steps MAX_EVALUATE_STEPS]
                  [--dataloader_num_workers DATALOADER_NUM_WORKERS]
                  [--past_index PAST_INDEX] [--run_name RUN_NAME]
                  [--device DEVICE] [--disable_tqdm DISABLE_TQDM]
                  [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]
                  [--no_remove_unused_columns]
                  [--label_names LABEL_NAMES [LABEL_NAMES ...]]
                  [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]
                  [--metric_for_best_model METRIC_FOR_BEST_MODEL]
                  [--greater_is_better GREATER_IS_BETTER]
                  [--ignore_data_skip [IGNORE_DATA_SKIP]] [--optim OPTIM]
                  [--report_to REPORT_TO [REPORT_TO ...]]
                  [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]
                  [--skip_memory_metrics [SKIP_MEMORY_METRICS]]
                  [--no_skip_memory_metrics]
                  [--flatten_param_grads [FLATTEN_PARAM_GRADS]]
                  [--lazy_data_processing [LAZY_DATA_PROCESSING]]
                  [--no_lazy_data_processing]
                  [--skip_profile_timer [SKIP_PROFILE_TIMER]]
                  [--no_skip_profile_timer]
run_mrc.py: error: the following arguments are required: --model_name_or_path, --output_dir
[33m[2023-11-10 08:29:32,394] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 08:29:33,471] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')
=======================================================================
I1110 08:29:33.472396 50888 tcp_utils.cc:181] The server starts to listen on IP_ANY:63781
I1110 08:29:33.472560 50888 tcp_utils.cc:130] Successfully connected to 172.31.1.102:63781
W1110 08:29:38.801831 50888 dynamic_loader.cc:274] You may need to install 'nccl2' from NVIDIA official website: https://developer.nvidia.com/nccl/nccl-download before install PaddlePaddle.
Traceback (most recent call last):
  File "run_mrc.py", line 243, in <module>
    main()
  File "run_mrc.py", line 35, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/argparser.py", line 223, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 83, in __init__
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/training_args.py", line 942, in __post_init__
    paddle.distributed.init_parallel_env()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/distributed/parallel.py", line 1100, in init_parallel_env
    paddle.distributed.barrier(group=group)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/distributed/communication/group.py", line 328, in barrier
    task = group.process_group.barrier(device_id)
RuntimeError: (PreconditionNotMet) The third-party dynamic library (libnccl.so) that Paddle depends on is not configured correctly. (error code is libnccl.so: cannot open shared object file: No such file or directory)
  Suggestions:
  1. Check if the third-party dynamic library (e.g. CUDA, CUDNN) is installed correctly and its version is matched with paddlepaddle you installed.
  2. Configure third-party dynamic library environment variables as follows:
  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`
  - Windows: set PATH by `set PATH=XXX; (at ../paddle/phi/backends/dynload/dynamic_loader.cc:301)

I1110 08:29:39.118430 51059 tcp_store.cc:273] receive shutdown event and so quit from MasterDaemon run loop
[33m[2023-11-10 08:46:57,321] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 08:46:58,393] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')
=======================================================================
I1110 08:46:58.393966 55306 tcp_utils.cc:181] The server starts to listen on IP_ANY:39256
I1110 08:46:58.394124 55306 tcp_utils.cc:130] Successfully connected to 172.31.1.102:39256
W1110 08:47:00.859284 55306 dynamic_loader.cc:274] You may need to install 'nccl2' from NVIDIA official website: https://developer.nvidia.com/nccl/nccl-download before install PaddlePaddle.
Traceback (most recent call last):
  File "run_mrc.py", line 243, in <module>
    main()
  File "run_mrc.py", line 35, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/argparser.py", line 223, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 83, in __init__
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/training_args.py", line 942, in __post_init__
    paddle.distributed.init_parallel_env()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/distributed/parallel.py", line 1100, in init_parallel_env
    paddle.distributed.barrier(group=group)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/distributed/communication/group.py", line 328, in barrier
    task = group.process_group.barrier(device_id)
RuntimeError: (PreconditionNotMet) The third-party dynamic library (libnccl.so) that Paddle depends on is not configured correctly. (error code is libnccl.so: cannot open shared object file: No such file or directory)
  Suggestions:
  1. Check if the third-party dynamic library (e.g. CUDA, CUDNN) is installed correctly and its version is matched with paddlepaddle you installed.
  2. Configure third-party dynamic library environment variables as follows:
  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`
  - Windows: set PATH by `set PATH=XXX; (at ../paddle/phi/backends/dynload/dynamic_loader.cc:301)

I1110 08:47:01.173666 55489 tcp_store.cc:273] receive shutdown event and so quit from MasterDaemon run loop
[33m[2023-11-10 08:50:54,013] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 08:50:55,094] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')
=======================================================================
I1110 08:50:55.095363 56860 tcp_utils.cc:181] The server starts to listen on IP_ANY:54877
I1110 08:50:55.095495 56860 tcp_utils.cc:130] Successfully connected to 172.31.1.102:54877
W1110 08:51:00.281368 56860 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 08:51:00.287510 56860 gpu_resources.cc:149] device: 0, cuDNN Version: 8.5.
[32m[2023-11-10 08:51:00,948] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 08:51:00,948] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - model_name_or_path            : ernie-layoutx-base-uncased[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - [0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 08:51:00,949] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - return_entity_level_metrics   : False[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 08:51:00,950] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 08:51:00,951] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 08:51:00,951] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 08:51:00,951] [    INFO][0m - [0m
[32m[2023-11-10 08:51:00,994] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'>, False) to load 'ernie-layoutx-base-uncased'.[0m
[32m[2023-11-10 08:51:00,994] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/vocab.txt[0m
[32m[2023-11-10 08:51:00,994] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/sentencepiece.bpe.model[0m
[32m[2023-11-10 08:51:01,571] [    INFO][0m - tokenizer config file saved in /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/tokenizer_config.json[0m
[32m[2023-11-10 08:51:01,571] [    INFO][0m - Special tokens file saved in /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/special_tokens_map.json[0m
[32m[2023-11-10 08:51:01,572] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.modeling.ErnieLayoutForQuestionAnswering'> to load 'ernie-layoutx-base-uncased'.[0m
[32m[2023-11-10 08:51:01,572] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/model_state.pdparams[0m
[32m[2023-11-10 08:51:01,573] [    INFO][0m - Loading weights file model_state.pdparams from cache at /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/model_state.pdparams[0m
[32m[2023-11-10 08:51:02,882] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
Traceback (most recent call last):
  File "run_mrc.py", line 243, in <module>
    main()
  File "run_mrc.py", line 73, in main
    model = AutoModelForQuestionAnswering.from_pretrained(model_args.model_name_or_path, num_classes=num_labels)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/transformers/auto/modeling.py", line 678, in from_pretrained
    return cls._from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/transformers/auto/modeling.py", line 328, in _from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/transformers/model_utils.py", line 1979, in from_pretrained
    model = cls(config, *init_args, **model_kwargs)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/transformers/utils.py", line 249, in __impl__
    init_func(self, *args, **kwargs)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/transformers/ernie_layout/modeling.py", line 1088, in __init__
    self.ernie_layout = ErnieLayoutModel(config)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/transformers/utils.py", line 249, in __impl__
    init_func(self, *args, **kwargs)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/transformers/ernie_layout/modeling.py", line 619, in __init__
    self.embeddings = ErnieLayoutEmbeddings(config)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/transformers/ernie_layout/modeling.py", line 117, in __init__
    self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/nn/layer/common.py", line 1503, in __init__
    self.weight = self.create_parameter(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 715, in create_parameter
    return self._helper.create_parameter(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/fluid/layer_helper_base.py", line 424, in create_parameter
    return self.main_program.global_block().create_parameter(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/fluid/framework.py", line 3935, in create_parameter
    initializer(param, self)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/nn/initializer/initializer.py", line 40, in __call__
    return self.forward(param, block)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/paddle/nn/initializer/xavier.py", line 123, in forward
    out_var = _C_ops.uniform(
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   uniform_ad_func(paddle::experimental::IntArrayBase<paddle::Tensor>, phi::DataType, paddle::experimental::ScalarBase<paddle::Tensor>, paddle::experimental::ScalarBase<paddle::Tensor>, int, phi::Place)
1   paddle::experimental::uniform(paddle::experimental::IntArrayBase<paddle::Tensor> const&, phi::DataType, paddle::experimental::ScalarBase<paddle::Tensor> const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, int, phi::Place const&)
2   void phi::UniformKernel<float, phi::GPUContext>(phi::GPUContext const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DataType, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 732.427734MB memory on GPU 0, 14.383789GB memory has been allocated and available memory is only 200.562500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/fluid/memory/allocation/cuda_allocator.cc:86)

I1110 08:51:13.747802 57055 tcp_store.cc:273] receive shutdown event and so quit from MasterDaemon run loop
[33m[2023-11-10 08:55:09,437] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 08:55:10,503] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')
=======================================================================
I1110 08:55:10.504252 58248 tcp_utils.cc:181] The server starts to listen on IP_ANY:36578
I1110 08:55:10.504395 58248 tcp_utils.cc:130] Successfully connected to 172.31.1.102:36578
W1110 08:55:15.811618 58248 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 08:55:15.817720 58248 gpu_resources.cc:149] device: 0, cuDNN Version: 8.5.
[32m[2023-11-10 08:55:16,801] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
Traceback (most recent call last):
  File "run_mrc.py", line 243, in <module>
    main()
  File "run_mrc.py", line 35, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/trainer/argparser.py", line 232, in parse_args_into_dataclasses
    raise ValueError(f"Some specified arguments are not used by the PdArgumentParser: {remaining_args}")
ValueError: Some specified arguments are not used by the PdArgumentParser: ['--devices', '=', '1,2,3']
I1110 08:55:17.273772 58434 tcp_store.cc:273] receive shutdown event and so quit from MasterDaemon run loop
[33m[2023-11-10 08:55:40,049] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 08:55:41,097] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I1110 08:55:41.098641 58687 tcp_utils.cc:181] The server starts to listen on IP_ANY:53292
I1110 08:55:41.098779 58687 tcp_utils.cc:130] Successfully connected to 172.31.1.102:53292
W1110 08:55:42.656548 58687 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 08:55:42.662580 58687 gpu_resources.cc:149] device: 1, cuDNN Version: 8.5.
[32m[2023-11-10 08:55:43,320] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 08:55:43,320] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 08:55:43,320] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m - model_name_or_path            : ernie-layoutx-base-uncased[0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m - [0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 08:55:43,321] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - return_entity_level_metrics   : False[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 08:55:43,322] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 08:55:43,323] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 08:55:43,323] [    INFO][0m - [0m
[32m[2023-11-10 08:55:43,362] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'>, False) to load 'ernie-layoutx-base-uncased'.[0m
[32m[2023-11-10 08:55:43,362] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/vocab.txt[0m
[32m[2023-11-10 08:55:43,362] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/sentencepiece.bpe.model[0m
[32m[2023-11-10 08:55:43,916] [    INFO][0m - tokenizer config file saved in /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/tokenizer_config.json[0m
[32m[2023-11-10 08:55:43,916] [    INFO][0m - Special tokens file saved in /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/special_tokens_map.json[0m
[32m[2023-11-10 08:55:43,916] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.modeling.ErnieLayoutForQuestionAnswering'> to load 'ernie-layoutx-base-uncased'.[0m
[32m[2023-11-10 08:55:43,917] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/model_state.pdparams[0m
[32m[2023-11-10 08:55:43,917] [    INFO][0m - Loading weights file model_state.pdparams from cache at /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/model_state.pdparams[0m
[32m[2023-11-10 08:55:45,202] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-11-10 08:55:46,643] [    INFO][0m - All model checkpoint weights were used when initializing ErnieLayoutForQuestionAnswering.
[0m
[33m[2023-11-10 08:55:46,643] [ WARNING][0m - Some weights of ErnieLayoutForQuestionAnswering were not initialized from the model checkpoint at ernie-layoutx-base-uncased and are newly initialized: ['visual.pixel_std', 'visual.pixel_mean', 'embeddings.position_ids', 'qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-11-10 08:55:46,674] [    INFO][0m - spliting train dataset into 16 shard[0m
[32m[2023-11-10 08:56:23,369] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 08:56:23,369] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-11-10 08:56:23,369] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 08:56:23,369] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - bf16                          : False[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - dataset_world_size            : 3[0m
[32m[2023-11-10 08:56:23,370] [    INFO][0m - device                        : gpu[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - do_eval                       : True[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - do_export                     : False[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - do_predict                    : False[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - do_train                      : True[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - eval_batch_size               : 6[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - eval_steps                    : 1000[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - fp16                          : False[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - fp16_opt_level                : O1[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - greater_is_better             : True[0m
[32m[2023-11-10 08:56:23,371] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - label_names                   : ['start_positions', 'end_positions'][0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - learning_rate                 : 2e-05[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - load_sharded_model            : False[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - local_process_index           : 0[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - local_rank                    : 0[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - log_level                     : -1[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - logging_dir                   : ./models/fidelity/runs/Nov10_08-55-41_ip-172-31-1-102[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - logging_steps                 : 500[0m
[32m[2023-11-10 08:56:23,372] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - metric_for_best_model         : anls[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - num_train_epochs              : 4.0[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - output_dir                    : ./models/fidelity/[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - overwrite_output_dir          : True[0m
[32m[2023-11-10 08:56:23,373] [    INFO][0m - past_index                    : -1[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - per_device_eval_batch_size    : 6[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - per_device_train_batch_size   : 6[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - power                         : 1.0[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - process_index                 : 0[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - recompute                     : False[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - run_name                      : ./models/fidelity/[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - save_sharded_model            : False[0m
[32m[2023-11-10 08:56:23,374] [    INFO][0m - save_steps                    : 1000[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - save_total_limit              : 1[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - seed                          : 1000[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - sharding                      : [][0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - should_log                    : True[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - should_save                   : True[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-11-10 08:56:23,375] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2023-11-10 08:56:23,376] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-11-10 08:56:23,376] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-11-10 08:56:23,376] [    INFO][0m - train_batch_size              : 6[0m
[32m[2023-11-10 08:56:23,376] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2023-11-10 08:56:23,376] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2023-11-10 08:56:23,376] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-11-10 08:56:23,376] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-11-10 08:56:23,376] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2023-11-10 08:56:23,376] [    INFO][0m - world_size                    : 3[0m
[32m[2023-11-10 08:56:23,376] [    INFO][0m - [0m
[32m[2023-11-10 08:56:23,376] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, token_is_max_context, end_labels, question_id, id, start_labels, questions, token_to_orig_map. If tokens, token_is_max_context, end_labels, question_id, id, start_labels, questions, token_to_orig_map are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 08:56:29,646] [    INFO][0m - ***** Running training *****[0m
[32m[2023-11-10 08:56:29,646] [    INFO][0m -   Num examples = 13,978[0m
[32m[2023-11-10 08:56:29,646] [    INFO][0m -   Num Epochs = 4[0m
[32m[2023-11-10 08:56:29,646] [    INFO][0m -   Instantaneous batch size per device = 6[0m
[32m[2023-11-10 08:56:29,646] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 18[0m
[32m[2023-11-10 08:56:29,646] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-11-10 08:56:29,646] [    INFO][0m -   Total optimization steps = 3,108[0m
[32m[2023-11-10 08:56:29,646] [    INFO][0m -   Total num train samples = 55,912[0m
[32m[2023-11-10 08:56:29,648] [    INFO][0m -   Number of trainable parameters = 281,693,122 (per device)[0m
  0%|          | 0/3108 [00:00<?, ?it/s]  0%|          | 1/3108 [00:04<4:16:24,  4.95s/it]  0%|          | 2/3108 [00:06<2:27:50,  2.86s/it]  0%|          | 3/3108 [00:07<1:53:07,  2.19s/it]  0%|          | 4/3108 [00:09<1:36:53,  1.87s/it]  0%|          | 5/3108 [00:10<1:27:55,  1.70s/it]  0%|          | 6/3108 [00:11<1:22:24,  1.59s/it]  0%|          | 7/3108 [00:13<1:19:04,  1.53s/it]  0%|          | 8/3108 [00:14<1:16:44,  1.49s/it]  0%|          | 9/3108 [00:16<1:15:15,  1.46s/it]  0%|          | 10/3108 [00:17<1:14:14,  1.44s/it]  0%|          | 11/3108 [00:18<1:13:31,  1.42s/it]  0%|          | 12/3108 [00:20<1:12:56,  1.41s/it]  0%|          | 13/3108 [00:21<1:12:29,  1.41s/it]  0%|          | 14/3108 [00:23<1:12:19,  1.40s/it]  0%|          | 15/3108 [00:24<1:12:12,  1.40s/it]  1%|          | 16/3108 [00:25<1:12:00,  1.40s/it]  1%|          | 17/3108 [00:27<1:11:51,  1.39s/it]  1%|          | 18/3108 [00:28<1:11:47,  1.39s/it]  1%|          | 19/3108 [00:30<1:11:47,  1.39s/it]  1%|          | 20/3108 [00:31<1:11:46,  1.39s/it]  1%|          | 21/3108 [00:32<1:11:44,  1.39s/it]  1%|          | 22/3108 [00:34<1:11:39,  1.39s/it]  1%|          | 23/3108 [00:35<1:11:42,  1.39s/it]  1%|          | 24/3108 [00:36<1:11:26,  1.39s/it]  1%|          | 25/3108 [00:38<1:11:35,  1.39s/it]  1%|          | 26/3108 [00:39<1:11:39,  1.40s/it]  1%|          | 27/3108 [00:41<1:11:37,  1.39s/it]  1%|          | 28/3108 [00:42<1:11:34,  1.39s/it]  1%|          | 29/3108 [00:43<1:11:28,  1.39s/it]  1%|          | 30/3108 [00:45<1:11:22,  1.39s/it]  1%|          | 31/3108 [00:46<1:11:22,  1.39s/it]  1%|          | 32/3108 [00:48<1:11:27,  1.39s/it]  1%|          | 33/3108 [00:49<1:11:25,  1.39s/it]  1%|          | 34/3108 [00:50<1:11:22,  1.39s/it]  1%|          | 35/3108 [00:52<1:11:17,  1.39s/it]  1%|          | 36/3108 [00:53<1:11:17,  1.39s/it]  1%|          | 37/3108 [00:55<1:11:15,  1.39s/it]  1%|          | 38/3108 [00:56<1:11:09,  1.39s/it]  1%|▏         | 39/3108 [00:57<1:11:10,  1.39s/it]  1%|▏         | 40/3108 [00:59<1:11:12,  1.39s/it]  1%|▏         | 41/3108 [01:00<1:11:11,  1.39s/it]  1%|▏         | 42/3108 [01:02<1:11:00,  1.39s/it]  1%|▏         | 43/3108 [01:03<1:11:13,  1.39s/it][33m[2023-11-10 09:02:29,812] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 09:02:30,861] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I1110 09:02:30.862179 60367 tcp_utils.cc:181] The server starts to listen on IP_ANY:63572
I1110 09:02:30.862305 60367 tcp_utils.cc:130] Successfully connected to 172.31.1.102:63572
W1110 09:02:35.148135 60367 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 09:02:35.154129 60367 gpu_resources.cc:149] device: 1, cuDNN Version: 8.5.
[32m[2023-11-10 09:02:35,756] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 09:02:35,757] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 09:02:35,757] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 09:02:35,757] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 09:02:35,757] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 09:02:35,757] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - model_name_or_path            : doc15k[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - [0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 09:02:35,758] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - return_entity_level_metrics   : False[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 09:02:35,759] [    INFO][0m - [0m
[32m[2023-11-10 09:02:35,803] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'> to load 'doc15k'.[0m
Traceback (most recent call last):
  File "run_mrc.py", line 243, in <module>
    main()
  File "run_mrc.py", line 72, in main
    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/transformers/auto/tokenizer.py", line 345, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/transformers/tokenizer_utils_base.py", line 1593, in from_pretrained
    tokenizer = cls(*init_args, **init_kwargs)
  File "/home/ubuntu/Demo-with-user-question/paddlenlp/transformers/utils.py", line 249, in __impl__
    init_func(self, *args, **kwargs)
TypeError: __init__() missing 1 required positional argument: 'vocab_file'
I1110 09:02:36.235502 60503 tcp_store.cc:273] receive shutdown event and so quit from MasterDaemon run loop
[33m[2023-11-10 09:04:44,690] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 09:04:45,748] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I1110 09:04:45.749251 60728 tcp_utils.cc:181] The server starts to listen on IP_ANY:55856
I1110 09:04:45.749394 60728 tcp_utils.cc:130] Successfully connected to 172.31.1.102:55856
W1110 09:04:47.284934 60728 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 09:04:47.290864 60728 gpu_resources.cc:149] device: 1, cuDNN Version: 8.5.
[32m[2023-11-10 09:04:47,904] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 09:04:47,904] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m - model_name_or_path            : doc15k[0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m - [0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 09:04:47,905] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - return_entity_level_metrics   : False[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 09:04:47,906] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 09:04:47,907] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 09:04:47,907] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 09:04:47,907] [    INFO][0m - [0m
[32m[2023-11-10 09:04:47,947] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'> to load 'doc15k'.[0m
[32m[2023-11-10 09:04:48,506] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.modeling.ErnieLayoutForQuestionAnswering'> to load 'doc15k'.[0m
[32m[2023-11-10 09:04:48,507] [    INFO][0m - Loading configuration file doc15k/config.json[0m
[32m[2023-11-10 09:04:48,507] [    INFO][0m - Loading weights file doc15k/model_state.pdparams[0m
[32m[2023-11-10 09:04:49,854] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-11-10 09:04:51,280] [    INFO][0m - All model checkpoint weights were used when initializing ErnieLayoutForQuestionAnswering.
[0m
[32m[2023-11-10 09:04:51,280] [    INFO][0m - All the weights of ErnieLayoutForQuestionAnswering were initialized from the model checkpoint at doc15k.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieLayoutForQuestionAnswering for predictions without further training.[0m
[32m[2023-11-10 09:04:51,312] [    INFO][0m - spliting train dataset into 16 shard[0m
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   3%|▎         | 2/68 [00:02<01:13,  1.11s/ examples]Map (num_proc=32):   6%|▌         | 4/68 [00:02<00:31,  2.00 examples/s]Map (num_proc=32):  13%|█▎        | 9/68 [00:03<00:15,  3.93 examples/s]Map (num_proc=32):  19%|█▉        | 13/68 [00:03<00:10,  5.12 examples/s]Map (num_proc=32):  29%|██▉       | 20/68 [00:03<00:05,  8.81 examples/s]Map (num_proc=32):  32%|███▏      | 22/68 [00:04<00:06,  7.03 examples/s]Map (num_proc=32):  43%|████▎     | 29/68 [00:05<00:04,  8.36 examples/s]Map (num_proc=32):  65%|██████▍   | 44/68 [00:05<00:01, 17.89 examples/s]Map (num_proc=32):  71%|███████   | 48/68 [00:05<00:01, 17.42 examples/s]Map (num_proc=32):  76%|███████▋  | 52/68 [00:05<00:00, 16.37 examples/s]Map (num_proc=32):  91%|█████████ | 62/68 [00:05<00:00, 24.65 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 23.56 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 10.68 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   4%|▍         | 3/68 [00:02<00:47,  1.36 examples/s]Map (num_proc=32):   9%|▉         | 6/68 [00:02<00:27,  2.26 examples/s]Map (num_proc=32):  12%|█▏        | 8/68 [00:03<00:19,  3.13 examples/s]Map (num_proc=32):  15%|█▍        | 10/68 [00:03<00:15,  3.75 examples/s]Map (num_proc=32):  26%|██▋       | 18/68 [00:03<00:05,  8.69 examples/s]Map (num_proc=32):  35%|███▌      | 24/68 [00:04<00:03, 11.49 examples/s]Map (num_proc=32):  38%|███▊      | 26/68 [00:04<00:04, 10.29 examples/s]Map (num_proc=32):  41%|████      | 28/68 [00:04<00:03, 10.42 examples/s]Map (num_proc=32):  44%|████▍     | 30/68 [00:04<00:03, 10.38 examples/s]Map (num_proc=32):  53%|█████▎    | 36/68 [00:05<00:02, 12.33 examples/s]Map (num_proc=32):  56%|█████▌    | 38/68 [00:05<00:02, 13.15 examples/s]Map (num_proc=32):  71%|███████   | 48/68 [00:05<00:00, 23.05 examples/s]Map (num_proc=32):  76%|███████▋  | 52/68 [00:05<00:00, 24.50 examples/s]Map (num_proc=32):  97%|█████████▋| 66/68 [00:05<00:00, 40.35 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 11.15 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   4%|▍         | 3/68 [00:02<00:58,  1.11 examples/s]Map (num_proc=32):   9%|▉         | 6/68 [00:02<00:24,  2.49 examples/s]Map (num_proc=32):  12%|█▏        | 8/68 [00:03<00:19,  3.05 examples/s]Map (num_proc=32):  25%|██▌       | 17/68 [00:03<00:05,  9.34 examples/s]Map (num_proc=32):  31%|███       | 21/68 [00:04<00:06,  7.06 examples/s]Map (num_proc=32):  40%|███▉      | 27/68 [00:04<00:04,  9.91 examples/s]Map (num_proc=32):  46%|████▌     | 31/68 [00:04<00:03, 12.33 examples/s]Map (num_proc=32):  59%|█████▉    | 40/68 [00:05<00:01, 15.18 examples/s]Map (num_proc=32):  71%|███████   | 48/68 [00:05<00:00, 21.72 examples/s]Map (num_proc=32):  82%|████████▏ | 56/68 [00:05<00:00, 26.33 examples/s]Map (num_proc=32):  91%|█████████ | 62/68 [00:05<00:00, 29.51 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:05<00:00, 32.35 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:05<00:00, 11.60 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   4%|▍         | 3/68 [00:02<00:55,  1.18 examples/s]Map (num_proc=32):  12%|█▏        | 8/68 [00:02<00:16,  3.65 examples/s]Map (num_proc=32):  15%|█▍        | 10/68 [00:02<00:13,  4.27 examples/s]Map (num_proc=32):  18%|█▊        | 12/68 [00:03<00:17,  3.24 examples/s]Map (num_proc=32):  32%|███▏      | 22/68 [00:04<00:06,  6.78 examples/s]Map (num_proc=32):  41%|████      | 28/68 [00:04<00:04,  9.15 examples/s]Map (num_proc=32):  62%|██████▏   | 42/68 [00:05<00:01, 18.41 examples/s]Map (num_proc=32):  71%|███████   | 48/68 [00:05<00:00, 21.37 examples/s]Map (num_proc=32):  79%|███████▉  | 54/68 [00:05<00:00, 24.66 examples/s]Map (num_proc=32):  88%|████████▊ | 60/68 [00:05<00:00, 25.07 examples/s]Map (num_proc=32):  97%|█████████▋| 66/68 [00:05<00:00, 25.16 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 11.10 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   3%|▎         | 2/68 [00:02<01:25,  1.30s/ examples]Map (num_proc=32):   6%|▌         | 4/68 [00:03<00:42,  1.49 examples/s]Map (num_proc=32):   9%|▉         | 6/68 [00:03<00:28,  2.21 examples/s]Map (num_proc=32):  21%|██        | 14/68 [00:04<00:09,  5.43 examples/s]Map (num_proc=32):  31%|███       | 21/68 [00:04<00:05,  8.49 examples/s]Map (num_proc=32):  35%|███▌      | 24/68 [00:04<00:04,  9.51 examples/s]Map (num_proc=32):  41%|████      | 28/68 [00:04<00:04,  9.80 examples/s]Map (num_proc=32):  59%|█████▉    | 40/68 [00:05<00:01, 19.24 examples/s]Map (num_proc=32):  74%|███████▎  | 50/68 [00:05<00:00, 28.21 examples/s]Map (num_proc=32):  82%|████████▏ | 56/68 [00:05<00:00, 25.42 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 24.07 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 10.88 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   3%|▎         | 2/68 [00:02<01:23,  1.26s/ examples]Map (num_proc=32):   6%|▌         | 4/68 [00:02<00:41,  1.55 examples/s]Map (num_proc=32):  10%|█         | 7/68 [00:03<00:19,  3.18 examples/s]Map (num_proc=32):  15%|█▍        | 10/68 [00:03<00:12,  4.72 examples/s]Map (num_proc=32):  19%|█▉        | 13/68 [00:03<00:08,  6.28 examples/s]Map (num_proc=32):  22%|██▏       | 15/68 [00:03<00:08,  5.99 examples/s]Map (num_proc=32):  31%|███       | 21/68 [00:04<00:04, 10.50 examples/s]Map (num_proc=32):  40%|███▉      | 27/68 [00:04<00:02, 15.15 examples/s]Map (num_proc=32):  44%|████▍     | 30/68 [00:04<00:03, 11.94 examples/s]Map (num_proc=32):  56%|█████▌    | 38/68 [00:04<00:01, 18.97 examples/s]Map (num_proc=32):  68%|██████▊   | 46/68 [00:05<00:00, 25.53 examples/s]Map (num_proc=32):  74%|███████▎  | 50/68 [00:05<00:00, 27.64 examples/s]Map (num_proc=32):  82%|████████▏ | 56/68 [00:05<00:00, 30.95 examples/s]Map (num_proc=32):  88%|████████▊ | 60/68 [00:05<00:00, 25.46 examples/s]Map (num_proc=32):  97%|█████████▋| 66/68 [00:05<00:00, 25.65 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:05<00:00, 11.43 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   4%|▍         | 3/68 [00:02<00:58,  1.11 examples/s]Map (num_proc=32):   7%|▋         | 5/68 [00:03<00:38,  1.63 examples/s]Map (num_proc=32):  13%|█▎        | 9/68 [00:03<00:16,  3.60 examples/s]Map (num_proc=32):  24%|██▎       | 16/68 [00:03<00:07,  6.51 examples/s]Map (num_proc=32):  26%|██▋       | 18/68 [00:04<00:07,  6.36 examples/s]Map (num_proc=32):  29%|██▉       | 20/68 [00:04<00:06,  7.28 examples/s]Map (num_proc=32):  40%|███▉      | 27/68 [00:04<00:03, 13.57 examples/s]Map (num_proc=32):  46%|████▌     | 31/68 [00:04<00:02, 13.78 examples/s]Map (num_proc=32):  63%|██████▎   | 43/68 [00:04<00:00, 25.61 examples/s]Map (num_proc=32):  72%|███████▏  | 49/68 [00:05<00:00, 25.77 examples/s]Map (num_proc=32):  79%|███████▉  | 54/68 [00:05<00:00, 26.32 examples/s]Map (num_proc=32):  94%|█████████▍| 64/68 [00:05<00:00, 33.29 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:05<00:00, 29.80 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:05<00:00, 11.44 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   4%|▍         | 3/68 [00:03<01:06,  1.03s/ examples]Map (num_proc=32):  12%|█▏        | 8/68 [00:03<00:19,  3.12 examples/s]Map (num_proc=32):  19%|█▉        | 13/68 [00:03<00:10,  5.22 examples/s]Map (num_proc=32):  25%|██▌       | 17/68 [00:04<00:08,  5.78 examples/s]Map (num_proc=32):  31%|███       | 21/68 [00:04<00:05,  8.01 examples/s]Map (num_proc=32):  35%|███▌      | 24/68 [00:04<00:05,  8.79 examples/s]Map (num_proc=32):  38%|███▊      | 26/68 [00:04<00:05,  8.09 examples/s]Map (num_proc=32):  50%|█████     | 34/68 [00:04<00:02, 14.67 examples/s]Map (num_proc=32):  56%|█████▌    | 38/68 [00:05<00:01, 16.34 examples/s]Map (num_proc=32):  65%|██████▍   | 44/68 [00:05<00:01, 20.28 examples/s]Map (num_proc=32):  97%|█████████▋| 66/68 [00:05<00:00, 48.42 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 11.03 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   3%|▎         | 2/68 [00:02<01:11,  1.08s/ examples]Map (num_proc=32):   6%|▌         | 4/68 [00:03<00:53,  1.19 examples/s]Map (num_proc=32):  12%|█▏        | 8/68 [00:03<00:19,  3.03 examples/s]Map (num_proc=32):  24%|██▎       | 16/68 [00:03<00:06,  7.89 examples/s]Map (num_proc=32):  32%|███▏      | 22/68 [00:04<00:05,  8.20 examples/s]Map (num_proc=32):  40%|███▉      | 27/68 [00:04<00:04, 10.16 examples/s]Map (num_proc=32):  44%|████▍     | 30/68 [00:04<00:03, 11.26 examples/s]Map (num_proc=32):  53%|█████▎    | 36/68 [00:05<00:02, 14.48 examples/s]Map (num_proc=32):  68%|██████▊   | 46/68 [00:05<00:00, 23.42 examples/s]Map (num_proc=32):  74%|███████▎  | 50/68 [00:05<00:00, 24.66 examples/s]Map (num_proc=32):  82%|████████▏ | 56/68 [00:05<00:00, 28.90 examples/s]Map (num_proc=32):  88%|████████▊ | 60/68 [00:05<00:00, 24.97 examples/s]Map (num_proc=32):  94%|█████████▍| 64/68 [00:05<00:00, 20.76 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 22.68 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 10.92 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   3%|▎         | 2/68 [00:02<01:29,  1.35s/ examples]Map (num_proc=32):   7%|▋         | 5/68 [00:03<00:35,  1.79 examples/s]Map (num_proc=32):  16%|█▌        | 11/68 [00:03<00:14,  3.86 examples/s]Map (num_proc=32):  21%|██        | 14/68 [00:04<00:11,  4.61 examples/s]Map (num_proc=32):  34%|███▍      | 23/68 [00:04<00:04, 10.15 examples/s]Map (num_proc=32):  40%|███▉      | 27/68 [00:04<00:03, 12.14 examples/s]Map (num_proc=32):  56%|█████▌    | 38/68 [00:04<00:01, 17.69 examples/s]Map (num_proc=32):  62%|██████▏   | 42/68 [00:05<00:01, 17.77 examples/s]Map (num_proc=32):  68%|██████▊   | 46/68 [00:05<00:01, 17.91 examples/s]Map (num_proc=32):  85%|████████▌ | 58/68 [00:05<00:00, 29.97 examples/s]Map (num_proc=32):  94%|█████████▍| 64/68 [00:05<00:00, 22.66 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 11.10 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   3%|▎         | 2/68 [00:02<01:29,  1.36s/ examples]Map (num_proc=32):   6%|▌         | 4/68 [00:03<00:48,  1.31 examples/s]Map (num_proc=32):  10%|█         | 7/68 [00:03<00:22,  2.75 examples/s]Map (num_proc=32):  18%|█▊        | 12/68 [00:03<00:09,  5.86 examples/s]Map (num_proc=32):  24%|██▎       | 16/68 [00:03<00:06,  7.66 examples/s]Map (num_proc=32):  26%|██▋       | 18/68 [00:04<00:07,  7.03 examples/s]Map (num_proc=32):  43%|████▎     | 29/68 [00:04<00:02, 16.69 examples/s]Map (num_proc=32):  49%|████▊     | 33/68 [00:04<00:02, 13.34 examples/s]Map (num_proc=32):  59%|█████▉    | 40/68 [00:05<00:01, 18.34 examples/s]Map (num_proc=32):  68%|██████▊   | 46/68 [00:05<00:00, 22.63 examples/s]Map (num_proc=32):  76%|███████▋  | 52/68 [00:05<00:00, 25.40 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:05<00:00, 29.12 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 11.32 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   3%|▎         | 2/68 [00:02<01:38,  1.49s/ examples]Map (num_proc=32):   7%|▋         | 5/68 [00:03<00:33,  1.90 examples/s]Map (num_proc=32):  10%|█         | 7/68 [00:03<00:22,  2.71 examples/s]Map (num_proc=32):  15%|█▍        | 10/68 [00:03<00:13,  4.37 examples/s]Map (num_proc=32):  25%|██▌       | 17/68 [00:04<00:07,  6.75 examples/s]Map (num_proc=32):  28%|██▊       | 19/68 [00:04<00:06,  7.68 examples/s]Map (num_proc=32):  41%|████      | 28/68 [00:04<00:02, 14.74 examples/s]Map (num_proc=32):  59%|█████▉    | 40/68 [00:04<00:01, 26.84 examples/s]Map (num_proc=32):  68%|██████▊   | 46/68 [00:04<00:00, 24.73 examples/s]Map (num_proc=32):  76%|███████▋  | 52/68 [00:05<00:00, 23.18 examples/s]Map (num_proc=32):  82%|████████▏ | 56/68 [00:05<00:00, 22.96 examples/s]Map (num_proc=32):  91%|█████████ | 62/68 [00:05<00:00, 25.27 examples/s]Map (num_proc=32):  97%|█████████▋| 66/68 [00:05<00:00, 25.96 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 11.30 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   4%|▍         | 3/68 [00:02<00:59,  1.08 examples/s]Map (num_proc=32):   7%|▋         | 5/68 [00:02<00:32,  1.93 examples/s]Map (num_proc=32):  10%|█         | 7/68 [00:03<00:27,  2.24 examples/s]Map (num_proc=32):  22%|██▏       | 15/68 [00:04<00:09,  5.58 examples/s]Map (num_proc=32):  29%|██▉       | 20/68 [00:04<00:05,  8.48 examples/s]Map (num_proc=32):  35%|███▌      | 24/68 [00:04<00:04, 10.59 examples/s]Map (num_proc=32):  41%|████      | 28/68 [00:04<00:02, 13.59 examples/s]Map (num_proc=32):  65%|██████▍   | 44/68 [00:04<00:00, 29.17 examples/s]Map (num_proc=32):  74%|███████▎  | 50/68 [00:04<00:00, 30.18 examples/s]Map (num_proc=32):  82%|████████▏ | 56/68 [00:05<00:00, 25.39 examples/s]Map (num_proc=32):  88%|████████▊ | 60/68 [00:05<00:00, 24.37 examples/s]Map (num_proc=32):  94%|█████████▍| 64/68 [00:05<00:00, 25.12 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:05<00:00, 11.75 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   3%|▎         | 2/68 [00:02<01:26,  1.30s/ examples]Map (num_proc=32):   7%|▋         | 5/68 [00:02<00:29,  2.12 examples/s]Map (num_proc=32):  13%|█▎        | 9/68 [00:03<00:18,  3.26 examples/s]Map (num_proc=32):  24%|██▎       | 16/68 [00:04<00:09,  5.61 examples/s]Map (num_proc=32):  37%|███▋      | 25/68 [00:04<00:04, 10.53 examples/s]Map (num_proc=32):  46%|████▌     | 31/68 [00:04<00:02, 14.05 examples/s]Map (num_proc=32):  54%|█████▍    | 37/68 [00:04<00:01, 17.39 examples/s]Map (num_proc=32):  68%|██████▊   | 46/68 [00:04<00:00, 23.28 examples/s]Map (num_proc=32):  74%|███████▎  | 50/68 [00:04<00:00, 24.09 examples/s]Map (num_proc=32):  79%|███████▉  | 54/68 [00:05<00:00, 18.52 examples/s]Map (num_proc=32):  91%|█████████ | 62/68 [00:05<00:00, 25.36 examples/s]Map (num_proc=32):  97%|█████████▋| 66/68 [00:05<00:00, 26.18 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:05<00:00, 11.54 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   3%|▎         | 2/68 [00:02<01:19,  1.20s/ examples]Map (num_proc=32):   7%|▋         | 5/68 [00:03<00:33,  1.86 examples/s]Map (num_proc=32):  10%|█         | 7/68 [00:03<00:21,  2.88 examples/s]Map (num_proc=32):  13%|█▎        | 9/68 [00:03<00:14,  4.05 examples/s]Map (num_proc=32):  16%|█▌        | 11/68 [00:03<00:10,  5.42 examples/s]Map (num_proc=32):  21%|██        | 14/68 [00:03<00:07,  7.41 examples/s]Map (num_proc=32):  26%|██▋       | 18/68 [00:04<00:10,  4.73 examples/s]Map (num_proc=32):  37%|███▋      | 25/68 [00:05<00:05,  7.77 examples/s]Map (num_proc=32):  40%|███▉      | 27/68 [00:05<00:04,  8.69 examples/s]Map (num_proc=32):  69%|██████▉   | 47/68 [00:05<00:00, 24.22 examples/s]Map (num_proc=32):  88%|████████▊ | 60/68 [00:05<00:00, 34.41 examples/s]Map (num_proc=32):  97%|█████████▋| 66/68 [00:05<00:00, 34.58 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:06<00:00, 10.35 examples/s]
Map (num_proc=32):   0%|          | 0/68 [00:00<?, ? examples/s]Map (num_proc=32):   3%|▎         | 2/68 [00:02<01:07,  1.03s/ examples]Map (num_proc=32):   6%|▌         | 4/68 [00:02<00:39,  1.64 examples/s]Map (num_proc=32):  13%|█▎        | 9/68 [00:02<00:13,  4.46 examples/s]Map (num_proc=32):  18%|█▊        | 12/68 [00:03<00:10,  5.41 examples/s]Map (num_proc=32):  21%|██        | 14/68 [00:03<00:09,  5.75 examples/s]Map (num_proc=32):  24%|██▎       | 16/68 [00:03<00:08,  6.20 examples/s]Map (num_proc=32):  29%|██▉       | 20/68 [00:03<00:05,  9.37 examples/s]Map (num_proc=32):  46%|████▌     | 31/68 [00:04<00:03, 10.89 examples/s]Map (num_proc=32):  62%|██████▏   | 42/68 [00:04<00:01, 18.98 examples/s]Map (num_proc=32):  71%|███████   | 48/68 [00:05<00:00, 22.00 examples/s]Map (num_proc=32):  79%|███████▉  | 54/68 [00:05<00:00, 25.54 examples/s]Map (num_proc=32):  94%|█████████▍| 64/68 [00:05<00:00, 34.56 examples/s]Map (num_proc=32): 100%|██████████| 68/68 [00:05<00:00, 11.63 examples/s]
Map (num_proc=32):   0%|          | 0/58 [00:00<?, ? examples/s]Map (num_proc=32):   3%|▎         | 2/58 [00:02<00:58,  1.04s/ examples]Map (num_proc=32):   7%|▋         | 4/58 [00:02<00:30,  1.77 examples/s]Map (num_proc=32):  14%|█▍        | 8/58 [00:02<00:12,  4.11 examples/s]Map (num_proc=32):  17%|█▋        | 10/58 [00:03<00:12,  3.99 examples/s]Map (num_proc=32):  21%|██        | 12/58 [00:03<00:09,  4.73 examples/s]Map (num_proc=32):  26%|██▌       | 15/58 [00:03<00:06,  6.18 examples/s]Map (num_proc=32):  33%|███▎      | 19/58 [00:03<00:04,  9.43 examples/s]Map (num_proc=32):  40%|███▉      | 23/58 [00:04<00:04,  8.53 examples/s]Map (num_proc=32):  45%|████▍     | 26/58 [00:04<00:03, 10.00 examples/s]Map (num_proc=32):  72%|███████▏  | 42/58 [00:05<00:00, 20.50 examples/s]Map (num_proc=32):  78%|███████▊  | 45/58 [00:05<00:00, 18.97 examples/s]Map (num_proc=32):  91%|█████████▏| 53/58 [00:05<00:00, 25.52 examples/s]Map (num_proc=32): 100%|██████████| 58/58 [00:05<00:00, 22.38 examples/s]Map (num_proc=32): 100%|██████████| 58/58 [00:05<00:00,  9.84 examples/s]
[32m[2023-11-10 09:07:16,785] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 09:07:16,785] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-11-10 09:07:16,785] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 09:07:16,786] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-11-10 09:07:16,786] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-11-10 09:07:16,786] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-11-10 09:07:16,786] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-11-10 09:07:16,786] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2023-11-10 09:07:16,786] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2023-11-10 09:07:16,786] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-11-10 09:07:16,786] [    INFO][0m - bf16                          : False[0m
[32m[2023-11-10 09:07:16,786] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-11-10 09:07:16,786] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-11-10 09:07:16,786] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - dataset_world_size            : 3[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - device                        : gpu[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - do_eval                       : True[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - do_export                     : False[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - do_predict                    : False[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - do_train                      : True[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - eval_batch_size               : 6[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - eval_steps                    : 1000[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-11-10 09:07:16,787] [    INFO][0m - fp16                          : False[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - fp16_opt_level                : O1[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - greater_is_better             : True[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - label_names                   : ['start_positions', 'end_positions'][0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - learning_rate                 : 2e-05[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - load_sharded_model            : False[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - local_process_index           : 0[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - local_rank                    : 0[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - log_level                     : -1[0m
[32m[2023-11-10 09:07:16,788] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - logging_dir                   : ./models/fidelity/runs/Nov10_09-04-45_ip-172-31-1-102[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - logging_steps                 : 500[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - metric_for_best_model         : anls[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2023-11-10 09:07:16,789] [    INFO][0m - num_train_epochs              : 4.0[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - output_dir                    : ./models/fidelity/[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - overwrite_output_dir          : True[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - past_index                    : -1[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - per_device_eval_batch_size    : 6[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - per_device_train_batch_size   : 6[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - power                         : 1.0[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - process_index                 : 0[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - recompute                     : False[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-11-10 09:07:16,790] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - run_name                      : ./models/fidelity/[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - save_sharded_model            : False[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - save_steps                    : 1000[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - save_total_limit              : 1[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - seed                          : 1000[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - sharding                      : [][0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2023-11-10 09:07:16,791] [    INFO][0m - should_log                    : True[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - should_save                   : True[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - train_batch_size              : 6[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - world_size                    : 3[0m
[32m[2023-11-10 09:07:16,792] [    INFO][0m - [0m
[32m[2023-11-10 09:07:16,793] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, token_to_orig_map, token_is_max_context, question_id, questions, tokens, id, start_labels. If end_labels, token_to_orig_map, token_is_max_context, question_id, questions, tokens, id, start_labels are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 09:07:23,055] [    INFO][0m - ***** Running training *****[0m
[32m[2023-11-10 09:07:23,055] [    INFO][0m -   Num examples = 13,978[0m
[32m[2023-11-10 09:07:23,055] [    INFO][0m -   Num Epochs = 4[0m
[32m[2023-11-10 09:07:23,055] [    INFO][0m -   Instantaneous batch size per device = 6[0m
[32m[2023-11-10 09:07:23,055] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 18[0m
[32m[2023-11-10 09:07:23,055] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-11-10 09:07:23,055] [    INFO][0m -   Total optimization steps = 3,108[0m
[32m[2023-11-10 09:07:23,055] [    INFO][0m -   Total num train samples = 55,912[0m
[32m[2023-11-10 09:07:23,058] [    INFO][0m -   Number of trainable parameters = 281,693,122 (per device)[0m
  0%|          | 0/3108 [00:00<?, ?it/s]  0%|          | 1/3108 [00:05<4:20:06,  5.02s/it]  0%|          | 2/3108 [00:06<2:29:59,  2.90s/it]  0%|          | 3/3108 [00:07<1:54:18,  2.21s/it]  0%|          | 4/3108 [00:09<1:37:35,  1.89s/it]  0%|          | 5/3108 [00:10<1:28:21,  1.71s/it]  0%|          | 6/3108 [00:12<1:22:49,  1.60s/it]  0%|          | 7/3108 [00:13<1:19:30,  1.54s/it]  0%|          | 8/3108 [00:14<1:17:08,  1.49s/it]  0%|          | 9/3108 [00:16<1:15:32,  1.46s/it]  0%|          | 10/3108 [00:17<1:14:18,  1.44s/it]  0%|          | 11/3108 [00:18<1:13:34,  1.43s/it]  0%|          | 12/3108 [00:20<1:13:01,  1.42s/it]  0%|          | 13/3108 [00:21<1:12:44,  1.41s/it]  0%|          | 14/3108 [00:23<1:12:41,  1.41s/it]  0%|          | 15/3108 [00:24<1:12:36,  1.41s/it]  1%|          | 16/3108 [00:25<1:12:24,  1.40s/it]  1%|          | 17/3108 [00:27<1:12:16,  1.40s/it]  1%|          | 18/3108 [00:28<1:12:20,  1.40s/it]  1%|          | 19/3108 [00:30<1:12:19,  1.40s/it]  1%|          | 20/3108 [00:31<1:12:04,  1.40s/it]  1%|          | 21/3108 [00:32<1:11:57,  1.40s/it]  1%|          | 22/3108 [00:34<1:11:55,  1.40s/it]  1%|          | 23/3108 [00:35<1:12:02,  1.40s/it]  1%|          | 24/3108 [00:37<1:11:47,  1.40s/it]  1%|          | 25/3108 [00:38<1:11:47,  1.40s/it]  1%|          | 26/3108 [00:39<1:11:44,  1.40s/it]  1%|          | 27/3108 [00:41<1:12:00,  1.40s/it]  1%|          | 28/3108 [00:42<1:11:52,  1.40s/it]  1%|          | 29/3108 [00:44<1:11:56,  1.40s/it]  1%|          | 30/3108 [00:45<1:11:51,  1.40s/it]  1%|          | 31/3108 [00:46<1:12:01,  1.40s/it]  1%|          | 32/3108 [00:48<1:11:50,  1.40s/it]  1%|          | 33/3108 [00:49<1:11:54,  1.40s/it]  1%|          | 34/3108 [00:51<1:11:48,  1.40s/it]  1%|          | 35/3108 [00:52<1:11:48,  1.40s/it]  1%|          | 36/3108 [00:53<1:11:40,  1.40s/it]  1%|          | 37/3108 [00:55<1:11:38,  1.40s/it]  1%|          | 38/3108 [00:56<1:11:35,  1.40s/it]  1%|▏         | 39/3108 [00:58<1:11:42,  1.40s/it]  1%|▏         | 40/3108 [00:59<1:11:46,  1.40s/it]  1%|▏         | 41/3108 [01:01<1:11:41,  1.40s/it]  1%|▏         | 42/3108 [01:02<1:11:39,  1.40s/it]  1%|▏         | 43/3108 [01:03<1:11:35,  1.40s/it]  1%|▏         | 44/3108 [01:05<1:11:42,  1.40s/it]  1%|▏         | 45/3108 [01:06<1:11:49,  1.41s/it]  1%|▏         | 46/3108 [01:08<1:11:51,  1.41s/it]  2%|▏         | 47/3108 [01:09<1:11:51,  1.41s/it]  2%|▏         | 48/3108 [01:10<1:11:47,  1.41s/it]  2%|▏         | 49/3108 [01:12<1:11:40,  1.41s/it]  2%|▏         | 50/3108 [01:13<1:11:38,  1.41s/it]  2%|▏         | 51/3108 [01:15<1:11:30,  1.40s/it]  2%|▏         | 52/3108 [01:16<1:13:02,  1.43s/it]  2%|▏         | 53/3108 [01:17<1:12:33,  1.43s/it]  2%|▏         | 54/3108 [01:19<1:12:18,  1.42s/it]  2%|▏         | 55/3108 [01:20<1:12:00,  1.42s/it]  2%|▏         | 56/3108 [01:22<1:12:02,  1.42s/it]  2%|▏         | 57/3108 [01:23<1:11:51,  1.41s/it]  2%|▏         | 58/3108 [01:25<1:11:38,  1.41s/it]  2%|▏         | 59/3108 [01:26<1:13:06,  1.44s/it]  2%|▏         | 60/3108 [01:28<1:13:53,  1.45s/it]  2%|▏         | 61/3108 [01:29<1:13:10,  1.44s/it]  2%|▏         | 62/3108 [01:30<1:12:33,  1.43s/it]  2%|▏         | 63/3108 [01:32<1:12:09,  1.42s/it]  2%|▏         | 64/3108 [01:33<1:11:48,  1.42s/it]  2%|▏         | 65/3108 [01:35<1:11:44,  1.41s/it]  2%|▏         | 66/3108 [01:36<1:11:31,  1.41s/it]  2%|▏         | 67/3108 [01:37<1:11:28,  1.41s/it]  2%|▏         | 68/3108 [01:39<1:11:19,  1.41s/it]  2%|▏         | 69/3108 [01:40<1:11:19,  1.41s/it]  2%|▏         | 70/3108 [01:42<1:11:21,  1.41s/it]  2%|▏         | 71/3108 [01:43<1:11:21,  1.41s/it]  2%|▏         | 72/3108 [01:44<1:11:25,  1.41s/it]  2%|▏         | 73/3108 [01:46<1:11:26,  1.41s/it]  2%|▏         | 74/3108 [01:47<1:11:21,  1.41s/it]  2%|▏         | 75/3108 [01:49<1:11:10,  1.41s/it]  2%|▏         | 76/3108 [01:50<1:11:04,  1.41s/it]  2%|▏         | 77/3108 [01:52<1:12:33,  1.44s/it]  3%|▎         | 78/3108 [01:53<1:12:08,  1.43s/it]  3%|▎         | 79/3108 [01:54<1:11:52,  1.42s/it]  3%|▎         | 80/3108 [01:56<1:11:28,  1.42s/it]  3%|▎         | 81/3108 [01:57<1:11:20,  1.41s/it]  3%|▎         | 82/3108 [01:59<1:11:11,  1.41s/it]  3%|▎         | 83/3108 [02:00<1:11:03,  1.41s/it]  3%|▎         | 84/3108 [02:01<1:11:02,  1.41s/it]  3%|▎         | 85/3108 [02:03<1:11:02,  1.41s/it]  3%|▎         | 86/3108 [02:04<1:10:50,  1.41s/it]  3%|▎         | 87/3108 [02:06<1:10:47,  1.41s/it]  3%|▎         | 88/3108 [02:07<1:10:42,  1.40s/it]  3%|▎         | 89/3108 [02:09<1:12:12,  1.44s/it]  3%|▎         | 90/3108 [02:10<1:12:48,  1.45s/it]  3%|▎         | 91/3108 [02:11<1:12:10,  1.44s/it]  3%|▎         | 92/3108 [02:13<1:11:29,  1.42s/it]  3%|▎         | 93/3108 [02:14<1:11:17,  1.42s/it]  3%|▎         | 94/3108 [02:16<1:11:06,  1.42s/it]  3%|▎         | 95/3108 [02:17<1:10:51,  1.41s/it]  3%|▎         | 96/3108 [02:18<1:10:45,  1.41s/it]  3%|▎         | 97/3108 [02:20<1:10:47,  1.41s/it]  3%|▎         | 98/3108 [02:21<1:10:30,  1.41s/it]  3%|▎         | 99/3108 [02:23<1:10:31,  1.41s/it]  3%|▎         | 100/3108 [02:24<1:10:28,  1.41s/it]  3%|▎         | 101/3108 [02:25<1:10:28,  1.41s/it]  3%|▎         | 102/3108 [02:27<1:11:58,  1.44s/it]  3%|▎         | 103/3108 [02:28<1:11:22,  1.43s/it]  3%|▎         | 104/3108 [02:30<1:11:05,  1.42s/it]  3%|▎         | 105/3108 [02:31<1:10:42,  1.41s/it]  3%|▎         | 106/3108 [02:33<1:10:37,  1.41s/it]  3%|▎         | 107/3108 [02:34<1:10:24,  1.41s/it]  3%|▎         | 108/3108 [02:35<1:10:22,  1.41s/it]  4%|▎         | 109/3108 [02:37<1:10:27,  1.41s/it]  4%|▎         | 110/3108 [02:38<1:10:28,  1.41s/it]  4%|▎         | 111/3108 [02:40<1:10:25,  1.41s/it]  4%|▎         | 112/3108 [02:41<1:10:21,  1.41s/it]  4%|▎         | 113/3108 [02:42<1:10:07,  1.40s/it]  4%|▎         | 114/3108 [02:44<1:10:08,  1.41s/it]  4%|▎         | 115/3108 [02:45<1:10:00,  1.40s/it]  4%|▎         | 116/3108 [02:47<1:10:07,  1.41s/it]  4%|▍         | 117/3108 [02:48<1:10:03,  1.41s/it]  4%|▍         | 118/3108 [02:49<1:10:01,  1.41s/it]  4%|▍         | 119/3108 [02:51<1:11:25,  1.43s/it]  4%|▍         | 120/3108 [02:52<1:12:08,  1.45s/it]  4%|▍         | 121/3108 [02:54<1:11:33,  1.44s/it]  4%|▍         | 122/3108 [02:55<1:11:10,  1.43s/it]  4%|▍         | 123/3108 [02:57<1:10:41,  1.42s/it]  4%|▍         | 124/3108 [02:58<1:10:26,  1.42s/it]  4%|▍         | 125/3108 [02:59<1:10:08,  1.41s/it]  4%|▍         | 126/3108 [03:01<1:10:10,  1.41s/it]  4%|▍         | 127/3108 [03:02<1:10:01,  1.41s/it]  4%|▍         | 128/3108 [03:04<1:11:22,  1.44s/it]  4%|▍         | 129/3108 [03:05<1:10:50,  1.43s/it]  4%|▍         | 130/3108 [03:07<1:10:30,  1.42s/it]  4%|▍         | 131/3108 [03:08<1:10:17,  1.42s/it]  4%|▍         | 132/3108 [03:09<1:10:10,  1.41s/it]  4%|▍         | 133/3108 [03:11<1:09:58,  1.41s/it]  4%|▍         | 134/3108 [03:12<1:09:55,  1.41s/it]  4%|▍         | 135/3108 [03:14<1:09:52,  1.41s/it]  4%|▍         | 136/3108 [03:15<1:09:43,  1.41s/it]  4%|▍         | 137/3108 [03:16<1:09:37,  1.41s/it]  4%|▍         | 138/3108 [03:18<1:09:36,  1.41s/it]  4%|▍         | 139/3108 [03:19<1:09:45,  1.41s/it]  5%|▍         | 140/3108 [03:21<1:09:40,  1.41s/it]  5%|▍         | 141/3108 [03:22<1:09:32,  1.41s/it]  5%|▍         | 142/3108 [03:23<1:09:34,  1.41s/it]  5%|▍         | 143/3108 [03:25<1:09:39,  1.41s/it]  5%|▍         | 144/3108 [03:26<1:09:36,  1.41s/it]  5%|▍         | 145/3108 [03:28<1:09:36,  1.41s/it]  5%|▍         | 146/3108 [03:29<1:09:26,  1.41s/it]  5%|▍         | 147/3108 [03:30<1:09:28,  1.41s/it]  5%|▍         | 148/3108 [03:32<1:09:28,  1.41s/it]  5%|▍         | 149/3108 [03:33<1:10:59,  1.44s/it]  5%|▍         | 150/3108 [03:35<1:11:26,  1.45s/it]  5%|▍         | 151/3108 [03:36<1:10:53,  1.44s/it]  5%|▍         | 152/3108 [03:38<1:10:27,  1.43s/it]  5%|▍         | 153/3108 [03:39<1:11:22,  1.45s/it]  5%|▍         | 154/3108 [03:41<1:10:45,  1.44s/it]  5%|▍         | 155/3108 [03:42<1:10:16,  1.43s/it]  5%|▌         | 156/3108 [03:43<1:09:48,  1.42s/it]  5%|▌         | 157/3108 [03:45<1:09:42,  1.42s/it]  5%|▌         | 158/3108 [03:46<1:09:33,  1.41s/it]  5%|▌         | 159/3108 [03:48<1:09:15,  1.41s/it]  5%|▌         | 160/3108 [03:49<1:09:15,  1.41s/it]  5%|▌         | 161/3108 [03:50<1:09:17,  1.41s/it]  5%|▌         | 162/3108 [03:52<1:09:12,  1.41s/it]  5%|▌         | 163/3108 [03:53<1:09:17,  1.41s/it]  5%|▌         | 164/3108 [03:55<1:09:17,  1.41s/it]  5%|▌         | 165/3108 [03:56<1:09:11,  1.41s/it]  5%|▌         | 166/3108 [03:58<1:09:07,  1.41s/it]  5%|▌         | 167/3108 [03:59<1:08:54,  1.41s/it]  5%|▌         | 168/3108 [04:00<1:08:48,  1.40s/it]  5%|▌         | 169/3108 [04:02<1:08:54,  1.41s/it]  5%|▌         | 170/3108 [04:03<1:08:59,  1.41s/it]  6%|▌         | 171/3108 [04:05<1:08:59,  1.41s/it]  6%|▌         | 172/3108 [04:06<1:09:03,  1.41s/it]  6%|▌         | 173/3108 [04:07<1:09:06,  1.41s/it]  6%|▌         | 174/3108 [04:09<1:09:00,  1.41s/it]  6%|▌         | 175/3108 [04:10<1:09:00,  1.41s/it]  6%|▌         | 176/3108 [04:12<1:08:44,  1.41s/it]  6%|▌         | 177/3108 [04:13<1:08:38,  1.41s/it]  6%|▌         | 178/3108 [04:14<1:10:05,  1.44s/it]  6%|▌         | 179/3108 [04:16<1:11:05,  1.46s/it]  6%|▌         | 180/3108 [04:17<1:11:22,  1.46s/it]  6%|▌         | 181/3108 [04:19<1:10:27,  1.44s/it]  6%|▌         | 182/3108 [04:20<1:09:52,  1.43s/it]  6%|▌         | 183/3108 [04:22<1:09:23,  1.42s/it]  6%|▌         | 184/3108 [04:23<1:09:05,  1.42s/it]  6%|▌         | 185/3108 [04:25<1:08:57,  1.42s/it]  6%|▌         | 186/3108 [04:26<1:08:52,  1.41s/it]  6%|▌         | 187/3108 [04:27<1:08:50,  1.41s/it]  6%|▌         | 188/3108 [04:29<1:08:51,  1.41s/it]  6%|▌         | 189/3108 [04:30<1:08:46,  1.41s/it]  6%|▌         | 190/3108 [04:32<1:08:40,  1.41s/it]  6%|▌         | 191/3108 [04:33<1:08:42,  1.41s/it]  6%|▌         | 192/3108 [04:34<1:08:38,  1.41s/it]  6%|▌         | 193/3108 [04:36<1:08:31,  1.41s/it]  6%|▌         | 194/3108 [04:37<1:08:23,  1.41s/it]  6%|▋         | 195/3108 [04:39<1:08:21,  1.41s/it]  6%|▋         | 196/3108 [04:40<1:08:22,  1.41s/it]  6%|▋         | 197/3108 [04:41<1:08:18,  1.41s/it]  6%|▋         | 198/3108 [04:43<1:08:12,  1.41s/it]  6%|▋         | 199/3108 [04:44<1:08:16,  1.41s/it]  6%|▋         | 200/3108 [04:46<1:08:19,  1.41s/it]  6%|▋         | 201/3108 [04:47<1:08:13,  1.41s/it]  6%|▋         | 202/3108 [04:48<1:08:06,  1.41s/it]  7%|▋         | 203/3108 [04:50<1:09:37,  1.44s/it]  7%|▋         | 204/3108 [04:51<1:09:03,  1.43s/it]  7%|▋         | 205/3108 [04:53<1:08:35,  1.42s/it]  7%|▋         | 206/3108 [04:54<1:08:13,  1.41s/it]  7%|▋         | 207/3108 [04:56<1:08:12,  1.41s/it]  7%|▋         | 208/3108 [04:57<1:08:12,  1.41s/it]  7%|▋         | 209/3108 [04:58<1:09:33,  1.44s/it]  7%|▋         | 210/3108 [05:00<1:09:54,  1.45s/it]  7%|▋         | 211/3108 [05:01<1:09:14,  1.43s/it]  7%|▋         | 212/3108 [05:03<1:08:39,  1.42s/it]  7%|▋         | 213/3108 [05:04<1:08:23,  1.42s/it]  7%|▋         | 214/3108 [05:06<1:08:02,  1.41s/it]  7%|▋         | 215/3108 [05:07<1:08:05,  1.41s/it]  7%|▋         | 216/3108 [05:08<1:07:56,  1.41s/it]  7%|▋         | 217/3108 [05:10<1:07:43,  1.41s/it]  7%|▋         | 218/3108 [05:11<1:07:38,  1.40s/it]  7%|▋         | 219/3108 [05:13<1:07:39,  1.41s/it]  7%|▋         | 220/3108 [05:14<1:07:41,  1.41s/it]  7%|▋         | 221/3108 [05:15<1:07:29,  1.40s/it]  7%|▋         | 222/3108 [05:17<1:07:39,  1.41s/it]  7%|▋         | 223/3108 [05:18<1:07:42,  1.41s/it]  7%|▋         | 224/3108 [05:20<1:07:22,  1.40s/it]  7%|▋         | 225/3108 [05:21<1:07:20,  1.40s/it]  7%|▋         | 226/3108 [05:22<1:07:28,  1.40s/it]  7%|▋         | 227/3108 [05:24<1:07:34,  1.41s/it]  7%|▋         | 228/3108 [05:25<1:09:06,  1.44s/it]  7%|▋         | 229/3108 [05:27<1:08:40,  1.43s/it]  7%|▋         | 230/3108 [05:28<1:08:22,  1.43s/it]  7%|▋         | 231/3108 [05:30<1:08:13,  1.42s/it]  7%|▋         | 232/3108 [05:31<1:07:51,  1.42s/it]  7%|▋         | 233/3108 [05:32<1:07:33,  1.41s/it]  8%|▊         | 234/3108 [05:34<1:07:35,  1.41s/it]  8%|▊         | 235/3108 [05:35<1:07:36,  1.41s/it]  8%|▊         | 236/3108 [05:37<1:07:30,  1.41s/it]  8%|▊         | 237/3108 [05:38<1:07:10,  1.40s/it]  8%|▊         | 238/3108 [05:39<1:07:07,  1.40s/it]  8%|▊         | 239/3108 [05:41<1:08:29,  1.43s/it]  8%|▊         | 240/3108 [05:42<1:08:55,  1.44s/it]  8%|▊         | 241/3108 [05:44<1:08:28,  1.43s/it]  8%|▊         | 242/3108 [05:45<1:08:08,  1.43s/it]  8%|▊         | 243/3108 [05:47<1:07:50,  1.42s/it]  8%|▊         | 244/3108 [05:48<1:07:25,  1.41s/it]  8%|▊         | 245/3108 [05:49<1:07:27,  1.41s/it]  8%|▊         | 246/3108 [05:51<1:07:18,  1.41s/it]  8%|▊         | 247/3108 [05:52<1:07:17,  1.41s/it]  8%|▊         | 248/3108 [05:54<1:07:02,  1.41s/it]  8%|▊         | 249/3108 [05:55<1:07:03,  1.41s/it]  8%|▊         | 250/3108 [05:56<1:07:07,  1.41s/it]  8%|▊         | 251/3108 [05:58<1:07:10,  1.41s/it]  8%|▊         | 252/3108 [05:59<1:07:10,  1.41s/it]  8%|▊         | 253/3108 [06:01<1:07:09,  1.41s/it]  8%|▊         | 254/3108 [06:02<1:08:26,  1.44s/it]  8%|▊         | 255/3108 [06:04<1:07:52,  1.43s/it]  8%|▊         | 256/3108 [06:05<1:07:37,  1.42s/it]  8%|▊         | 257/3108 [06:06<1:07:28,  1.42s/it]  8%|▊         | 258/3108 [06:08<1:07:24,  1.42s/it]  8%|▊         | 259/3108 [06:09<1:07:15,  1.42s/it]  8%|▊         | 260/3108 [06:11<1:07:10,  1.42s/it]  8%|▊         | 261/3108 [06:12<1:06:59,  1.41s/it]  8%|▊         | 262/3108 [06:13<1:06:50,  1.41s/it]  8%|▊         | 263/3108 [06:15<1:06:47,  1.41s/it]  8%|▊         | 264/3108 [06:16<1:06:48,  1.41s/it]  9%|▊         | 265/3108 [06:18<1:06:45,  1.41s/it]  9%|▊         | 266/3108 [06:19<1:06:39,  1.41s/it]  9%|▊         | 267/3108 [06:20<1:06:41,  1.41s/it]  9%|▊         | 268/3108 [06:22<1:06:40,  1.41s/it]  9%|▊         | 269/3108 [06:23<1:07:56,  1.44s/it]  9%|▊         | 270/3108 [06:25<1:08:36,  1.45s/it]  9%|▊         | 271/3108 [06:26<1:07:56,  1.44s/it]  9%|▉         | 272/3108 [06:28<1:07:32,  1.43s/it]  9%|▉         | 273/3108 [06:29<1:07:20,  1.43s/it]  9%|▉         | 274/3108 [06:31<1:07:10,  1.42s/it]  9%|▉         | 275/3108 [06:32<1:07:01,  1.42s/it]  9%|▉         | 276/3108 [06:33<1:06:52,  1.42s/it]  9%|▉         | 277/3108 [06:35<1:06:45,  1.41s/it]  9%|▉         | 278/3108 [06:36<1:06:40,  1.41s/it]  9%|▉         | 279/3108 [06:38<1:07:48,  1.44s/it]  9%|▉         | 280/3108 [06:39<1:07:20,  1.43s/it]  9%|▉         | 281/3108 [06:40<1:07:07,  1.42s/it]  9%|▉         | 282/3108 [06:42<1:06:51,  1.42s/it]  9%|▉         | 283/3108 [06:43<1:06:47,  1.42s/it]  9%|▉         | 284/3108 [06:45<1:06:41,  1.42s/it]  9%|▉         | 285/3108 [06:46<1:06:34,  1.42s/it]  9%|▉         | 286/3108 [06:48<1:06:30,  1.41s/it]  9%|▉         | 287/3108 [06:49<1:06:23,  1.41s/it]  9%|▉         | 288/3108 [06:50<1:06:15,  1.41s/it]  9%|▉         | 289/3108 [06:52<1:06:14,  1.41s/it]  9%|▉         | 290/3108 [06:53<1:06:01,  1.41s/it]  9%|▉         | 291/3108 [06:55<1:06:01,  1.41s/it]  9%|▉         | 292/3108 [06:56<1:05:59,  1.41s/it]  9%|▉         | 293/3108 [06:57<1:06:05,  1.41s/it]  9%|▉         | 294/3108 [06:59<1:06:07,  1.41s/it]  9%|▉         | 295/3108 [07:00<1:06:06,  1.41s/it] 10%|▉         | 296/3108 [07:02<1:06:10,  1.41s/it] 10%|▉         | 297/3108 [07:03<1:06:04,  1.41s/it] 10%|▉         | 298/3108 [07:04<1:05:57,  1.41s/it] 10%|▉         | 299/3108 [07:06<1:07:08,  1.43s/it] 10%|▉         | 300/3108 [07:07<1:07:33,  1.44s/it] 10%|▉         | 301/3108 [07:09<1:06:56,  1.43s/it] 10%|▉         | 302/3108 [07:10<1:06:25,  1.42s/it] 10%|▉         | 303/3108 [07:12<1:06:05,  1.41s/it] 10%|▉         | 304/3108 [07:13<1:07:24,  1.44s/it] 10%|▉         | 305/3108 [07:15<1:06:57,  1.43s/it] 10%|▉         | 306/3108 [07:16<1:06:39,  1.43s/it] 10%|▉         | 307/3108 [07:17<1:06:24,  1.42s/it] 10%|▉         | 308/3108 [07:19<1:06:14,  1.42s/it] 10%|▉         | 309/3108 [07:20<1:06:08,  1.42s/it] 10%|▉         | 310/3108 [07:22<1:06:02,  1.42s/it] 10%|█         | 311/3108 [07:23<1:05:57,  1.41s/it] 10%|█         | 312/3108 [07:24<1:05:52,  1.41s/it] 10%|█         | 313/3108 [07:26<1:05:52,  1.41s/it] 10%|█         | 314/3108 [07:27<1:05:46,  1.41s/it] 10%|█         | 315/3108 [07:29<1:05:31,  1.41s/it] 10%|█         | 316/3108 [07:30<1:05:23,  1.41s/it] 10%|█         | 317/3108 [07:31<1:05:27,  1.41s/it] 10%|█         | 318/3108 [07:33<1:05:32,  1.41s/it] 10%|█         | 319/3108 [07:34<1:05:35,  1.41s/it] 10%|█         | 320/3108 [07:36<1:05:29,  1.41s/it] 10%|█         | 321/3108 [07:37<1:05:26,  1.41s/it] 10%|█         | 322/3108 [07:38<1:05:18,  1.41s/it] 10%|█         | 323/3108 [07:40<1:05:13,  1.41s/it] 10%|█         | 324/3108 [07:41<1:05:11,  1.41s/it] 10%|█         | 325/3108 [07:43<1:05:13,  1.41s/it] 10%|█         | 326/3108 [07:44<1:05:08,  1.41s/it] 11%|█         | 327/3108 [07:46<1:05:06,  1.40s/it] 11%|█         | 328/3108 [07:47<1:05:09,  1.41s/it] 11%|█         | 329/3108 [07:48<1:06:42,  1.44s/it] 11%|█         | 330/3108 [07:50<1:07:21,  1.45s/it] 11%|█         | 331/3108 [07:51<1:06:41,  1.44s/it] 11%|█         | 332/3108 [07:53<1:06:04,  1.43s/it] 11%|█         | 333/3108 [07:54<1:05:48,  1.42s/it] 11%|█         | 334/3108 [07:56<1:05:35,  1.42s/it] 11%|█         | 335/3108 [07:57<1:05:24,  1.42s/it] 11%|█         | 336/3108 [07:58<1:05:11,  1.41s/it] 11%|█         | 337/3108 [08:00<1:05:13,  1.41s/it] 11%|█         | 338/3108 [08:01<1:05:11,  1.41s/it] 11%|█         | 339/3108 [08:03<1:05:06,  1.41s/it] 11%|█         | 340/3108 [08:04<1:05:02,  1.41s/it] 11%|█         | 341/3108 [08:05<1:05:08,  1.41s/it] 11%|█         | 342/3108 [08:07<1:05:04,  1.41s/it] 11%|█         | 343/3108 [08:08<1:05:05,  1.41s/it] 11%|█         | 344/3108 [08:10<1:05:01,  1.41s/it] 11%|█         | 345/3108 [08:11<1:04:59,  1.41s/it] 11%|█         | 346/3108 [08:12<1:04:54,  1.41s/it] 11%|█         | 347/3108 [08:14<1:04:49,  1.41s/it] 11%|█         | 348/3108 [08:15<1:04:37,  1.41s/it] 11%|█         | 349/3108 [08:17<1:04:38,  1.41s/it] 11%|█▏        | 350/3108 [08:18<1:04:36,  1.41s/it] 11%|█▏        | 351/3108 [08:20<1:04:41,  1.41s/it] 11%|█▏        | 352/3108 [08:21<1:04:42,  1.41s/it] 11%|█▏        | 353/3108 [08:22<1:04:40,  1.41s/it] 11%|█▏        | 354/3108 [08:24<1:06:10,  1.44s/it] 11%|█▏        | 355/3108 [08:25<1:05:44,  1.43s/it] 11%|█▏        | 356/3108 [08:27<1:05:23,  1.43s/it] 11%|█▏        | 357/3108 [08:28<1:05:10,  1.42s/it] 12%|█▏        | 358/3108 [08:29<1:04:50,  1.41s/it] 12%|█▏        | 359/3108 [08:31<1:06:02,  1.44s/it] 12%|█▏        | 360/3108 [08:32<1:06:32,  1.45s/it] 12%|█▏        | 361/3108 [08:34<1:05:54,  1.44s/it] 12%|█▏        | 362/3108 [08:35<1:05:19,  1.43s/it] 12%|█▏        | 363/3108 [08:37<1:05:07,  1.42s/it] 12%|█▏        | 364/3108 [08:38<1:04:53,  1.42s/it] 12%|█▏        | 365/3108 [08:39<1:04:33,  1.41s/it] 12%|█▏        | 366/3108 [08:41<1:04:33,  1.41s/it] 12%|█▏        | 367/3108 [08:42<1:04:31,  1.41s/it] 12%|█▏        | 368/3108 [08:44<1:04:14,  1.41s/it] 12%|█▏        | 369/3108 [08:45<1:04:16,  1.41s/it] 12%|█▏        | 370/3108 [08:47<1:04:06,  1.40s/it] 12%|█▏        | 371/3108 [08:48<1:04:17,  1.41s/it] 12%|█▏        | 372/3108 [08:49<1:04:12,  1.41s/it] 12%|█▏        | 373/3108 [08:51<1:04:13,  1.41s/it] 12%|█▏        | 374/3108 [08:52<1:04:11,  1.41s/it] 12%|█▏        | 375/3108 [08:54<1:04:00,  1.41s/it] 12%|█▏        | 376/3108 [08:55<1:03:57,  1.40s/it] 12%|█▏        | 377/3108 [08:56<1:03:46,  1.40s/it] 12%|█▏        | 378/3108 [08:58<1:03:44,  1.40s/it] 12%|█▏        | 379/3108 [08:59<1:03:50,  1.40s/it] 12%|█▏        | 380/3108 [09:01<1:06:03,  1.45s/it] 12%|█▏        | 381/3108 [09:02<1:05:27,  1.44s/it] 12%|█▏        | 382/3108 [09:04<1:04:48,  1.43s/it] 12%|█▏        | 383/3108 [09:05<1:04:34,  1.42s/it] 12%|█▏        | 384/3108 [09:06<1:04:29,  1.42s/it] 12%|█▏        | 385/3108 [09:08<1:04:19,  1.42s/it] 12%|█▏        | 386/3108 [09:09<1:04:12,  1.42s/it] 12%|█▏        | 387/3108 [09:11<1:04:07,  1.41s/it] 12%|█▏        | 388/3108 [09:12<1:04:06,  1.41s/it] 13%|█▎        | 389/3108 [09:13<1:05:11,  1.44s/it] 13%|█▎        | 390/3108 [09:15<1:05:41,  1.45s/it] 13%|█▎        | 391/3108 [09:16<1:05:06,  1.44s/it] 13%|█▎        | 392/3108 [09:18<1:04:41,  1.43s/it] 13%|█▎        | 393/3108 [09:19<1:04:28,  1.42s/it] 13%|█▎        | 394/3108 [09:21<1:04:09,  1.42s/it] 13%|█▎        | 395/3108 [09:22<1:03:46,  1.41s/it] 13%|█▎        | 396/3108 [09:23<1:03:50,  1.41s/it] 13%|█▎        | 397/3108 [09:25<1:03:52,  1.41s/it] 13%|█▎        | 398/3108 [09:26<1:03:45,  1.41s/it] 13%|█▎        | 399/3108 [09:28<1:03:44,  1.41s/it] 13%|█▎        | 400/3108 [09:29<1:03:44,  1.41s/it] 13%|█▎        | 401/3108 [09:30<1:03:39,  1.41s/it] 13%|█▎        | 402/3108 [09:32<1:03:30,  1.41s/it] 13%|█▎        | 403/3108 [09:33<1:03:33,  1.41s/it] 13%|█▎        | 404/3108 [09:35<1:03:32,  1.41s/it] 13%|█▎        | 405/3108 [09:36<1:04:52,  1.44s/it] 13%|█▎        | 406/3108 [09:38<1:04:18,  1.43s/it] 13%|█▎        | 407/3108 [09:39<1:04:00,  1.42s/it] 13%|█▎        | 408/3108 [09:40<1:03:53,  1.42s/it] 13%|█▎        | 409/3108 [09:42<1:03:49,  1.42s/it] 13%|█▎        | 410/3108 [09:43<1:03:36,  1.41s/it] 13%|█▎        | 411/3108 [09:45<1:03:19,  1.41s/it] 13%|█▎        | 412/3108 [09:46<1:03:09,  1.41s/it] 13%|█▎        | 413/3108 [09:47<1:03:11,  1.41s/it] 13%|█▎        | 414/3108 [09:49<1:03:17,  1.41s/it] 13%|█▎        | 415/3108 [09:50<1:03:22,  1.41s/it] 13%|█▎        | 416/3108 [09:52<1:03:17,  1.41s/it] 13%|█▎        | 417/3108 [09:53<1:03:10,  1.41s/it] 13%|█▎        | 418/3108 [09:55<1:03:04,  1.41s/it] 13%|█▎        | 419/3108 [09:56<1:04:26,  1.44s/it] 14%|█▎        | 420/3108 [09:57<1:04:58,  1.45s/it] 14%|█▎        | 421/3108 [09:59<1:04:30,  1.44s/it] 14%|█▎        | 422/3108 [10:00<1:03:57,  1.43s/it] 14%|█▎        | 423/3108 [10:02<1:03:35,  1.42s/it] 14%|█▎        | 424/3108 [10:03<1:03:27,  1.42s/it] 14%|█▎        | 425/3108 [10:05<1:03:20,  1.42s/it] 14%|█▎        | 426/3108 [10:06<1:03:04,  1.41s/it] 14%|█▎        | 427/3108 [10:07<1:02:58,  1.41s/it] 14%|█▍        | 428/3108 [10:09<1:02:57,  1.41s/it] 14%|█▍        | 429/3108 [10:10<1:02:58,  1.41s/it] 14%|█▍        | 430/3108 [10:12<1:04:19,  1.44s/it] 14%|█▍        | 431/3108 [10:13<1:03:56,  1.43s/it] 14%|█▍        | 432/3108 [10:15<1:03:31,  1.42s/it] 14%|█▍        | 433/3108 [10:16<1:03:26,  1.42s/it] 14%|█▍        | 434/3108 [10:17<1:03:17,  1.42s/it] 14%|█▍        | 435/3108 [10:19<1:03:00,  1.41s/it] 14%|█▍        | 436/3108 [10:20<1:02:47,  1.41s/it] 14%|█▍        | 437/3108 [10:22<1:02:44,  1.41s/it] 14%|█▍        | 438/3108 [10:23<1:02:44,  1.41s/it] 14%|█▍        | 439/3108 [10:24<1:02:48,  1.41s/it] 14%|█▍        | 440/3108 [10:26<1:02:48,  1.41s/it] 14%|█▍        | 441/3108 [10:27<1:02:46,  1.41s/it] 14%|█▍        | 442/3108 [10:29<1:02:45,  1.41s/it] 14%|█▍        | 443/3108 [10:30<1:02:46,  1.41s/it] 14%|█▍        | 444/3108 [10:31<1:02:43,  1.41s/it] 14%|█▍        | 445/3108 [10:33<1:02:39,  1.41s/it] 14%|█▍        | 446/3108 [10:34<1:02:36,  1.41s/it] 14%|█▍        | 447/3108 [10:36<1:02:34,  1.41s/it] 14%|█▍        | 448/3108 [10:37<1:02:37,  1.41s/it] 14%|█▍        | 449/3108 [10:39<1:03:57,  1.44s/it] 14%|█▍        | 450/3108 [10:40<1:04:26,  1.45s/it] 15%|█▍        | 451/3108 [10:41<1:03:42,  1.44s/it] 15%|█▍        | 452/3108 [10:43<1:03:07,  1.43s/it] 15%|█▍        | 453/3108 [10:44<1:02:52,  1.42s/it] 15%|█▍        | 454/3108 [10:46<1:02:44,  1.42s/it] 15%|█▍        | 455/3108 [10:47<1:03:59,  1.45s/it] 15%|█▍        | 456/3108 [10:49<1:03:32,  1.44s/it] 15%|█▍        | 457/3108 [10:50<1:03:10,  1.43s/it] 15%|█▍        | 458/3108 [10:51<1:03:00,  1.43s/it] 15%|█▍        | 459/3108 [10:53<1:02:45,  1.42s/it] 15%|█▍        | 460/3108 [10:54<1:02:33,  1.42s/it] 15%|█▍        | 461/3108 [10:56<1:02:32,  1.42s/it] 15%|█▍        | 462/3108 [10:57<1:02:27,  1.42s/it] 15%|█▍        | 463/3108 [10:59<1:02:23,  1.42s/it] 15%|█▍        | 464/3108 [11:00<1:02:14,  1.41s/it] 15%|█▍        | 465/3108 [11:01<1:02:19,  1.41s/it] 15%|█▍        | 466/3108 [11:03<1:02:15,  1.41s/it] 15%|█▌        | 467/3108 [11:04<1:02:12,  1.41s/it] 15%|█▌        | 468/3108 [11:06<1:02:04,  1.41s/it] 15%|█▌        | 469/3108 [11:07<1:02:07,  1.41s/it] 15%|█▌        | 470/3108 [11:08<1:02:04,  1.41s/it] 15%|█▌        | 471/3108 [11:10<1:02:06,  1.41s/it] 15%|█▌        | 472/3108 [11:11<1:02:03,  1.41s/it] 15%|█▌        | 473/3108 [11:13<1:01:56,  1.41s/it] 15%|█▌        | 474/3108 [11:14<1:02:03,  1.41s/it] 15%|█▌        | 475/3108 [11:15<1:02:00,  1.41s/it] 15%|█▌        | 476/3108 [11:17<1:01:55,  1.41s/it] 15%|█▌        | 477/3108 [11:18<1:01:50,  1.41s/it] 15%|█▌        | 478/3108 [11:20<1:01:42,  1.41s/it] 15%|█▌        | 479/3108 [11:21<1:02:58,  1.44s/it] 15%|█▌        | 480/3108 [11:23<1:03:55,  1.46s/it] 15%|█▌        | 481/3108 [11:24<1:03:16,  1.45s/it] 16%|█▌        | 482/3108 [11:26<1:02:43,  1.43s/it] 16%|█▌        | 483/3108 [11:27<1:02:16,  1.42s/it] 16%|█▌        | 484/3108 [11:28<1:02:04,  1.42s/it] 16%|█▌        | 485/3108 [11:30<1:01:52,  1.42s/it] 16%|█▌        | 486/3108 [11:31<1:01:51,  1.42s/it] 16%|█▌        | 487/3108 [11:33<1:01:50,  1.42s/it] 16%|█▌        | 488/3108 [11:34<1:01:39,  1.41s/it] 16%|█▌        | 489/3108 [11:35<1:01:30,  1.41s/it] 16%|█▌        | 490/3108 [11:37<1:01:30,  1.41s/it] 16%|█▌        | 491/3108 [11:38<1:01:33,  1.41s/it] 16%|█▌        | 492/3108 [11:40<1:01:33,  1.41s/it] 16%|█▌        | 493/3108 [11:41<1:01:31,  1.41s/it] 16%|█▌        | 494/3108 [11:42<1:01:24,  1.41s/it] 16%|█▌        | 495/3108 [11:44<1:01:13,  1.41s/it] 16%|█▌        | 496/3108 [11:45<1:01:19,  1.41s/it] 16%|█▌        | 497/3108 [11:47<1:01:15,  1.41s/it] 16%|█▌        | 498/3108 [11:48<1:01:16,  1.41s/it] 16%|█▌        | 499/3108 [11:49<1:01:25,  1.41s/it] 16%|█▌        | 500/3108 [11:51<1:01:15,  1.41s/it]                                                    loss: 1.91866882, learning_rate: 1.766e-05, global_step: 500, interval_runtime: 711.4363, interval_samples_per_second: 12.650464243531784, interval_steps_per_second: 0.7028035690850992, epoch: 0.6435
 16%|█▌        | 500/3108 [11:51<1:01:15,  1.41s/it] 16%|█▌        | 501/3108 [11:52<1:01:57,  1.43s/it] 16%|█▌        | 502/3108 [11:54<1:01:42,  1.42s/it] 16%|█▌        | 503/3108 [11:55<1:01:36,  1.42s/it] 16%|█▌        | 504/3108 [11:57<1:01:32,  1.42s/it] 16%|█▌        | 505/3108 [11:58<1:01:25,  1.42s/it] 16%|█▋        | 506/3108 [12:00<1:02:37,  1.44s/it] 16%|█▋        | 507/3108 [12:01<1:02:01,  1.43s/it] 16%|█▋        | 508/3108 [12:02<1:01:46,  1.43s/it] 16%|█▋        | 509/3108 [12:04<1:02:45,  1.45s/it] 16%|█▋        | 510/3108 [12:05<1:03:00,  1.46s/it] 16%|█▋        | 511/3108 [12:07<1:02:23,  1.44s/it] 16%|█▋        | 512/3108 [12:08<1:01:59,  1.43s/it] 17%|█▋        | 513/3108 [12:10<1:01:38,  1.43s/it] 17%|█▋        | 514/3108 [12:11<1:01:26,  1.42s/it] 17%|█▋        | 515/3108 [12:12<1:01:22,  1.42s/it] 17%|█▋        | 516/3108 [12:14<1:01:11,  1.42s/it] 17%|█▋        | 517/3108 [12:15<1:01:10,  1.42s/it] 17%|█▋        | 518/3108 [12:17<1:00:59,  1.41s/it] 17%|█▋        | 519/3108 [12:18<1:01:00,  1.41s/it] 17%|█▋        | 520/3108 [12:19<1:01:01,  1.41s/it] 17%|█▋        | 521/3108 [12:21<1:00:57,  1.41s/it] 17%|█▋        | 522/3108 [12:22<1:00:51,  1.41s/it] 17%|█▋        | 523/3108 [12:24<1:00:51,  1.41s/it] 17%|█▋        | 524/3108 [12:25<1:00:45,  1.41s/it] 17%|█▋        | 525/3108 [12:26<1:00:51,  1.41s/it] 17%|█▋        | 526/3108 [12:28<1:00:49,  1.41s/it] 17%|█▋        | 527/3108 [12:29<1:00:38,  1.41s/it] 17%|█▋        | 528/3108 [12:31<1:00:40,  1.41s/it] 17%|█▋        | 529/3108 [12:32<1:00:39,  1.41s/it] 17%|█▋        | 530/3108 [12:34<1:00:41,  1.41s/it] 17%|█▋        | 531/3108 [12:35<1:01:55,  1.44s/it] 17%|█▋        | 532/3108 [12:36<1:01:27,  1.43s/it] 17%|█▋        | 533/3108 [12:38<1:01:03,  1.42s/it] 17%|█▋        | 534/3108 [12:39<1:00:47,  1.42s/it] 17%|█▋        | 535/3108 [12:41<1:00:44,  1.42s/it] 17%|█▋        | 536/3108 [12:42<1:00:40,  1.42s/it] 17%|█▋        | 537/3108 [12:43<1:00:35,  1.41s/it] 17%|█▋        | 538/3108 [12:45<1:00:29,  1.41s/it] 17%|█▋        | 539/3108 [12:46<1:01:34,  1.44s/it] 17%|█▋        | 540/3108 [12:48<1:01:57,  1.45s/it] 17%|█▋        | 541/3108 [12:49<1:01:23,  1.43s/it] 17%|█▋        | 542/3108 [12:51<1:00:56,  1.43s/it] 17%|█▋        | 543/3108 [12:52<1:00:46,  1.42s/it] 18%|█▊        | 544/3108 [12:54<1:00:37,  1.42s/it] 18%|█▊        | 545/3108 [12:55<1:00:32,  1.42s/it] 18%|█▊        | 546/3108 [12:56<1:00:29,  1.42s/it] 18%|█▊        | 547/3108 [12:58<1:00:21,  1.41s/it] 18%|█▊        | 548/3108 [12:59<1:00:16,  1.41s/it] 18%|█▊        | 549/3108 [13:01<1:00:04,  1.41s/it] 18%|█▊        | 550/3108 [13:02<1:00:05,  1.41s/it] 18%|█▊        | 551/3108 [13:03<1:00:08,  1.41s/it] 18%|█▊        | 552/3108 [13:05<1:00:10,  1.41s/it] 18%|█▊        | 553/3108 [13:06<1:00:06,  1.41s/it] 18%|█▊        | 554/3108 [13:08<1:00:09,  1.41s/it] 18%|█▊        | 555/3108 [13:09<1:00:02,  1.41s/it] 18%|█▊        | 556/3108 [13:11<1:01:24,  1.44s/it] 18%|█▊        | 557/3108 [13:12<1:00:59,  1.43s/it] 18%|█▊        | 558/3108 [13:13<1:00:44,  1.43s/it] 18%|█▊        | 559/3108 [13:15<1:00:24,  1.42s/it] 18%|█▊        | 560/3108 [13:16<1:00:17,  1.42s/it] 18%|█▊        | 561/3108 [13:18<1:00:05,  1.42s/it] 18%|█▊        | 562/3108 [13:19<59:52,  1.41s/it]   18%|█▊        | 563/3108 [13:20<59:44,  1.41s/it] 18%|█▊        | 564/3108 [13:22<59:42,  1.41s/it] 18%|█▊        | 565/3108 [13:23<59:31,  1.40s/it] 18%|█▊        | 566/3108 [13:25<59:32,  1.41s/it] 18%|█▊        | 567/3108 [13:26<59:29,  1.40s/it] 18%|█▊        | 568/3108 [13:27<59:31,  1.41s/it] 18%|█▊        | 569/3108 [13:29<1:00:41,  1.43s/it] 18%|█▊        | 570/3108 [13:30<1:01:09,  1.45s/it] 18%|█▊        | 571/3108 [13:32<1:00:37,  1.43s/it] 18%|█▊        | 572/3108 [13:33<1:00:16,  1.43s/it] 18%|█▊        | 573/3108 [13:35<59:57,  1.42s/it]   18%|█▊        | 574/3108 [13:36<59:45,  1.42s/it] 19%|█▊        | 575/3108 [13:37<59:34,  1.41s/it] 19%|█▊        | 576/3108 [13:39<59:30,  1.41s/it] 19%|█▊        | 577/3108 [13:40<59:30,  1.41s/it] 19%|█▊        | 578/3108 [13:42<59:31,  1.41s/it] 19%|█▊        | 579/3108 [13:43<59:30,  1.41s/it] 19%|█▊        | 580/3108 [13:44<59:22,  1.41s/it] 19%|█▊        | 581/3108 [13:46<1:00:38,  1.44s/it] 19%|█▊        | 582/3108 [13:47<1:00:06,  1.43s/it] 19%|█▉        | 583/3108 [13:49<59:56,  1.42s/it]   19%|█▉        | 584/3108 [13:50<59:41,  1.42s/it] 19%|█▉        | 585/3108 [13:52<59:39,  1.42s/it] 19%|█▉        | 586/3108 [13:53<59:32,  1.42s/it] 19%|█▉        | 587/3108 [13:54<59:24,  1.41s/it] 19%|█▉        | 588/3108 [13:56<59:22,  1.41s/it] 19%|█▉        | 589/3108 [13:57<59:24,  1.42s/it] 19%|█▉        | 590/3108 [13:59<59:18,  1.41s/it] 19%|█▉        | 591/3108 [14:00<59:13,  1.41s/it] 19%|█▉        | 592/3108 [14:02<59:15,  1.41s/it] 19%|█▉        | 593/3108 [14:03<59:10,  1.41s/it] 19%|█▉        | 594/3108 [14:04<59:12,  1.41s/it] 19%|█▉        | 595/3108 [14:06<59:10,  1.41s/it] 19%|█▉        | 596/3108 [14:07<59:11,  1.41s/it] 19%|█▉        | 597/3108 [14:09<59:05,  1.41s/it] 19%|█▉        | 598/3108 [14:10<59:05,  1.41s/it] 19%|█▉        | 599/3108 [14:11<1:00:08,  1.44s/it] 19%|█▉        | 600/3108 [14:13<1:00:34,  1.45s/it] 19%|█▉        | 601/3108 [14:14<1:00:05,  1.44s/it] 19%|█▉        | 602/3108 [14:16<59:45,  1.43s/it]   19%|█▉        | 603/3108 [14:17<59:28,  1.42s/it] 19%|█▉        | 604/3108 [14:19<59:23,  1.42s/it] 19%|█▉        | 605/3108 [14:20<59:13,  1.42s/it] 19%|█▉        | 606/3108 [14:22<1:00:24,  1.45s/it] 20%|█▉        | 607/3108 [14:23<59:59,  1.44s/it]   20%|█▉        | 608/3108 [14:24<59:24,  1.43s/it] 20%|█▉        | 609/3108 [14:26<59:11,  1.42s/it] 20%|█▉        | 610/3108 [14:27<58:59,  1.42s/it] 20%|█▉        | 611/3108 [14:29<58:56,  1.42s/it] 20%|█▉        | 612/3108 [14:30<58:53,  1.42s/it] 20%|█▉        | 613/3108 [14:31<58:43,  1.41s/it] 20%|█▉        | 614/3108 [14:33<58:36,  1.41s/it] 20%|█▉        | 615/3108 [14:34<58:28,  1.41s/it] 20%|█▉        | 616/3108 [14:36<58:22,  1.41s/it] 20%|█▉        | 617/3108 [14:37<58:24,  1.41s/it] 20%|█▉        | 618/3108 [14:38<58:28,  1.41s/it] 20%|█▉        | 619/3108 [14:40<58:31,  1.41s/it] 20%|█▉        | 620/3108 [14:41<58:26,  1.41s/it] 20%|█▉        | 621/3108 [14:43<58:14,  1.41s/it] 20%|██        | 622/3108 [14:44<58:15,  1.41s/it] 20%|██        | 623/3108 [14:45<58:18,  1.41s/it] 20%|██        | 624/3108 [14:47<58:18,  1.41s/it] 20%|██        | 625/3108 [14:48<58:23,  1.41s/it] 20%|██        | 626/3108 [14:50<58:21,  1.41s/it] 20%|██        | 627/3108 [14:51<58:12,  1.41s/it] 20%|██        | 628/3108 [14:53<58:12,  1.41s/it] 20%|██        | 629/3108 [14:54<59:12,  1.43s/it] 20%|██        | 630/3108 [14:55<59:46,  1.45s/it] 20%|██        | 631/3108 [14:57<59:18,  1.44s/it] 20%|██        | 632/3108 [14:58<1:00:03,  1.46s/it] 20%|██        | 633/3108 [15:00<59:29,  1.44s/it]   20%|██        | 634/3108 [15:01<59:04,  1.43s/it] 20%|██        | 635/3108 [15:03<58:45,  1.43s/it] 20%|██        | 636/3108 [15:04<58:36,  1.42s/it] 20%|██        | 637/3108 [15:05<58:29,  1.42s/it] 21%|██        | 638/3108 [15:07<58:20,  1.42s/it] 21%|██        | 639/3108 [15:08<58:13,  1.42s/it] 21%|██        | 640/3108 [15:10<58:14,  1.42s/it] 21%|██        | 641/3108 [15:11<58:08,  1.41s/it] 21%|██        | 642/3108 [15:13<58:09,  1.41s/it] 21%|██        | 643/3108 [15:14<58:02,  1.41s/it] 21%|██        | 644/3108 [15:15<57:58,  1.41s/it] 21%|██        | 645/3108 [15:17<57:49,  1.41s/it] 21%|██        | 646/3108 [15:18<57:48,  1.41s/it] 21%|██        | 647/3108 [15:20<57:52,  1.41s/it] 21%|██        | 648/3108 [15:21<57:51,  1.41s/it] 21%|██        | 649/3108 [15:22<57:50,  1.41s/it] 21%|██        | 650/3108 [15:24<57:42,  1.41s/it] 21%|██        | 651/3108 [15:25<57:36,  1.41s/it] 21%|██        | 652/3108 [15:27<57:31,  1.41s/it] 21%|██        | 653/3108 [15:28<57:33,  1.41s/it] 21%|██        | 654/3108 [15:29<57:39,  1.41s/it] 21%|██        | 655/3108 [15:31<57:36,  1.41s/it] 21%|██        | 656/3108 [15:32<57:38,  1.41s/it] 21%|██        | 657/3108 [15:34<58:48,  1.44s/it] 21%|██        | 658/3108 [15:35<58:24,  1.43s/it] 21%|██        | 659/3108 [15:37<59:18,  1.45s/it] 21%|██        | 660/3108 [15:38<59:29,  1.46s/it] 21%|██▏       | 661/3108 [15:40<58:47,  1.44s/it] 21%|██▏       | 662/3108 [15:41<58:21,  1.43s/it] 21%|██▏       | 663/3108 [15:42<57:54,  1.42s/it] 21%|██▏       | 664/3108 [15:44<57:43,  1.42s/it] 21%|██▏       | 665/3108 [15:45<57:40,  1.42s/it] 21%|██▏       | 666/3108 [15:47<57:35,  1.42s/it] 21%|██▏       | 667/3108 [15:48<57:31,  1.41s/it] 21%|██▏       | 668/3108 [15:49<57:25,  1.41s/it] 22%|██▏       | 669/3108 [15:51<57:15,  1.41s/it] 22%|██▏       | 670/3108 [15:52<57:16,  1.41s/it] 22%|██▏       | 671/3108 [15:54<57:17,  1.41s/it] 22%|██▏       | 672/3108 [15:55<57:18,  1.41s/it] 22%|██▏       | 673/3108 [15:56<57:09,  1.41s/it] 22%|██▏       | 674/3108 [15:58<56:59,  1.40s/it] 22%|██▏       | 675/3108 [15:59<56:55,  1.40s/it] 22%|██▏       | 676/3108 [16:01<56:56,  1.40s/it] 22%|██▏       | 677/3108 [16:02<56:58,  1.41s/it] 22%|██▏       | 678/3108 [16:03<57:00,  1.41s/it] 22%|██▏       | 679/3108 [16:05<57:04,  1.41s/it] 22%|██▏       | 680/3108 [16:06<57:01,  1.41s/it] 22%|██▏       | 681/3108 [16:08<56:56,  1.41s/it] 22%|██▏       | 682/3108 [16:09<58:06,  1.44s/it] 22%|██▏       | 683/3108 [16:11<57:46,  1.43s/it] 22%|██▏       | 684/3108 [16:12<57:30,  1.42s/it] 22%|██▏       | 685/3108 [16:13<57:18,  1.42s/it] 22%|██▏       | 686/3108 [16:15<57:04,  1.41s/it] 22%|██▏       | 687/3108 [16:16<56:58,  1.41s/it] 22%|██▏       | 688/3108 [16:18<56:53,  1.41s/it] 22%|██▏       | 689/3108 [16:19<58:04,  1.44s/it] 22%|██▏       | 690/3108 [16:21<58:34,  1.45s/it] 22%|██▏       | 691/3108 [16:22<58:02,  1.44s/it] 22%|██▏       | 692/3108 [16:23<57:40,  1.43s/it] 22%|██▏       | 693/3108 [16:25<57:21,  1.43s/it] 22%|██▏       | 694/3108 [16:26<56:59,  1.42s/it] 22%|██▏       | 695/3108 [16:28<56:55,  1.42s/it] 22%|██▏       | 696/3108 [16:29<56:52,  1.41s/it] 22%|██▏       | 697/3108 [16:31<56:50,  1.41s/it] 22%|██▏       | 698/3108 [16:32<56:44,  1.41s/it] 22%|██▏       | 699/3108 [16:33<56:37,  1.41s/it] 23%|██▎       | 700/3108 [16:35<56:34,  1.41s/it] 23%|██▎       | 701/3108 [16:36<56:39,  1.41s/it] 23%|██▎       | 702/3108 [16:38<56:38,  1.41s/it] 23%|██▎       | 703/3108 [16:39<56:34,  1.41s/it] 23%|██▎       | 704/3108 [16:40<56:29,  1.41s/it] 23%|██▎       | 705/3108 [16:42<56:20,  1.41s/it] 23%|██▎       | 706/3108 [16:43<56:20,  1.41s/it] 23%|██▎       | 707/3108 [16:45<57:36,  1.44s/it] 23%|██▎       | 708/3108 [16:46<57:09,  1.43s/it] 23%|██▎       | 709/3108 [16:48<56:53,  1.42s/it] 23%|██▎       | 710/3108 [16:49<56:36,  1.42s/it] 23%|██▎       | 711/3108 [16:50<56:25,  1.41s/it] 23%|██▎       | 712/3108 [16:52<56:19,  1.41s/it] 23%|██▎       | 713/3108 [16:53<56:16,  1.41s/it] 23%|██▎       | 714/3108 [16:55<56:16,  1.41s/it] 23%|██▎       | 715/3108 [16:56<56:18,  1.41s/it] 23%|██▎       | 716/3108 [16:57<56:12,  1.41s/it] 23%|██▎       | 717/3108 [16:59<56:03,  1.41s/it] 23%|██▎       | 718/3108 [17:00<55:58,  1.41s/it] 23%|██▎       | 719/3108 [17:02<57:08,  1.44s/it] 23%|██▎       | 720/3108 [17:03<57:45,  1.45s/it] 23%|██▎       | 721/3108 [17:05<57:15,  1.44s/it] 23%|██▎       | 722/3108 [17:06<56:51,  1.43s/it] 23%|██▎       | 723/3108 [17:07<56:28,  1.42s/it] 23%|██▎       | 724/3108 [17:09<56:20,  1.42s/it] 23%|██▎       | 725/3108 [17:10<56:15,  1.42s/it] 23%|██▎       | 726/3108 [17:12<56:11,  1.42s/it] 23%|██▎       | 727/3108 [17:13<55:59,  1.41s/it] 23%|██▎       | 728/3108 [17:14<55:52,  1.41s/it] 23%|██▎       | 729/3108 [17:16<55:49,  1.41s/it] 23%|██▎       | 730/3108 [17:17<55:54,  1.41s/it] 24%|██▎       | 731/3108 [17:19<55:51,  1.41s/it] 24%|██▎       | 732/3108 [17:20<57:02,  1.44s/it] 24%|██▎       | 733/3108 [17:22<56:41,  1.43s/it] 24%|██▎       | 734/3108 [17:23<56:19,  1.42s/it] 24%|██▎       | 735/3108 [17:24<56:01,  1.42s/it] 24%|██▎       | 736/3108 [17:26<55:54,  1.41s/it] 24%|██▎       | 737/3108 [17:27<55:43,  1.41s/it] 24%|██▎       | 738/3108 [17:29<55:40,  1.41s/it] 24%|██▍       | 739/3108 [17:30<55:40,  1.41s/it] 24%|██▍       | 740/3108 [17:31<55:27,  1.41s/it] 24%|██▍       | 741/3108 [17:33<55:19,  1.40s/it] 24%|██▍       | 742/3108 [17:34<55:14,  1.40s/it] 24%|██▍       | 743/3108 [17:36<55:17,  1.40s/it] 24%|██▍       | 744/3108 [17:37<55:15,  1.40s/it] 24%|██▍       | 745/3108 [17:38<55:20,  1.41s/it] 24%|██▍       | 746/3108 [17:40<55:19,  1.41s/it] 24%|██▍       | 747/3108 [17:41<55:26,  1.41s/it] 24%|██▍       | 748/3108 [17:43<55:19,  1.41s/it] 24%|██▍       | 749/3108 [17:44<56:22,  1.43s/it] 24%|██▍       | 750/3108 [17:46<56:47,  1.45s/it] 24%|██▍       | 751/3108 [17:47<56:17,  1.43s/it] 24%|██▍       | 752/3108 [17:48<55:55,  1.42s/it] 24%|██▍       | 753/3108 [17:50<55:35,  1.42s/it] 24%|██▍       | 754/3108 [17:51<55:30,  1.41s/it] 24%|██▍       | 755/3108 [17:53<55:27,  1.41s/it] 24%|██▍       | 756/3108 [17:54<55:21,  1.41s/it] 24%|██▍       | 757/3108 [17:55<55:22,  1.41s/it] 24%|██▍       | 758/3108 [17:57<56:30,  1.44s/it] 24%|██▍       | 759/3108 [17:58<56:10,  1.43s/it] 24%|██▍       | 760/3108 [18:00<55:44,  1.42s/it] 24%|██▍       | 761/3108 [18:01<55:27,  1.42s/it] 25%|██▍       | 762/3108 [18:03<55:24,  1.42s/it] 25%|██▍       | 763/3108 [18:04<55:15,  1.41s/it] 25%|██▍       | 764/3108 [18:05<55:12,  1.41s/it] 25%|██▍       | 765/3108 [18:07<55:05,  1.41s/it] 25%|██▍       | 766/3108 [18:08<54:56,  1.41s/it] 25%|██▍       | 767/3108 [18:10<54:57,  1.41s/it] 25%|██▍       | 768/3108 [18:11<54:54,  1.41s/it] 25%|██▍       | 769/3108 [18:12<54:57,  1.41s/it] 25%|██▍       | 770/3108 [18:14<54:51,  1.41s/it] 25%|██▍       | 771/3108 [18:15<54:46,  1.41s/it] 25%|██▍       | 772/3108 [18:17<53:15,  1.37s/it] 25%|██▍       | 773/3108 [18:18<49:41,  1.28s/it] 25%|██▍       | 774/3108 [18:19<47:05,  1.21s/it] 25%|██▍       | 775/3108 [18:20<45:20,  1.17s/it] 25%|██▍       | 776/3108 [18:21<44:02,  1.13s/it] 25%|██▌       | 777/3108 [18:22<40:27,  1.04s/it] 25%|██▌       | 778/3108 [18:25<1:08:00,  1.75s/it] 25%|██▌       | 779/3108 [18:27<1:04:57,  1.67s/it] 25%|██▌       | 780/3108 [18:28<1:02:40,  1.62s/it] 25%|██▌       | 781/3108 [18:29<1:00:18,  1.56s/it] 25%|██▌       | 782/3108 [18:31<59:33,  1.54s/it]   25%|██▌       | 783/3108 [18:32<58:00,  1.50s/it] 25%|██▌       | 784/3108 [18:34<56:52,  1.47s/it] 25%|██▌       | 785/3108 [18:35<56:09,  1.45s/it] 25%|██▌       | 786/3108 [18:37<55:29,  1.43s/it] 25%|██▌       | 787/3108 [18:38<55:01,  1.42s/it] 25%|██▌       | 788/3108 [18:39<54:40,  1.41s/it] 25%|██▌       | 789/3108 [18:41<54:35,  1.41s/it] 25%|██▌       | 790/3108 [18:42<54:23,  1.41s/it] 25%|██▌       | 791/3108 [18:44<54:25,  1.41s/it] 25%|██▌       | 792/3108 [18:45<54:25,  1.41s/it] 26%|██▌       | 793/3108 [18:46<54:26,  1.41s/it] 26%|██▌       | 794/3108 [18:48<54:12,  1.41s/it] 26%|██▌       | 795/3108 [18:49<54:15,  1.41s/it] 26%|██▌       | 796/3108 [18:51<54:09,  1.41s/it] 26%|██▌       | 797/3108 [18:52<54:04,  1.40s/it] 26%|██▌       | 798/3108 [18:53<54:02,  1.40s/it] 26%|██▌       | 799/3108 [18:55<53:58,  1.40s/it] 26%|██▌       | 800/3108 [18:56<53:50,  1.40s/it] 26%|██▌       | 801/3108 [18:58<53:48,  1.40s/it] 26%|██▌       | 802/3108 [18:59<53:57,  1.40s/it] 26%|██▌       | 803/3108 [19:00<53:59,  1.41s/it] 26%|██▌       | 804/3108 [19:02<54:00,  1.41s/it] 26%|██▌       | 805/3108 [19:03<54:02,  1.41s/it] 26%|██▌       | 806/3108 [19:05<53:55,  1.41s/it] 26%|██▌       | 807/3108 [19:06<55:03,  1.44s/it] 26%|██▌       | 808/3108 [19:08<54:49,  1.43s/it] 26%|██▌       | 809/3108 [19:09<55:43,  1.45s/it] 26%|██▌       | 810/3108 [19:10<55:51,  1.46s/it] 26%|██▌       | 811/3108 [19:12<55:13,  1.44s/it] 26%|██▌       | 812/3108 [19:13<54:39,  1.43s/it] 26%|██▌       | 813/3108 [19:15<54:20,  1.42s/it] 26%|██▌       | 814/3108 [19:16<54:06,  1.42s/it] 26%|██▌       | 815/3108 [19:17<53:52,  1.41s/it] 26%|██▋       | 816/3108 [19:19<53:53,  1.41s/it] 26%|██▋       | 817/3108 [19:20<53:51,  1.41s/it] 26%|██▋       | 818/3108 [19:22<53:51,  1.41s/it] 26%|██▋       | 819/3108 [19:23<53:43,  1.41s/it] 26%|██▋       | 820/3108 [19:25<53:37,  1.41s/it] 26%|██▋       | 821/3108 [19:26<53:37,  1.41s/it] 26%|██▋       | 822/3108 [19:27<53:41,  1.41s/it] 26%|██▋       | 823/3108 [19:29<53:40,  1.41s/it] 27%|██▋       | 824/3108 [19:30<53:25,  1.40s/it] 27%|██▋       | 825/3108 [19:32<53:19,  1.40s/it] 27%|██▋       | 826/3108 [19:33<53:26,  1.40s/it] 27%|██▋       | 827/3108 [19:34<53:23,  1.40s/it] 27%|██▋       | 828/3108 [19:36<53:19,  1.40s/it] 27%|██▋       | 829/3108 [19:37<53:18,  1.40s/it] 27%|██▋       | 830/3108 [19:39<53:15,  1.40s/it] 27%|██▋       | 831/3108 [19:40<53:16,  1.40s/it] 27%|██▋       | 832/3108 [19:41<54:30,  1.44s/it] 27%|██▋       | 833/3108 [19:43<54:00,  1.42s/it] 27%|██▋       | 834/3108 [19:44<53:47,  1.42s/it] 27%|██▋       | 835/3108 [19:46<53:41,  1.42s/it] 27%|██▋       | 836/3108 [19:47<53:25,  1.41s/it] 27%|██▋       | 837/3108 [19:48<53:11,  1.41s/it] 27%|██▋       | 838/3108 [19:50<53:07,  1.40s/it] 27%|██▋       | 839/3108 [19:51<54:14,  1.43s/it] 27%|██▋       | 840/3108 [19:53<54:38,  1.45s/it] 27%|██▋       | 841/3108 [19:54<54:08,  1.43s/it] 27%|██▋       | 842/3108 [19:56<53:47,  1.42s/it] 27%|██▋       | 843/3108 [19:57<53:28,  1.42s/it] 27%|██▋       | 844/3108 [19:58<53:19,  1.41s/it] 27%|██▋       | 845/3108 [20:00<53:11,  1.41s/it] 27%|██▋       | 846/3108 [20:01<53:05,  1.41s/it] 27%|██▋       | 847/3108 [20:03<53:03,  1.41s/it] 27%|██▋       | 848/3108 [20:04<53:00,  1.41s/it] 27%|██▋       | 849/3108 [20:06<53:00,  1.41s/it] 27%|██▋       | 850/3108 [20:07<53:01,  1.41s/it] 27%|██▋       | 851/3108 [20:08<53:04,  1.41s/it] 27%|██▋       | 852/3108 [20:10<53:06,  1.41s/it] 27%|██▋       | 853/3108 [20:11<53:01,  1.41s/it] 27%|██▋       | 854/3108 [20:13<52:58,  1.41s/it] 28%|██▊       | 855/3108 [20:14<52:55,  1.41s/it] 28%|██▊       | 856/3108 [20:15<52:48,  1.41s/it] 28%|██▊       | 857/3108 [20:17<53:55,  1.44s/it] 28%|██▊       | 858/3108 [20:18<53:26,  1.43s/it] 28%|██▊       | 859/3108 [20:20<53:10,  1.42s/it] 28%|██▊       | 860/3108 [20:21<52:49,  1.41s/it] 28%|██▊       | 861/3108 [20:22<52:43,  1.41s/it] 28%|██▊       | 862/3108 [20:24<52:42,  1.41s/it] 28%|██▊       | 863/3108 [20:25<52:44,  1.41s/it] 28%|██▊       | 864/3108 [20:27<52:42,  1.41s/it] 28%|██▊       | 865/3108 [20:28<52:33,  1.41s/it] 28%|██▊       | 866/3108 [20:30<52:32,  1.41s/it] 28%|██▊       | 867/3108 [20:31<52:27,  1.40s/it] 28%|██▊       | 868/3108 [20:32<52:28,  1.41s/it] 28%|██▊       | 869/3108 [20:34<53:32,  1.43s/it] 28%|██▊       | 870/3108 [20:35<53:54,  1.45s/it] 28%|██▊       | 871/3108 [20:37<53:27,  1.43s/it] 28%|██▊       | 872/3108 [20:38<53:10,  1.43s/it] 28%|██▊       | 873/3108 [20:40<52:56,  1.42s/it] 28%|██▊       | 874/3108 [20:41<52:38,  1.41s/it] 28%|██▊       | 875/3108 [20:42<52:33,  1.41s/it] 28%|██▊       | 876/3108 [20:44<52:32,  1.41s/it] 28%|██▊       | 877/3108 [20:45<52:28,  1.41s/it] 28%|██▊       | 878/3108 [20:47<52:17,  1.41s/it] 28%|██▊       | 879/3108 [20:48<52:17,  1.41s/it] 28%|██▊       | 880/3108 [20:49<52:20,  1.41s/it] 28%|██▊       | 881/3108 [20:51<52:16,  1.41s/it] 28%|██▊       | 882/3108 [20:52<53:28,  1.44s/it] 28%|██▊       | 883/3108 [20:54<53:06,  1.43s/it] 28%|██▊       | 884/3108 [20:55<52:41,  1.42s/it] 28%|██▊       | 885/3108 [20:57<52:32,  1.42s/it] 29%|██▊       | 886/3108 [20:58<52:15,  1.41s/it] 29%|██▊       | 887/3108 [20:59<52:07,  1.41s/it] 29%|██▊       | 888/3108 [21:01<52:01,  1.41s/it] 29%|██▊       | 889/3108 [21:02<51:56,  1.40s/it] 29%|██▊       | 890/3108 [21:04<51:56,  1.40s/it] 29%|██▊       | 891/3108 [21:05<51:53,  1.40s/it] 29%|██▊       | 892/3108 [21:06<51:55,  1.41s/it] 29%|██▊       | 893/3108 [21:08<51:55,  1.41s/it] 29%|██▉       | 894/3108 [21:09<51:59,  1.41s/it] 29%|██▉       | 895/3108 [21:11<52:00,  1.41s/it] 29%|██▉       | 896/3108 [21:12<51:55,  1.41s/it] 29%|██▉       | 897/3108 [21:13<51:47,  1.41s/it] 29%|██▉       | 898/3108 [21:15<51:40,  1.40s/it] 29%|██▉       | 899/3108 [21:16<52:48,  1.43s/it] 29%|██▉       | 900/3108 [21:18<53:10,  1.44s/it] 29%|██▉       | 901/3108 [21:19<52:48,  1.44s/it] 29%|██▉       | 902/3108 [21:21<52:20,  1.42s/it] 29%|██▉       | 903/3108 [21:22<52:06,  1.42s/it] 29%|██▉       | 904/3108 [21:23<51:48,  1.41s/it] 29%|██▉       | 905/3108 [21:25<51:37,  1.41s/it] 29%|██▉       | 906/3108 [21:26<51:39,  1.41s/it] 29%|██▉       | 907/3108 [21:28<51:38,  1.41s/it] 29%|██▉       | 908/3108 [21:29<52:38,  1.44s/it] 29%|██▉       | 909/3108 [21:30<52:20,  1.43s/it] 29%|██▉       | 910/3108 [21:32<52:05,  1.42s/it] 29%|██▉       | 911/3108 [21:33<51:53,  1.42s/it] 29%|██▉       | 912/3108 [21:35<51:40,  1.41s/it] 29%|██▉       | 913/3108 [21:36<51:38,  1.41s/it] 29%|██▉       | 914/3108 [21:37<51:25,  1.41s/it] 29%|██▉       | 915/3108 [21:39<51:24,  1.41s/it] 29%|██▉       | 916/3108 [21:40<51:13,  1.40s/it] 30%|██▉       | 917/3108 [21:42<51:18,  1.41s/it] 30%|██▉       | 918/3108 [21:43<51:10,  1.40s/it] 30%|██▉       | 919/3108 [21:45<51:16,  1.41s/it] 30%|██▉       | 920/3108 [21:46<51:04,  1.40s/it] 30%|██▉       | 921/3108 [21:47<51:00,  1.40s/it] 30%|██▉       | 922/3108 [21:49<51:07,  1.40s/it] 30%|██▉       | 923/3108 [21:50<51:10,  1.41s/it] 30%|██▉       | 924/3108 [21:52<51:15,  1.41s/it] 30%|██▉       | 925/3108 [21:53<51:15,  1.41s/it] 30%|██▉       | 926/3108 [21:54<51:16,  1.41s/it] 30%|██▉       | 927/3108 [21:56<51:16,  1.41s/it] 30%|██▉       | 928/3108 [21:57<51:06,  1.41s/it] 30%|██▉       | 929/3108 [21:59<52:12,  1.44s/it] 30%|██▉       | 930/3108 [22:00<52:35,  1.45s/it] 30%|██▉       | 931/3108 [22:02<52:07,  1.44s/it] 30%|██▉       | 932/3108 [22:03<51:38,  1.42s/it] 30%|███       | 933/3108 [22:04<52:25,  1.45s/it] 30%|███       | 934/3108 [22:06<51:57,  1.43s/it] 30%|███       | 935/3108 [22:07<51:33,  1.42s/it] 30%|███       | 936/3108 [22:09<51:20,  1.42s/it] 30%|███       | 937/3108 [22:10<51:10,  1.41s/it] 30%|███       | 938/3108 [22:11<51:04,  1.41s/it] 30%|███       | 939/3108 [22:13<50:53,  1.41s/it] 30%|███       | 940/3108 [22:14<50:53,  1.41s/it] 30%|███       | 941/3108 [22:16<50:49,  1.41s/it] 30%|███       | 942/3108 [22:17<50:41,  1.40s/it] 30%|███       | 943/3108 [22:18<50:46,  1.41s/it] 30%|███       | 944/3108 [22:20<50:47,  1.41s/it] 30%|███       | 945/3108 [22:21<50:38,  1.40s/it] 30%|███       | 946/3108 [22:23<50:33,  1.40s/it] 30%|███       | 947/3108 [22:24<50:32,  1.40s/it] 31%|███       | 948/3108 [22:26<50:36,  1.41s/it] 31%|███       | 949/3108 [22:27<50:39,  1.41s/it] 31%|███       | 950/3108 [22:28<50:38,  1.41s/it] 31%|███       | 951/3108 [22:30<50:32,  1.41s/it] 31%|███       | 952/3108 [22:31<50:27,  1.40s/it] 31%|███       | 953/3108 [22:33<50:17,  1.40s/it] 31%|███       | 954/3108 [22:34<50:19,  1.40s/it] 31%|███       | 955/3108 [22:35<50:17,  1.40s/it] 31%|███       | 956/3108 [22:37<50:12,  1.40s/it] 31%|███       | 957/3108 [22:38<50:13,  1.40s/it] 31%|███       | 958/3108 [22:40<51:20,  1.43s/it] 31%|███       | 959/3108 [22:41<51:59,  1.45s/it] 31%|███       | 960/3108 [22:43<52:14,  1.46s/it] 31%|███       | 961/3108 [22:44<51:33,  1.44s/it] 31%|███       | 962/3108 [22:45<51:05,  1.43s/it] 31%|███       | 963/3108 [22:47<50:46,  1.42s/it] 31%|███       | 964/3108 [22:48<50:35,  1.42s/it] 31%|███       | 965/3108 [22:50<50:24,  1.41s/it] 31%|███       | 966/3108 [22:51<50:16,  1.41s/it] 31%|███       | 967/3108 [22:52<50:18,  1.41s/it] 31%|███       | 968/3108 [22:54<50:15,  1.41s/it] 31%|███       | 969/3108 [22:55<50:06,  1.41s/it] 31%|███       | 970/3108 [22:57<50:07,  1.41s/it] 31%|███       | 971/3108 [22:58<49:55,  1.40s/it] 31%|███▏      | 972/3108 [22:59<50:00,  1.40s/it] 31%|███▏      | 973/3108 [23:01<50:04,  1.41s/it] 31%|███▏      | 974/3108 [23:02<50:04,  1.41s/it] 31%|███▏      | 975/3108 [23:04<49:54,  1.40s/it] 31%|███▏      | 976/3108 [23:05<49:49,  1.40s/it] 31%|███▏      | 977/3108 [23:06<49:49,  1.40s/it] 31%|███▏      | 978/3108 [23:08<49:47,  1.40s/it] 31%|███▏      | 979/3108 [23:09<49:50,  1.40s/it] 32%|███▏      | 980/3108 [23:11<49:53,  1.41s/it] 32%|███▏      | 981/3108 [23:12<49:50,  1.41s/it] 32%|███▏      | 982/3108 [23:14<49:46,  1.40s/it] 32%|███▏      | 983/3108 [23:15<50:56,  1.44s/it] 32%|███▏      | 984/3108 [23:16<50:36,  1.43s/it] 32%|███▏      | 985/3108 [23:18<50:25,  1.43s/it] 32%|███▏      | 986/3108 [23:19<50:17,  1.42s/it] 32%|███▏      | 987/3108 [23:21<50:07,  1.42s/it] 32%|███▏      | 988/3108 [23:22<49:53,  1.41s/it] 32%|███▏      | 989/3108 [23:24<50:52,  1.44s/it] 32%|███▏      | 990/3108 [23:25<51:09,  1.45s/it] 32%|███▏      | 991/3108 [23:26<50:36,  1.43s/it] 32%|███▏      | 992/3108 [23:28<50:10,  1.42s/it] 32%|███▏      | 993/3108 [23:29<49:57,  1.42s/it] 32%|███▏      | 994/3108 [23:31<49:44,  1.41s/it] 32%|███▏      | 995/3108 [23:32<49:40,  1.41s/it] 32%|███▏      | 996/3108 [23:33<49:42,  1.41s/it] 32%|███▏      | 997/3108 [23:35<49:38,  1.41s/it] 32%|███▏      | 998/3108 [23:36<49:28,  1.41s/it] 32%|███▏      | 999/3108 [23:38<49:27,  1.41s/it] 32%|███▏      | 1000/3108 [23:39<49:28,  1.41s/it]                                                   loss: 1.16145544, learning_rate: 1.428e-05, global_step: 1000, interval_runtime: 708.2049, interval_samples_per_second: 12.708185843656276, interval_steps_per_second: 0.7060103246475709, epoch: 1.287
 32%|███▏      | 1000/3108 [23:39<49:28,  1.41s/it][32m[2023-11-10 09:31:02,700] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, token_to_orig_map, token_is_max_context, question_id, questions, tokens, id, start_labels. If end_labels, token_to_orig_map, token_is_max_context, question_id, questions, tokens, id, start_labels are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 09:31:03,167] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 09:31:03,167] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 09:31:03,168] [    INFO][0m -   Total prediction steps = 48[0m
[32m[2023-11-10 09:31:03,168] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 09:31:03,168] [    INFO][0m -   Total Batch size = 18[0m

  0%|          | 0/48 [00:00<?, ?it/s][A
  4%|▍         | 2/48 [00:00<00:15,  2.90it/s][A
  6%|▋         | 3/48 [00:01<00:21,  2.05it/s][A
  8%|▊         | 4/48 [00:02<00:24,  1.78it/s][A
 10%|█         | 5/48 [00:02<00:25,  1.65it/s][A
 12%|█▎        | 6/48 [00:03<00:26,  1.58it/s][A
 15%|█▍        | 7/48 [00:04<00:26,  1.54it/s][A
 17%|█▋        | 8/48 [00:04<00:26,  1.51it/s][A
 19%|█▉        | 9/48 [00:05<00:26,  1.50it/s][A
 21%|██        | 10/48 [00:06<00:25,  1.48it/s][A
 23%|██▎       | 11/48 [00:06<00:25,  1.48it/s][A
 25%|██▌       | 12/48 [00:07<00:24,  1.47it/s][A
 27%|██▋       | 13/48 [00:08<00:25,  1.40it/s][A
 29%|██▉       | 14/48 [00:09<00:24,  1.41it/s][A
 31%|███▏      | 15/48 [00:09<00:23,  1.43it/s][A
 33%|███▎      | 16/48 [00:10<00:22,  1.44it/s][A
 35%|███▌      | 17/48 [00:11<00:21,  1.44it/s][A
 38%|███▊      | 18/48 [00:11<00:20,  1.45it/s][A
 40%|███▉      | 19/48 [00:12<00:19,  1.45it/s][A
 42%|████▏     | 20/48 [00:13<00:19,  1.45it/s][A
 44%|████▍     | 21/48 [00:13<00:18,  1.46it/s][A
 46%|████▌     | 22/48 [00:14<00:17,  1.46it/s][A
 48%|████▊     | 23/48 [00:15<00:17,  1.46it/s][A
 50%|█████     | 24/48 [00:15<00:17,  1.39it/s][A
 52%|█████▏    | 25/48 [00:16<00:16,  1.42it/s][A
 54%|█████▍    | 26/48 [00:17<00:15,  1.43it/s][A
 56%|█████▋    | 27/48 [00:18<00:14,  1.44it/s][A
 58%|█████▊    | 28/48 [00:18<00:13,  1.44it/s][A
 60%|██████    | 29/48 [00:19<00:13,  1.45it/s][A
 62%|██████▎   | 30/48 [00:20<00:12,  1.45it/s][A
 65%|██████▍   | 31/48 [00:20<00:11,  1.45it/s][A
 67%|██████▋   | 32/48 [00:21<00:10,  1.45it/s][A
 69%|██████▉   | 33/48 [00:22<00:10,  1.46it/s][A
 71%|███████   | 34/48 [00:22<00:09,  1.46it/s][A
 73%|███████▎  | 35/48 [00:23<00:08,  1.46it/s][A
 75%|███████▌  | 36/48 [00:24<00:08,  1.46it/s][A
 77%|███████▋  | 37/48 [00:24<00:07,  1.46it/s][A
 79%|███████▉  | 38/48 [00:25<00:06,  1.46it/s][A
 81%|████████▏ | 39/48 [00:26<00:06,  1.46it/s][A
 83%|████████▎ | 40/48 [00:26<00:05,  1.46it/s][A
 85%|████████▌ | 41/48 [00:27<00:04,  1.46it/s][A
 88%|████████▊ | 42/48 [00:28<00:04,  1.46it/s][A
 90%|████████▉ | 43/48 [00:28<00:03,  1.65it/s][A
 92%|█████████▏| 44/48 [00:29<00:02,  1.94it/s][A
 94%|█████████▍| 45/48 [00:29<00:01,  2.21it/s][A
 96%|█████████▌| 46/48 [00:29<00:00,  2.46it/s][A
 98%|█████████▊| 47/48 [00:29<00:00,  2.67it/s][A
100%|██████████| 48/48 [00:30<00:00,  3.39it/s][A                                                   
                                               [Aeval_anls: 61.12011982881637, epoch: 1.287
 32%|███▏      | 1000/3108 [24:16<49:28,  1.41s/it]
100%|██████████| 48/48 [00:33<00:00,  3.39it/s][A
                                               [A[32m[2023-11-10 09:31:39,617] [    INFO][0m - Saving model checkpoint to ./models/fidelity/checkpoint-1000[0m
[32m[2023-11-10 09:31:39,625] [    INFO][0m - Configuration saved in ./models/fidelity/checkpoint-1000/config.json[0m
[32m[2023-11-10 09:31:42,208] [    INFO][0m - Model weights saved in ./models/fidelity/checkpoint-1000/model_state.pdparams[0m
[32m[2023-11-10 09:31:42,209] [    INFO][0m - tokenizer config file saved in ./models/fidelity/checkpoint-1000/tokenizer_config.json[0m
[32m[2023-11-10 09:31:42,209] [    INFO][0m - Special tokens file saved in ./models/fidelity/checkpoint-1000/special_tokens_map.json[0m
 32%|███▏      | 1001/3108 [24:25<8:40:50, 14.83s/it] 32%|███▏      | 1002/3108 [24:27<6:19:04, 10.80s/it] 32%|███▏      | 1003/3108 [24:28<4:39:54,  7.98s/it] 32%|███▏      | 1004/3108 [24:30<3:31:38,  6.04s/it] 32%|███▏      | 1005/3108 [24:31<2:43:34,  4.67s/it] 32%|███▏      | 1006/3108 [24:32<2:09:10,  3.69s/it] 32%|███▏      | 1007/3108 [24:34<1:44:57,  3.00s/it] 32%|███▏      | 1008/3108 [24:35<1:28:02,  2.52s/it] 32%|███▏      | 1009/3108 [24:37<1:16:16,  2.18s/it] 32%|███▏      | 1010/3108 [24:38<1:08:06,  1.95s/it] 33%|███▎      | 1011/3108 [24:39<1:02:23,  1.79s/it] 33%|███▎      | 1012/3108 [24:41<58:25,  1.67s/it]   33%|███▎      | 1013/3108 [24:42<55:29,  1.59s/it] 33%|███▎      | 1014/3108 [24:44<53:31,  1.53s/it] 33%|███▎      | 1015/3108 [24:45<52:04,  1.49s/it] 33%|███▎      | 1016/3108 [24:46<51:08,  1.47s/it] 33%|███▎      | 1017/3108 [24:48<50:23,  1.45s/it] 33%|███▎      | 1018/3108 [24:49<49:59,  1.44s/it] 33%|███▎      | 1019/3108 [24:51<49:43,  1.43s/it] 33%|███▎      | 1020/3108 [24:52<49:22,  1.42s/it] 33%|███▎      | 1021/3108 [24:53<49:09,  1.41s/it] 33%|███▎      | 1022/3108 [24:55<48:59,  1.41s/it] 33%|███▎      | 1023/3108 [24:56<48:48,  1.40s/it] 33%|███▎      | 1024/3108 [24:58<48:46,  1.40s/it] 33%|███▎      | 1025/3108 [24:59<48:43,  1.40s/it] 33%|███▎      | 1026/3108 [25:01<49:46,  1.43s/it] 33%|███▎      | 1027/3108 [25:02<49:19,  1.42s/it] 33%|███▎      | 1028/3108 [25:03<49:06,  1.42s/it] 33%|███▎      | 1029/3108 [25:05<48:58,  1.41s/it] 33%|███▎      | 1030/3108 [25:06<49:53,  1.44s/it] 33%|███▎      | 1031/3108 [25:08<50:11,  1.45s/it] 33%|███▎      | 1032/3108 [25:09<49:37,  1.43s/it] 33%|███▎      | 1033/3108 [25:11<49:21,  1.43s/it] 33%|███▎      | 1034/3108 [25:12<49:13,  1.42s/it] 33%|███▎      | 1035/3108 [25:13<49:02,  1.42s/it] 33%|███▎      | 1036/3108 [25:15<48:58,  1.42s/it] 33%|███▎      | 1037/3108 [25:16<48:50,  1.41s/it] 33%|███▎      | 1038/3108 [25:18<48:47,  1.41s/it] 33%|███▎      | 1039/3108 [25:19<48:30,  1.41s/it] 33%|███▎      | 1040/3108 [25:20<48:26,  1.41s/it] 33%|███▎      | 1041/3108 [25:22<48:26,  1.41s/it] 34%|███▎      | 1042/3108 [25:23<48:28,  1.41s/it] 34%|███▎      | 1043/3108 [25:25<48:29,  1.41s/it] 34%|███▎      | 1044/3108 [25:26<48:21,  1.41s/it] 34%|███▎      | 1045/3108 [25:27<48:21,  1.41s/it] 34%|███▎      | 1046/3108 [25:29<48:15,  1.40s/it] 34%|███▎      | 1047/3108 [25:30<48:18,  1.41s/it] 34%|███▎      | 1048/3108 [25:32<48:16,  1.41s/it] 34%|███▍      | 1049/3108 [25:33<48:11,  1.40s/it] 34%|███▍      | 1050/3108 [25:34<48:10,  1.40s/it] 34%|███▍      | 1051/3108 [25:36<48:06,  1.40s/it] 34%|███▍      | 1052/3108 [25:37<49:12,  1.44s/it] 34%|███▍      | 1053/3108 [25:39<48:51,  1.43s/it] 34%|███▍      | 1054/3108 [25:40<48:33,  1.42s/it] 34%|███▍      | 1055/3108 [25:42<49:28,  1.45s/it] 34%|███▍      | 1056/3108 [25:43<49:56,  1.46s/it] 34%|███▍      | 1057/3108 [25:45<49:14,  1.44s/it] 34%|███▍      | 1058/3108 [25:46<48:54,  1.43s/it] 34%|███▍      | 1059/3108 [25:47<48:30,  1.42s/it] 34%|███▍      | 1060/3108 [25:49<48:19,  1.42s/it] 34%|███▍      | 1061/3108 [25:50<48:01,  1.41s/it] 34%|███▍      | 1062/3108 [25:52<47:56,  1.41s/it] 34%|███▍      | 1063/3108 [25:53<47:54,  1.41s/it] 34%|███▍      | 1064/3108 [25:54<47:44,  1.40s/it] 34%|███▍      | 1065/3108 [25:56<47:42,  1.40s/it] 34%|███▍      | 1066/3108 [25:57<47:44,  1.40s/it] 34%|███▍      | 1067/3108 [25:59<47:36,  1.40s/it] 34%|███▍      | 1068/3108 [26:00<47:35,  1.40s/it] 34%|███▍      | 1069/3108 [26:01<47:34,  1.40s/it] 34%|███▍      | 1070/3108 [26:03<47:35,  1.40s/it] 34%|███▍      | 1071/3108 [26:04<47:37,  1.40s/it] 34%|███▍      | 1072/3108 [26:06<47:39,  1.40s/it] 35%|███▍      | 1073/3108 [26:07<47:45,  1.41s/it] 35%|███▍      | 1074/3108 [26:08<47:39,  1.41s/it] 35%|███▍      | 1075/3108 [26:10<47:33,  1.40s/it] 35%|███▍      | 1076/3108 [26:11<47:29,  1.40s/it] 35%|███▍      | 1077/3108 [26:13<48:35,  1.44s/it] 35%|███▍      | 1078/3108 [26:14<48:14,  1.43s/it] 35%|███▍      | 1079/3108 [26:15<47:58,  1.42s/it] 35%|███▍      | 1080/3108 [26:17<48:52,  1.45s/it] 35%|███▍      | 1081/3108 [26:18<49:16,  1.46s/it] 35%|███▍      | 1082/3108 [26:20<48:39,  1.44s/it] 35%|███▍      | 1083/3108 [26:21<48:18,  1.43s/it] 35%|███▍      | 1084/3108 [26:23<48:02,  1.42s/it] 35%|███▍      | 1085/3108 [26:24<47:45,  1.42s/it] 35%|███▍      | 1086/3108 [26:26<47:39,  1.41s/it] 35%|███▍      | 1087/3108 [26:27<47:27,  1.41s/it] 35%|███▌      | 1088/3108 [26:28<47:26,  1.41s/it] 35%|███▌      | 1089/3108 [26:30<47:21,  1.41s/it] 35%|███▌      | 1090/3108 [26:31<47:16,  1.41s/it] 35%|███▌      | 1091/3108 [26:33<47:06,  1.40s/it] 35%|███▌      | 1092/3108 [26:34<47:06,  1.40s/it] 35%|███▌      | 1093/3108 [26:35<47:09,  1.40s/it] 35%|███▌      | 1094/3108 [26:37<47:10,  1.41s/it] 35%|███▌      | 1095/3108 [26:38<47:04,  1.40s/it] 35%|███▌      | 1096/3108 [26:40<47:08,  1.41s/it] 35%|███▌      | 1097/3108 [26:41<47:05,  1.40s/it] 35%|███▌      | 1098/3108 [26:42<47:01,  1.40s/it] 35%|███▌      | 1099/3108 [26:44<46:55,  1.40s/it] 35%|███▌      | 1100/3108 [26:45<47:00,  1.40s/it] 35%|███▌      | 1101/3108 [26:47<47:00,  1.41s/it] 35%|███▌      | 1102/3108 [26:48<48:05,  1.44s/it] 35%|███▌      | 1103/3108 [26:49<47:41,  1.43s/it] 36%|███▌      | 1104/3108 [26:51<47:27,  1.42s/it] 36%|███▌      | 1105/3108 [26:52<47:18,  1.42s/it] 36%|███▌      | 1106/3108 [26:54<48:01,  1.44s/it] 36%|███▌      | 1107/3108 [26:55<48:17,  1.45s/it] 36%|███▌      | 1108/3108 [26:57<47:45,  1.43s/it] 36%|███▌      | 1109/3108 [26:58<47:29,  1.43s/it] 36%|███▌      | 1110/3108 [26:59<47:10,  1.42s/it] 36%|███▌      | 1111/3108 [27:01<46:58,  1.41s/it] 36%|███▌      | 1112/3108 [27:02<46:52,  1.41s/it] 36%|███▌      | 1113/3108 [27:04<46:43,  1.41s/it] 36%|███▌      | 1114/3108 [27:05<46:43,  1.41s/it] 36%|███▌      | 1115/3108 [27:06<46:45,  1.41s/it] 36%|███▌      | 1116/3108 [27:08<46:34,  1.40s/it] 36%|███▌      | 1117/3108 [27:09<46:25,  1.40s/it] 36%|███▌      | 1118/3108 [27:11<46:22,  1.40s/it] 36%|███▌      | 1119/3108 [27:12<46:17,  1.40s/it] 36%|███▌      | 1120/3108 [27:13<46:25,  1.40s/it] 36%|███▌      | 1121/3108 [27:15<46:17,  1.40s/it] 36%|███▌      | 1122/3108 [27:16<46:16,  1.40s/it] 36%|███▌      | 1123/3108 [27:18<46:19,  1.40s/it] 36%|███▌      | 1124/3108 [27:19<46:13,  1.40s/it] 36%|███▌      | 1125/3108 [27:20<46:12,  1.40s/it] 36%|███▌      | 1126/3108 [27:22<46:18,  1.40s/it] 36%|███▋      | 1127/3108 [27:23<46:22,  1.40s/it] 36%|███▋      | 1128/3108 [27:25<47:26,  1.44s/it] 36%|███▋      | 1129/3108 [27:26<47:06,  1.43s/it] 36%|███▋      | 1130/3108 [27:28<46:46,  1.42s/it] 36%|███▋      | 1131/3108 [27:29<46:37,  1.42s/it] 36%|███▋      | 1132/3108 [27:30<47:30,  1.44s/it] 36%|███▋      | 1133/3108 [27:32<47:46,  1.45s/it] 36%|███▋      | 1134/3108 [27:33<47:13,  1.44s/it] 37%|███▋      | 1135/3108 [27:35<47:00,  1.43s/it] 37%|███▋      | 1136/3108 [27:36<46:39,  1.42s/it] 37%|███▋      | 1137/3108 [27:38<46:34,  1.42s/it] 37%|███▋      | 1138/3108 [27:39<46:27,  1.41s/it] 37%|███▋      | 1139/3108 [27:40<46:23,  1.41s/it] 37%|███▋      | 1140/3108 [27:42<46:10,  1.41s/it] 37%|███▋      | 1141/3108 [27:43<46:12,  1.41s/it] 37%|███▋      | 1142/3108 [27:45<46:01,  1.40s/it] 37%|███▋      | 1143/3108 [27:46<46:06,  1.41s/it] 37%|███▋      | 1144/3108 [27:47<45:59,  1.41s/it] 37%|███▋      | 1145/3108 [27:49<46:11,  1.41s/it] 37%|███▋      | 1146/3108 [27:50<46:00,  1.41s/it] 37%|███▋      | 1147/3108 [27:52<45:58,  1.41s/it] 37%|███▋      | 1148/3108 [27:53<45:51,  1.40s/it] 37%|███▋      | 1149/3108 [27:54<45:57,  1.41s/it] 37%|███▋      | 1150/3108 [27:56<45:58,  1.41s/it] 37%|███▋      | 1151/3108 [27:57<46:05,  1.41s/it] 37%|███▋      | 1152/3108 [27:59<45:56,  1.41s/it] 37%|███▋      | 1153/3108 [28:00<45:54,  1.41s/it] 37%|███▋      | 1154/3108 [28:02<46:53,  1.44s/it] 37%|███▋      | 1155/3108 [28:03<46:35,  1.43s/it] 37%|███▋      | 1156/3108 [28:04<46:21,  1.42s/it] 37%|███▋      | 1157/3108 [28:06<47:09,  1.45s/it] 37%|███▋      | 1158/3108 [28:07<47:21,  1.46s/it] 37%|███▋      | 1159/3108 [28:09<46:42,  1.44s/it] 37%|███▋      | 1160/3108 [28:10<46:21,  1.43s/it] 37%|███▋      | 1161/3108 [28:12<46:06,  1.42s/it] 37%|███▋      | 1162/3108 [28:13<46:01,  1.42s/it] 37%|███▋      | 1163/3108 [28:14<45:51,  1.41s/it] 37%|███▋      | 1164/3108 [28:16<45:39,  1.41s/it] 37%|███▋      | 1165/3108 [28:17<45:30,  1.41s/it] 38%|███▊      | 1166/3108 [28:19<45:30,  1.41s/it] 38%|███▊      | 1167/3108 [28:20<45:28,  1.41s/it] 38%|███▊      | 1168/3108 [28:21<45:18,  1.40s/it] 38%|███▊      | 1169/3108 [28:23<45:16,  1.40s/it] 38%|███▊      | 1170/3108 [28:24<45:20,  1.40s/it] 38%|███▊      | 1171/3108 [28:26<45:22,  1.41s/it] 38%|███▊      | 1172/3108 [28:27<45:24,  1.41s/it] 38%|███▊      | 1173/3108 [28:28<45:15,  1.40s/it] 38%|███▊      | 1174/3108 [28:30<45:20,  1.41s/it] 38%|███▊      | 1175/3108 [28:31<45:19,  1.41s/it] 38%|███▊      | 1176/3108 [28:33<45:22,  1.41s/it] 38%|███▊      | 1177/3108 [28:34<45:12,  1.40s/it] 38%|███▊      | 1178/3108 [28:36<45:12,  1.41s/it] 38%|███▊      | 1179/3108 [28:37<46:14,  1.44s/it] 38%|███▊      | 1180/3108 [28:38<45:56,  1.43s/it] 38%|███▊      | 1181/3108 [28:40<45:44,  1.42s/it] 38%|███▊      | 1182/3108 [28:41<46:37,  1.45s/it] 38%|███▊      | 1183/3108 [28:43<46:57,  1.46s/it] 38%|███▊      | 1184/3108 [28:44<46:17,  1.44s/it] 38%|███▊      | 1185/3108 [28:46<45:55,  1.43s/it] 38%|███▊      | 1186/3108 [28:47<45:33,  1.42s/it] 38%|███▊      | 1187/3108 [28:48<45:19,  1.42s/it] 38%|███▊      | 1188/3108 [28:50<45:12,  1.41s/it] 38%|███▊      | 1189/3108 [28:51<45:01,  1.41s/it] 38%|███▊      | 1190/3108 [28:53<45:02,  1.41s/it] 38%|███▊      | 1191/3108 [28:54<45:01,  1.41s/it] 38%|███▊      | 1192/3108 [28:55<44:52,  1.41s/it] 38%|███▊      | 1193/3108 [28:57<44:52,  1.41s/it] 38%|███▊      | 1194/3108 [28:58<44:47,  1.40s/it] 38%|███▊      | 1195/3108 [29:00<44:40,  1.40s/it] 38%|███▊      | 1196/3108 [29:01<44:39,  1.40s/it] 39%|███▊      | 1197/3108 [29:02<44:37,  1.40s/it] 39%|███▊      | 1198/3108 [29:04<44:40,  1.40s/it] 39%|███▊      | 1199/3108 [29:05<44:44,  1.41s/it] 39%|███▊      | 1200/3108 [29:07<44:45,  1.41s/it] 39%|███▊      | 1201/3108 [29:08<44:36,  1.40s/it] 39%|███▊      | 1202/3108 [29:10<44:34,  1.40s/it] 39%|███▊      | 1203/3108 [29:11<44:33,  1.40s/it] 39%|███▊      | 1204/3108 [29:12<45:45,  1.44s/it] 39%|███▉      | 1205/3108 [29:14<45:24,  1.43s/it] 39%|███▉      | 1206/3108 [29:15<45:13,  1.43s/it] 39%|███▉      | 1207/3108 [29:17<45:02,  1.42s/it] 39%|███▉      | 1208/3108 [29:18<45:48,  1.45s/it] 39%|███▉      | 1209/3108 [29:20<46:05,  1.46s/it] 39%|███▉      | 1210/3108 [29:21<45:36,  1.44s/it] 39%|███▉      | 1211/3108 [29:22<45:08,  1.43s/it] 39%|███▉      | 1212/3108 [29:24<44:52,  1.42s/it] 39%|███▉      | 1213/3108 [29:25<44:40,  1.41s/it] 39%|███▉      | 1214/3108 [29:27<44:26,  1.41s/it] 39%|███▉      | 1215/3108 [29:28<44:28,  1.41s/it] 39%|███▉      | 1216/3108 [29:29<44:25,  1.41s/it] 39%|███▉      | 1217/3108 [29:31<44:28,  1.41s/it] 39%|███▉      | 1218/3108 [29:32<44:23,  1.41s/it] 39%|███▉      | 1219/3108 [29:34<44:15,  1.41s/it] 39%|███▉      | 1220/3108 [29:35<44:11,  1.40s/it] 39%|███▉      | 1221/3108 [29:36<44:08,  1.40s/it] 39%|███▉      | 1222/3108 [29:38<44:11,  1.41s/it] 39%|███▉      | 1223/3108 [29:39<44:12,  1.41s/it] 39%|███▉      | 1224/3108 [29:41<44:02,  1.40s/it] 39%|███▉      | 1225/3108 [29:42<43:59,  1.40s/it] 39%|███▉      | 1226/3108 [29:44<44:01,  1.40s/it] 39%|███▉      | 1227/3108 [29:45<44:04,  1.41s/it] 40%|███▉      | 1228/3108 [29:46<43:55,  1.40s/it] 40%|███▉      | 1229/3108 [29:48<43:58,  1.40s/it] 40%|███▉      | 1230/3108 [29:49<45:01,  1.44s/it] 40%|███▉      | 1231/3108 [29:51<44:42,  1.43s/it] 40%|███▉      | 1232/3108 [29:52<44:23,  1.42s/it] 40%|███▉      | 1233/3108 [29:53<44:19,  1.42s/it] 40%|███▉      | 1234/3108 [29:55<45:05,  1.44s/it] 40%|███▉      | 1235/3108 [29:56<45:23,  1.45s/it] 40%|███▉      | 1236/3108 [29:58<44:48,  1.44s/it] 40%|███▉      | 1237/3108 [29:59<44:36,  1.43s/it] 40%|███▉      | 1238/3108 [30:01<44:12,  1.42s/it] 40%|███▉      | 1239/3108 [30:02<44:02,  1.41s/it] 40%|███▉      | 1240/3108 [30:03<43:57,  1.41s/it] 40%|███▉      | 1241/3108 [30:05<43:52,  1.41s/it] 40%|███▉      | 1242/3108 [30:06<43:39,  1.40s/it] 40%|███▉      | 1243/3108 [30:08<43:37,  1.40s/it] 40%|████      | 1244/3108 [30:09<43:37,  1.40s/it] 40%|████      | 1245/3108 [30:10<43:37,  1.41s/it] 40%|████      | 1246/3108 [30:12<43:35,  1.40s/it] 40%|████      | 1247/3108 [30:13<43:35,  1.41s/it] 40%|████      | 1248/3108 [30:15<43:29,  1.40s/it] 40%|████      | 1249/3108 [30:16<43:33,  1.41s/it] 40%|████      | 1250/3108 [30:18<43:35,  1.41s/it] 40%|████      | 1251/3108 [30:19<43:33,  1.41s/it] 40%|████      | 1252/3108 [30:20<43:33,  1.41s/it] 40%|████      | 1253/3108 [30:22<43:38,  1.41s/it] 40%|████      | 1254/3108 [30:23<43:35,  1.41s/it] 40%|████      | 1255/3108 [30:25<43:35,  1.41s/it] 40%|████      | 1256/3108 [30:26<44:29,  1.44s/it] 40%|████      | 1257/3108 [30:27<44:03,  1.43s/it] 40%|████      | 1258/3108 [30:29<43:47,  1.42s/it] 41%|████      | 1259/3108 [30:30<44:30,  1.44s/it] 41%|████      | 1260/3108 [30:32<44:53,  1.46s/it] 41%|████      | 1261/3108 [30:33<44:17,  1.44s/it] 41%|████      | 1262/3108 [30:35<43:59,  1.43s/it] 41%|████      | 1263/3108 [30:36<43:48,  1.42s/it] 41%|████      | 1264/3108 [30:37<43:41,  1.42s/it] 41%|████      | 1265/3108 [30:39<43:35,  1.42s/it] 41%|████      | 1266/3108 [30:40<43:28,  1.42s/it] 41%|████      | 1267/3108 [30:42<43:14,  1.41s/it] 41%|████      | 1268/3108 [30:43<43:10,  1.41s/it] 41%|████      | 1269/3108 [30:45<43:06,  1.41s/it] 41%|████      | 1270/3108 [30:46<43:06,  1.41s/it] 41%|████      | 1271/3108 [30:47<43:05,  1.41s/it] 41%|████      | 1272/3108 [30:49<43:11,  1.41s/it] 41%|████      | 1273/3108 [30:50<43:02,  1.41s/it] 41%|████      | 1274/3108 [30:52<42:59,  1.41s/it] 41%|████      | 1275/3108 [30:53<42:53,  1.40s/it] 41%|████      | 1276/3108 [30:54<43:01,  1.41s/it] 41%|████      | 1277/3108 [30:56<42:54,  1.41s/it] 41%|████      | 1278/3108 [30:57<42:53,  1.41s/it] 41%|████      | 1279/3108 [30:59<42:45,  1.40s/it] 41%|████      | 1280/3108 [31:00<42:49,  1.41s/it] 41%|████      | 1281/3108 [31:02<43:46,  1.44s/it] 41%|████      | 1282/3108 [31:03<43:21,  1.42s/it] 41%|████▏     | 1283/3108 [31:04<43:14,  1.42s/it] 41%|████▏     | 1284/3108 [31:06<43:59,  1.45s/it] 41%|████▏     | 1285/3108 [31:07<44:23,  1.46s/it] 41%|████▏     | 1286/3108 [31:09<43:49,  1.44s/it] 41%|████▏     | 1287/3108 [31:10<43:33,  1.43s/it] 41%|████▏     | 1288/3108 [31:12<43:16,  1.43s/it] 41%|████▏     | 1289/3108 [31:13<43:05,  1.42s/it] 42%|████▏     | 1290/3108 [31:14<42:51,  1.41s/it] 42%|████▏     | 1291/3108 [31:16<42:45,  1.41s/it] 42%|████▏     | 1292/3108 [31:17<42:38,  1.41s/it] 42%|████▏     | 1293/3108 [31:19<42:36,  1.41s/it] 42%|████▏     | 1294/3108 [31:20<42:36,  1.41s/it] 42%|████▏     | 1295/3108 [31:21<42:37,  1.41s/it] 42%|████▏     | 1296/3108 [31:23<42:36,  1.41s/it] 42%|████▏     | 1297/3108 [31:24<42:34,  1.41s/it] 42%|████▏     | 1298/3108 [31:26<42:27,  1.41s/it] 42%|████▏     | 1299/3108 [31:27<42:21,  1.41s/it] 42%|████▏     | 1300/3108 [31:28<42:15,  1.40s/it] 42%|████▏     | 1301/3108 [31:30<42:20,  1.41s/it] 42%|████▏     | 1302/3108 [31:31<42:12,  1.40s/it] 42%|████▏     | 1303/3108 [31:33<42:16,  1.41s/it] 42%|████▏     | 1304/3108 [31:34<42:07,  1.40s/it] 42%|████▏     | 1305/3108 [31:35<42:11,  1.40s/it] 42%|████▏     | 1306/3108 [31:37<43:06,  1.44s/it] 42%|████▏     | 1307/3108 [31:38<42:47,  1.43s/it] 42%|████▏     | 1308/3108 [31:40<42:34,  1.42s/it] 42%|████▏     | 1309/3108 [31:41<42:21,  1.41s/it] 42%|████▏     | 1310/3108 [31:43<43:07,  1.44s/it] 42%|████▏     | 1311/3108 [31:44<43:30,  1.45s/it] 42%|████▏     | 1312/3108 [31:46<43:08,  1.44s/it] 42%|████▏     | 1313/3108 [31:47<42:48,  1.43s/it] 42%|████▏     | 1314/3108 [31:48<42:30,  1.42s/it] 42%|████▏     | 1315/3108 [31:50<42:21,  1.42s/it] 42%|████▏     | 1316/3108 [31:51<42:15,  1.41s/it] 42%|████▏     | 1317/3108 [31:53<42:03,  1.41s/it] 42%|████▏     | 1318/3108 [31:54<42:04,  1.41s/it] 42%|████▏     | 1319/3108 [31:55<41:59,  1.41s/it] 42%|████▏     | 1320/3108 [31:57<41:49,  1.40s/it] 43%|████▎     | 1321/3108 [31:58<41:45,  1.40s/it] 43%|████▎     | 1322/3108 [32:00<41:48,  1.40s/it] 43%|████▎     | 1323/3108 [32:01<41:40,  1.40s/it] 43%|████▎     | 1324/3108 [32:02<41:44,  1.40s/it] 43%|████▎     | 1325/3108 [32:04<41:40,  1.40s/it] 43%|████▎     | 1326/3108 [32:05<41:41,  1.40s/it] 43%|████▎     | 1327/3108 [32:07<41:39,  1.40s/it] 43%|████▎     | 1328/3108 [32:08<41:42,  1.41s/it] 43%|████▎     | 1329/3108 [32:09<41:43,  1.41s/it] 43%|████▎     | 1330/3108 [32:11<41:43,  1.41s/it] 43%|████▎     | 1331/3108 [32:12<41:36,  1.40s/it] 43%|████▎     | 1332/3108 [32:14<42:33,  1.44s/it] 43%|████▎     | 1333/3108 [32:15<42:15,  1.43s/it] 43%|████▎     | 1334/3108 [32:17<41:55,  1.42s/it] 43%|████▎     | 1335/3108 [32:18<41:53,  1.42s/it] 43%|████▎     | 1336/3108 [32:19<42:36,  1.44s/it] 43%|████▎     | 1337/3108 [32:21<42:58,  1.46s/it] 43%|████▎     | 1338/3108 [32:22<42:32,  1.44s/it] 43%|████▎     | 1339/3108 [32:24<42:15,  1.43s/it] 43%|████▎     | 1340/3108 [32:25<41:52,  1.42s/it] 43%|████▎     | 1341/3108 [32:27<41:45,  1.42s/it] 43%|████▎     | 1342/3108 [32:28<41:33,  1.41s/it] 43%|████▎     | 1343/3108 [32:29<41:37,  1.42s/it] 43%|████▎     | 1344/3108 [32:31<41:25,  1.41s/it] 43%|████▎     | 1345/3108 [32:32<41:24,  1.41s/it] 43%|████▎     | 1346/3108 [32:34<41:14,  1.40s/it] 43%|████▎     | 1347/3108 [32:35<41:19,  1.41s/it] 43%|████▎     | 1348/3108 [32:36<41:12,  1.40s/it] 43%|████▎     | 1349/3108 [32:38<41:10,  1.40s/it] 43%|████▎     | 1350/3108 [32:39<41:05,  1.40s/it] 43%|████▎     | 1351/3108 [32:41<41:06,  1.40s/it] 44%|████▎     | 1352/3108 [32:42<40:59,  1.40s/it] 44%|████▎     | 1353/3108 [32:43<41:03,  1.40s/it] 44%|████▎     | 1354/3108 [32:45<41:00,  1.40s/it] 44%|████▎     | 1355/3108 [32:46<41:10,  1.41s/it] 44%|████▎     | 1356/3108 [32:48<41:05,  1.41s/it] 44%|████▎     | 1357/3108 [32:49<41:06,  1.41s/it] 44%|████▎     | 1358/3108 [32:51<41:54,  1.44s/it] 44%|████▎     | 1359/3108 [32:52<41:37,  1.43s/it] 44%|████▍     | 1360/3108 [32:53<41:29,  1.42s/it] 44%|████▍     | 1361/3108 [32:55<42:07,  1.45s/it] 44%|████▍     | 1362/3108 [32:56<42:23,  1.46s/it] 44%|████▍     | 1363/3108 [32:58<41:51,  1.44s/it] 44%|████▍     | 1364/3108 [32:59<41:31,  1.43s/it] 44%|████▍     | 1365/3108 [33:01<41:15,  1.42s/it] 44%|████▍     | 1366/3108 [33:02<41:09,  1.42s/it] 44%|████▍     | 1367/3108 [33:03<41:00,  1.41s/it] 44%|████▍     | 1368/3108 [33:05<40:50,  1.41s/it] 44%|████▍     | 1369/3108 [33:06<40:44,  1.41s/it] 44%|████▍     | 1370/3108 [33:08<40:45,  1.41s/it] 44%|████▍     | 1371/3108 [33:09<40:42,  1.41s/it] 44%|████▍     | 1372/3108 [33:10<40:35,  1.40s/it] 44%|████▍     | 1373/3108 [33:12<40:30,  1.40s/it] 44%|████▍     | 1374/3108 [33:13<40:21,  1.40s/it] 44%|████▍     | 1375/3108 [33:15<40:23,  1.40s/it] 44%|████▍     | 1376/3108 [33:16<40:19,  1.40s/it] 44%|████▍     | 1377/3108 [33:17<40:14,  1.39s/it] 44%|████▍     | 1378/3108 [33:19<40:18,  1.40s/it] 44%|████▍     | 1379/3108 [33:20<40:20,  1.40s/it] 44%|████▍     | 1380/3108 [33:22<40:23,  1.40s/it] 44%|████▍     | 1381/3108 [33:23<40:18,  1.40s/it] 44%|████▍     | 1382/3108 [33:24<40:11,  1.40s/it] 44%|████▍     | 1383/3108 [33:26<41:12,  1.43s/it] 45%|████▍     | 1384/3108 [33:27<40:51,  1.42s/it] 45%|████▍     | 1385/3108 [33:29<40:42,  1.42s/it] 45%|████▍     | 1386/3108 [33:30<41:28,  1.45s/it] 45%|████▍     | 1387/3108 [33:32<41:43,  1.45s/it] 45%|████▍     | 1388/3108 [33:33<41:11,  1.44s/it] 45%|████▍     | 1389/3108 [33:34<40:54,  1.43s/it] 45%|████▍     | 1390/3108 [33:36<40:35,  1.42s/it] 45%|████▍     | 1391/3108 [33:37<40:30,  1.42s/it] 45%|████▍     | 1392/3108 [33:39<40:13,  1.41s/it] 45%|████▍     | 1393/3108 [33:40<40:08,  1.40s/it] 45%|████▍     | 1394/3108 [33:41<40:01,  1.40s/it] 45%|████▍     | 1395/3108 [33:43<39:55,  1.40s/it] 45%|████▍     | 1396/3108 [33:44<39:50,  1.40s/it] 45%|████▍     | 1397/3108 [33:46<39:50,  1.40s/it] 45%|████▍     | 1398/3108 [33:47<39:50,  1.40s/it] 45%|████▌     | 1399/3108 [33:48<39:49,  1.40s/it] 45%|████▌     | 1400/3108 [33:50<39:47,  1.40s/it] 45%|████▌     | 1401/3108 [33:51<39:50,  1.40s/it] 45%|████▌     | 1402/3108 [33:53<39:47,  1.40s/it] 45%|████▌     | 1403/3108 [33:54<39:48,  1.40s/it] 45%|████▌     | 1404/3108 [33:55<39:41,  1.40s/it] 45%|████▌     | 1405/3108 [33:57<39:42,  1.40s/it] 45%|████▌     | 1406/3108 [33:58<39:43,  1.40s/it] 45%|████▌     | 1407/3108 [34:00<39:35,  1.40s/it] 45%|████▌     | 1408/3108 [34:01<40:36,  1.43s/it] 45%|████▌     | 1409/3108 [34:03<40:12,  1.42s/it] 45%|████▌     | 1410/3108 [34:04<40:01,  1.41s/it] 45%|████▌     | 1411/3108 [34:05<39:49,  1.41s/it] 45%|████▌     | 1412/3108 [34:07<40:29,  1.43s/it] 45%|████▌     | 1413/3108 [34:08<40:45,  1.44s/it] 45%|████▌     | 1414/3108 [34:10<40:21,  1.43s/it] 46%|████▌     | 1415/3108 [34:11<40:10,  1.42s/it] 46%|████▌     | 1416/3108 [34:12<39:57,  1.42s/it] 46%|████▌     | 1417/3108 [34:14<39:49,  1.41s/it] 46%|████▌     | 1418/3108 [34:15<39:46,  1.41s/it] 46%|████▌     | 1419/3108 [34:17<39:41,  1.41s/it] 46%|████▌     | 1420/3108 [34:18<39:37,  1.41s/it] 46%|████▌     | 1421/3108 [34:20<39:35,  1.41s/it] 46%|████▌     | 1422/3108 [34:21<39:31,  1.41s/it] 46%|████▌     | 1423/3108 [34:22<39:27,  1.41s/it] 46%|████▌     | 1424/3108 [34:24<39:24,  1.40s/it] 46%|████▌     | 1425/3108 [34:25<39:20,  1.40s/it] 46%|████▌     | 1426/3108 [34:27<39:18,  1.40s/it] 46%|████▌     | 1427/3108 [34:28<39:16,  1.40s/it] 46%|████▌     | 1428/3108 [34:29<39:18,  1.40s/it] 46%|████▌     | 1429/3108 [34:31<39:14,  1.40s/it] 46%|████▌     | 1430/3108 [34:32<39:16,  1.40s/it] 46%|████▌     | 1431/3108 [34:34<39:12,  1.40s/it] 46%|████▌     | 1432/3108 [34:35<39:10,  1.40s/it] 46%|████▌     | 1433/3108 [34:36<39:08,  1.40s/it] 46%|████▌     | 1434/3108 [34:38<40:00,  1.43s/it] 46%|████▌     | 1435/3108 [34:39<39:39,  1.42s/it] 46%|████▌     | 1436/3108 [34:41<39:30,  1.42s/it] 46%|████▌     | 1437/3108 [34:42<39:18,  1.41s/it] 46%|████▋     | 1438/3108 [34:44<40:05,  1.44s/it] 46%|████▋     | 1439/3108 [34:45<40:26,  1.45s/it] 46%|████▋     | 1440/3108 [34:46<39:58,  1.44s/it] 46%|████▋     | 1441/3108 [34:48<39:36,  1.43s/it] 46%|████▋     | 1442/3108 [34:49<39:26,  1.42s/it] 46%|████▋     | 1443/3108 [34:51<39:19,  1.42s/it] 46%|████▋     | 1444/3108 [34:52<39:08,  1.41s/it] 46%|████▋     | 1445/3108 [34:53<39:07,  1.41s/it] 47%|████▋     | 1446/3108 [34:55<39:05,  1.41s/it] 47%|████▋     | 1447/3108 [34:56<38:58,  1.41s/it] 47%|████▋     | 1448/3108 [34:58<38:52,  1.40s/it] 47%|████▋     | 1449/3108 [34:59<38:47,  1.40s/it] 47%|████▋     | 1450/3108 [35:00<38:49,  1.41s/it] 47%|████▋     | 1451/3108 [35:02<38:50,  1.41s/it] 47%|████▋     | 1452/3108 [35:03<38:40,  1.40s/it] 47%|████▋     | 1453/3108 [35:05<38:42,  1.40s/it] 47%|████▋     | 1454/3108 [35:06<38:42,  1.40s/it] 47%|████▋     | 1455/3108 [35:07<38:43,  1.41s/it] 47%|████▋     | 1456/3108 [35:09<38:47,  1.41s/it] 47%|████▋     | 1457/3108 [35:10<38:46,  1.41s/it] 47%|████▋     | 1458/3108 [35:12<38:46,  1.41s/it] 47%|████▋     | 1459/3108 [35:13<38:43,  1.41s/it] 47%|████▋     | 1460/3108 [35:15<39:37,  1.44s/it] 47%|████▋     | 1461/3108 [35:16<39:19,  1.43s/it] 47%|████▋     | 1462/3108 [35:17<39:04,  1.42s/it] 47%|████▋     | 1463/3108 [35:19<39:45,  1.45s/it] 47%|████▋     | 1464/3108 [35:20<39:57,  1.46s/it] 47%|████▋     | 1465/3108 [35:22<39:25,  1.44s/it] 47%|████▋     | 1466/3108 [35:23<39:04,  1.43s/it] 47%|████▋     | 1467/3108 [35:25<38:52,  1.42s/it] 47%|████▋     | 1468/3108 [35:26<38:41,  1.42s/it] 47%|████▋     | 1469/3108 [35:27<38:37,  1.41s/it] 47%|████▋     | 1470/3108 [35:29<38:29,  1.41s/it] 47%|████▋     | 1471/3108 [35:30<38:27,  1.41s/it] 47%|████▋     | 1472/3108 [35:32<38:26,  1.41s/it] 47%|████▋     | 1473/3108 [35:33<38:27,  1.41s/it] 47%|████▋     | 1474/3108 [35:35<38:26,  1.41s/it] 47%|████▋     | 1475/3108 [35:36<38:22,  1.41s/it] 47%|████▋     | 1476/3108 [35:37<38:17,  1.41s/it] 48%|████▊     | 1477/3108 [35:39<38:17,  1.41s/it] 48%|████▊     | 1478/3108 [35:40<38:10,  1.41s/it] 48%|████▊     | 1479/3108 [35:42<38:08,  1.40s/it] 48%|████▊     | 1480/3108 [35:43<38:05,  1.40s/it] 48%|████▊     | 1481/3108 [35:44<38:07,  1.41s/it] 48%|████▊     | 1482/3108 [35:46<37:57,  1.40s/it] 48%|████▊     | 1483/3108 [35:47<37:55,  1.40s/it] 48%|████▊     | 1484/3108 [35:49<37:55,  1.40s/it] 48%|████▊     | 1485/3108 [35:50<38:49,  1.44s/it] 48%|████▊     | 1486/3108 [35:51<38:28,  1.42s/it] 48%|████▊     | 1487/3108 [35:53<38:20,  1.42s/it] 48%|████▊     | 1488/3108 [35:54<39:00,  1.44s/it] 48%|████▊     | 1489/3108 [35:56<39:14,  1.45s/it] 48%|████▊     | 1490/3108 [35:57<38:44,  1.44s/it] 48%|████▊     | 1491/3108 [35:59<38:23,  1.42s/it] 48%|████▊     | 1492/3108 [36:00<38:13,  1.42s/it] 48%|████▊     | 1493/3108 [36:01<38:05,  1.42s/it] 48%|████▊     | 1494/3108 [36:03<37:56,  1.41s/it] 48%|████▊     | 1495/3108 [36:04<37:45,  1.40s/it] 48%|████▊     | 1496/3108 [36:06<37:41,  1.40s/it] 48%|████▊     | 1497/3108 [36:07<37:38,  1.40s/it] 48%|████▊     | 1498/3108 [36:08<37:39,  1.40s/it] 48%|████▊     | 1499/3108 [36:10<37:39,  1.40s/it] 48%|████▊     | 1500/3108 [36:11<37:32,  1.40s/it]                                                   loss: 0.9413656, learning_rate: 1.089e-05, global_step: 1500, interval_runtime: 752.1612, interval_samples_per_second: 11.965519651931583, interval_steps_per_second: 0.6647510917739768, epoch: 1.9305
 48%|████▊     | 1500/3108 [36:11<37:32,  1.40s/it] 48%|████▊     | 1501/3108 [36:13<37:56,  1.42s/it] 48%|████▊     | 1502/3108 [36:14<37:44,  1.41s/it] 48%|████▊     | 1503/3108 [36:16<37:43,  1.41s/it] 48%|████▊     | 1504/3108 [36:17<37:38,  1.41s/it] 48%|████▊     | 1505/3108 [36:18<37:32,  1.41s/it] 48%|████▊     | 1506/3108 [36:20<37:26,  1.40s/it] 48%|████▊     | 1507/3108 [36:21<37:25,  1.40s/it] 49%|████▊     | 1508/3108 [36:23<37:21,  1.40s/it] 49%|████▊     | 1509/3108 [36:24<37:20,  1.40s/it] 49%|████▊     | 1510/3108 [36:25<38:17,  1.44s/it] 49%|████▊     | 1511/3108 [36:27<38:05,  1.43s/it] 49%|████▊     | 1512/3108 [36:28<37:44,  1.42s/it] 49%|████▊     | 1513/3108 [36:30<37:32,  1.41s/it] 49%|████▊     | 1514/3108 [36:31<38:09,  1.44s/it] 49%|████▊     | 1515/3108 [36:33<38:30,  1.45s/it] 49%|████▉     | 1516/3108 [36:34<38:05,  1.44s/it] 49%|████▉     | 1517/3108 [36:35<37:51,  1.43s/it] 49%|████▉     | 1518/3108 [36:37<37:38,  1.42s/it] 49%|████▉     | 1519/3108 [36:38<37:31,  1.42s/it] 49%|████▉     | 1520/3108 [36:40<37:20,  1.41s/it] 49%|████▉     | 1521/3108 [36:41<37:15,  1.41s/it] 49%|████▉     | 1522/3108 [36:42<37:11,  1.41s/it] 49%|████▉     | 1523/3108 [36:44<37:13,  1.41s/it] 49%|████▉     | 1524/3108 [36:45<37:06,  1.41s/it] 49%|████▉     | 1525/3108 [36:47<37:05,  1.41s/it] 49%|████▉     | 1526/3108 [36:48<36:57,  1.40s/it] 49%|████▉     | 1527/3108 [36:49<36:55,  1.40s/it] 49%|████▉     | 1528/3108 [36:51<36:57,  1.40s/it] 49%|████▉     | 1529/3108 [36:52<37:00,  1.41s/it] 49%|████▉     | 1530/3108 [36:54<36:55,  1.40s/it] 49%|████▉     | 1531/3108 [36:55<36:49,  1.40s/it] 49%|████▉     | 1532/3108 [36:56<36:51,  1.40s/it] 49%|████▉     | 1533/3108 [36:58<36:50,  1.40s/it] 49%|████▉     | 1534/3108 [36:59<36:46,  1.40s/it] 49%|████▉     | 1535/3108 [37:01<36:49,  1.40s/it] 49%|████▉     | 1536/3108 [37:02<37:34,  1.43s/it] 49%|████▉     | 1537/3108 [37:04<37:18,  1.42s/it] 49%|████▉     | 1538/3108 [37:05<37:09,  1.42s/it] 50%|████▉     | 1539/3108 [37:06<37:05,  1.42s/it] 50%|████▉     | 1540/3108 [37:08<37:45,  1.44s/it] 50%|████▉     | 1541/3108 [37:09<37:57,  1.45s/it] 50%|████▉     | 1542/3108 [37:11<37:29,  1.44s/it] 50%|████▉     | 1543/3108 [37:12<37:15,  1.43s/it] 50%|████▉     | 1544/3108 [37:14<37:04,  1.42s/it] 50%|████▉     | 1545/3108 [37:15<37:00,  1.42s/it] 50%|████▉     | 1546/3108 [37:16<36:55,  1.42s/it] 50%|████▉     | 1547/3108 [37:18<36:51,  1.42s/it] 50%|████▉     | 1548/3108 [37:19<36:47,  1.41s/it] 50%|████▉     | 1549/3108 [37:21<35:41,  1.37s/it] 50%|████▉     | 1550/3108 [37:22<33:17,  1.28s/it] 50%|████▉     | 1551/3108 [37:23<31:33,  1.22s/it] 50%|████▉     | 1552/3108 [37:24<30:19,  1.17s/it] 50%|████▉     | 1553/3108 [37:25<29:26,  1.14s/it] 50%|█████     | 1554/3108 [37:26<26:59,  1.04s/it] 50%|█████     | 1555/3108 [37:29<45:19,  1.75s/it] 50%|█████     | 1556/3108 [37:30<42:34,  1.65s/it] 50%|█████     | 1557/3108 [37:32<40:42,  1.57s/it] 50%|█████     | 1558/3108 [37:33<39:24,  1.53s/it] 50%|█████     | 1559/3108 [37:35<38:31,  1.49s/it] 50%|█████     | 1560/3108 [37:36<38:38,  1.50s/it] 50%|█████     | 1561/3108 [37:38<37:48,  1.47s/it] 50%|█████     | 1562/3108 [37:39<37:22,  1.45s/it] 50%|█████     | 1563/3108 [37:40<37:01,  1.44s/it] 50%|█████     | 1564/3108 [37:42<36:48,  1.43s/it] 50%|█████     | 1565/3108 [37:43<36:37,  1.42s/it] 50%|█████     | 1566/3108 [37:45<37:13,  1.45s/it] 50%|█████     | 1567/3108 [37:46<37:28,  1.46s/it] 50%|█████     | 1568/3108 [37:48<36:57,  1.44s/it] 50%|█████     | 1569/3108 [37:49<36:37,  1.43s/it] 51%|█████     | 1570/3108 [37:50<36:27,  1.42s/it] 51%|█████     | 1571/3108 [37:52<36:20,  1.42s/it] 51%|█████     | 1572/3108 [37:53<36:08,  1.41s/it] 51%|█████     | 1573/3108 [37:55<35:58,  1.41s/it] 51%|█████     | 1574/3108 [37:56<35:56,  1.41s/it] 51%|█████     | 1575/3108 [37:57<35:51,  1.40s/it] 51%|█████     | 1576/3108 [37:59<35:53,  1.41s/it] 51%|█████     | 1577/3108 [38:00<35:57,  1.41s/it] 51%|█████     | 1578/3108 [38:02<35:55,  1.41s/it] 51%|█████     | 1579/3108 [38:03<35:49,  1.41s/it] 51%|█████     | 1580/3108 [38:04<35:50,  1.41s/it] 51%|█████     | 1581/3108 [38:06<35:48,  1.41s/it] 51%|█████     | 1582/3108 [38:07<35:47,  1.41s/it] 51%|█████     | 1583/3108 [38:09<35:42,  1.40s/it] 51%|█████     | 1584/3108 [38:10<35:44,  1.41s/it] 51%|█████     | 1585/3108 [38:11<35:43,  1.41s/it] 51%|█████     | 1586/3108 [38:13<36:30,  1.44s/it] 51%|█████     | 1587/3108 [38:14<36:12,  1.43s/it] 51%|█████     | 1588/3108 [38:16<35:59,  1.42s/it] 51%|█████     | 1589/3108 [38:17<35:53,  1.42s/it] 51%|█████     | 1590/3108 [38:19<35:48,  1.42s/it] 51%|█████     | 1591/3108 [38:20<35:39,  1.41s/it] 51%|█████     | 1592/3108 [38:22<36:20,  1.44s/it] 51%|█████▏    | 1593/3108 [38:23<36:33,  1.45s/it] 51%|█████▏    | 1594/3108 [38:24<36:10,  1.43s/it] 51%|█████▏    | 1595/3108 [38:26<35:58,  1.43s/it] 51%|█████▏    | 1596/3108 [38:27<35:50,  1.42s/it] 51%|█████▏    | 1597/3108 [38:29<35:34,  1.41s/it] 51%|█████▏    | 1598/3108 [38:30<35:31,  1.41s/it] 51%|█████▏    | 1599/3108 [38:31<35:20,  1.41s/it] 51%|█████▏    | 1600/3108 [38:33<35:24,  1.41s/it] 52%|█████▏    | 1601/3108 [38:34<35:21,  1.41s/it] 52%|█████▏    | 1602/3108 [38:36<35:15,  1.40s/it] 52%|█████▏    | 1603/3108 [38:37<35:08,  1.40s/it] 52%|█████▏    | 1604/3108 [38:38<35:11,  1.40s/it] 52%|█████▏    | 1605/3108 [38:40<35:05,  1.40s/it] 52%|█████▏    | 1606/3108 [38:41<35:11,  1.41s/it] 52%|█████▏    | 1607/3108 [38:43<35:10,  1.41s/it] 52%|█████▏    | 1608/3108 [38:44<35:11,  1.41s/it] 52%|█████▏    | 1609/3108 [38:45<35:05,  1.40s/it] 52%|█████▏    | 1610/3108 [38:47<35:04,  1.40s/it] 52%|█████▏    | 1611/3108 [38:48<35:52,  1.44s/it] 52%|█████▏    | 1612/3108 [38:50<35:32,  1.43s/it] 52%|█████▏    | 1613/3108 [38:51<35:23,  1.42s/it] 52%|█████▏    | 1614/3108 [38:53<35:12,  1.41s/it] 52%|█████▏    | 1615/3108 [38:54<35:03,  1.41s/it] 52%|█████▏    | 1616/3108 [38:55<35:01,  1.41s/it] 52%|█████▏    | 1617/3108 [38:57<34:53,  1.40s/it] 52%|█████▏    | 1618/3108 [38:58<35:34,  1.43s/it] 52%|█████▏    | 1619/3108 [39:00<35:53,  1.45s/it] 52%|█████▏    | 1620/3108 [39:01<35:32,  1.43s/it] 52%|█████▏    | 1621/3108 [39:03<35:15,  1.42s/it] 52%|█████▏    | 1622/3108 [39:04<35:04,  1.42s/it] 52%|█████▏    | 1623/3108 [39:05<34:58,  1.41s/it] 52%|█████▏    | 1624/3108 [39:07<34:53,  1.41s/it] 52%|█████▏    | 1625/3108 [39:08<34:41,  1.40s/it] 52%|█████▏    | 1626/3108 [39:10<34:37,  1.40s/it] 52%|█████▏    | 1627/3108 [39:11<34:34,  1.40s/it] 52%|█████▏    | 1628/3108 [39:12<34:36,  1.40s/it] 52%|█████▏    | 1629/3108 [39:14<34:31,  1.40s/it] 52%|█████▏    | 1630/3108 [39:15<34:33,  1.40s/it] 52%|█████▏    | 1631/3108 [39:17<34:36,  1.41s/it] 53%|█████▎    | 1632/3108 [39:18<34:37,  1.41s/it] 53%|█████▎    | 1633/3108 [39:19<34:32,  1.40s/it] 53%|█████▎    | 1634/3108 [39:21<34:24,  1.40s/it] 53%|█████▎    | 1635/3108 [39:22<34:24,  1.40s/it] 53%|█████▎    | 1636/3108 [39:24<35:19,  1.44s/it] 53%|█████▎    | 1637/3108 [39:25<34:59,  1.43s/it] 53%|█████▎    | 1638/3108 [39:27<34:52,  1.42s/it] 53%|█████▎    | 1639/3108 [39:28<34:41,  1.42s/it] 53%|█████▎    | 1640/3108 [39:29<34:30,  1.41s/it] 53%|█████▎    | 1641/3108 [39:31<34:22,  1.41s/it] 53%|█████▎    | 1642/3108 [39:32<34:23,  1.41s/it] 53%|█████▎    | 1643/3108 [39:34<35:04,  1.44s/it] 53%|█████▎    | 1644/3108 [39:35<35:27,  1.45s/it] 53%|█████▎    | 1645/3108 [39:37<35:03,  1.44s/it] 53%|█████▎    | 1646/3108 [39:38<34:50,  1.43s/it] 53%|█████▎    | 1647/3108 [39:39<34:39,  1.42s/it] 53%|█████▎    | 1648/3108 [39:41<34:33,  1.42s/it] 53%|█████▎    | 1649/3108 [39:42<34:28,  1.42s/it] 53%|█████▎    | 1650/3108 [39:44<34:22,  1.41s/it] 53%|█████▎    | 1651/3108 [39:45<34:20,  1.41s/it] 53%|█████▎    | 1652/3108 [39:46<34:17,  1.41s/it] 53%|█████▎    | 1653/3108 [39:48<34:06,  1.41s/it] 53%|█████▎    | 1654/3108 [39:49<34:04,  1.41s/it] 53%|█████▎    | 1655/3108 [39:51<34:01,  1.40s/it] 53%|█████▎    | 1656/3108 [39:52<34:02,  1.41s/it] 53%|█████▎    | 1657/3108 [39:53<33:54,  1.40s/it] 53%|█████▎    | 1658/3108 [39:55<33:54,  1.40s/it] 53%|█████▎    | 1659/3108 [39:56<33:53,  1.40s/it] 53%|█████▎    | 1660/3108 [39:58<33:53,  1.40s/it] 53%|█████▎    | 1661/3108 [39:59<33:47,  1.40s/it] 53%|█████▎    | 1662/3108 [40:01<34:31,  1.43s/it] 54%|█████▎    | 1663/3108 [40:02<34:12,  1.42s/it] 54%|█████▎    | 1664/3108 [40:03<34:04,  1.42s/it] 54%|█████▎    | 1665/3108 [40:05<33:55,  1.41s/it] 54%|█████▎    | 1666/3108 [40:06<33:55,  1.41s/it] 54%|█████▎    | 1667/3108 [40:08<33:54,  1.41s/it] 54%|█████▎    | 1668/3108 [40:09<34:28,  1.44s/it] 54%|█████▎    | 1669/3108 [40:11<34:49,  1.45s/it] 54%|█████▎    | 1670/3108 [40:12<34:26,  1.44s/it] 54%|█████▍    | 1671/3108 [40:13<34:09,  1.43s/it] 54%|█████▍    | 1672/3108 [40:15<33:55,  1.42s/it] 54%|█████▍    | 1673/3108 [40:16<33:49,  1.41s/it] 54%|█████▍    | 1674/3108 [40:18<33:48,  1.41s/it] 54%|█████▍    | 1675/3108 [40:19<33:45,  1.41s/it] 54%|█████▍    | 1676/3108 [40:20<33:39,  1.41s/it] 54%|█████▍    | 1677/3108 [40:22<33:35,  1.41s/it] 54%|█████▍    | 1678/3108 [40:23<33:34,  1.41s/it] 54%|█████▍    | 1679/3108 [40:25<33:28,  1.41s/it] 54%|█████▍    | 1680/3108 [40:26<33:25,  1.40s/it] 54%|█████▍    | 1681/3108 [40:27<33:21,  1.40s/it] 54%|█████▍    | 1682/3108 [40:29<33:23,  1.40s/it] 54%|█████▍    | 1683/3108 [40:30<33:22,  1.41s/it] 54%|█████▍    | 1684/3108 [40:32<33:19,  1.40s/it] 54%|█████▍    | 1685/3108 [40:33<33:18,  1.40s/it] 54%|█████▍    | 1686/3108 [40:34<33:16,  1.40s/it] 54%|█████▍    | 1687/3108 [40:36<33:13,  1.40s/it] 54%|█████▍    | 1688/3108 [40:37<34:00,  1.44s/it] 54%|█████▍    | 1689/3108 [40:39<33:41,  1.42s/it] 54%|█████▍    | 1690/3108 [40:40<33:33,  1.42s/it] 54%|█████▍    | 1691/3108 [40:42<33:29,  1.42s/it] 54%|█████▍    | 1692/3108 [40:43<33:23,  1.42s/it] 54%|█████▍    | 1693/3108 [40:44<33:13,  1.41s/it] 55%|█████▍    | 1694/3108 [40:46<33:46,  1.43s/it] 55%|█████▍    | 1695/3108 [40:47<34:10,  1.45s/it] 55%|█████▍    | 1696/3108 [40:49<33:46,  1.44s/it] 55%|█████▍    | 1697/3108 [40:50<33:34,  1.43s/it] 55%|█████▍    | 1698/3108 [40:52<33:25,  1.42s/it] 55%|█████▍    | 1699/3108 [40:53<33:12,  1.41s/it] 55%|█████▍    | 1700/3108 [40:54<33:07,  1.41s/it] 55%|█████▍    | 1701/3108 [40:56<32:57,  1.41s/it] 55%|█████▍    | 1702/3108 [40:57<32:51,  1.40s/it] 55%|█████▍    | 1703/3108 [40:59<32:53,  1.40s/it] 55%|█████▍    | 1704/3108 [41:00<32:47,  1.40s/it] 55%|█████▍    | 1705/3108 [41:01<32:43,  1.40s/it] 55%|█████▍    | 1706/3108 [41:03<32:42,  1.40s/it] 55%|█████▍    | 1707/3108 [41:04<32:42,  1.40s/it] 55%|█████▍    | 1708/3108 [41:06<32:42,  1.40s/it] 55%|█████▍    | 1709/3108 [41:07<32:42,  1.40s/it] 55%|█████▌    | 1710/3108 [41:08<32:39,  1.40s/it] 55%|█████▌    | 1711/3108 [41:10<32:40,  1.40s/it] 55%|█████▌    | 1712/3108 [41:11<32:38,  1.40s/it] 55%|█████▌    | 1713/3108 [41:13<33:25,  1.44s/it] 55%|█████▌    | 1714/3108 [41:14<33:08,  1.43s/it] 55%|█████▌    | 1715/3108 [41:15<32:59,  1.42s/it] 55%|█████▌    | 1716/3108 [41:17<32:48,  1.41s/it] 55%|█████▌    | 1717/3108 [41:18<32:44,  1.41s/it] 55%|█████▌    | 1718/3108 [41:20<32:35,  1.41s/it] 55%|█████▌    | 1719/3108 [41:21<32:33,  1.41s/it] 55%|█████▌    | 1720/3108 [41:23<33:09,  1.43s/it] 55%|█████▌    | 1721/3108 [41:24<33:28,  1.45s/it] 55%|█████▌    | 1722/3108 [41:25<33:06,  1.43s/it] 55%|█████▌    | 1723/3108 [41:27<32:52,  1.42s/it] 55%|█████▌    | 1724/3108 [41:28<32:44,  1.42s/it] 56%|█████▌    | 1725/3108 [41:30<32:37,  1.42s/it] 56%|█████▌    | 1726/3108 [41:31<32:26,  1.41s/it] 56%|█████▌    | 1727/3108 [41:32<32:23,  1.41s/it] 56%|█████▌    | 1728/3108 [41:34<32:17,  1.40s/it] 56%|█████▌    | 1729/3108 [41:35<32:19,  1.41s/it] 56%|█████▌    | 1730/3108 [41:37<32:13,  1.40s/it] 56%|█████▌    | 1731/3108 [41:38<32:12,  1.40s/it] 56%|█████▌    | 1732/3108 [41:39<32:06,  1.40s/it] 56%|█████▌    | 1733/3108 [41:41<32:08,  1.40s/it] 56%|█████▌    | 1734/3108 [41:42<32:09,  1.40s/it] 56%|█████▌    | 1735/3108 [41:44<32:09,  1.41s/it] 56%|█████▌    | 1736/3108 [41:45<32:07,  1.40s/it] 56%|█████▌    | 1737/3108 [41:46<32:06,  1.40s/it] 56%|█████▌    | 1738/3108 [41:48<32:53,  1.44s/it] 56%|█████▌    | 1739/3108 [41:49<32:36,  1.43s/it] 56%|█████▌    | 1740/3108 [41:51<32:23,  1.42s/it] 56%|█████▌    | 1741/3108 [41:52<32:14,  1.42s/it] 56%|█████▌    | 1742/3108 [41:54<32:09,  1.41s/it] 56%|█████▌    | 1743/3108 [41:55<32:04,  1.41s/it] 56%|█████▌    | 1744/3108 [41:56<32:01,  1.41s/it] 56%|█████▌    | 1745/3108 [41:58<32:45,  1.44s/it] 56%|█████▌    | 1746/3108 [41:59<33:02,  1.46s/it] 56%|█████▌    | 1747/3108 [42:01<32:38,  1.44s/it] 56%|█████▌    | 1748/3108 [42:02<32:21,  1.43s/it] 56%|█████▋    | 1749/3108 [42:04<32:11,  1.42s/it] 56%|█████▋    | 1750/3108 [42:05<31:59,  1.41s/it] 56%|█████▋    | 1751/3108 [42:06<31:53,  1.41s/it] 56%|█████▋    | 1752/3108 [42:08<31:48,  1.41s/it] 56%|█████▋    | 1753/3108 [42:09<31:43,  1.41s/it] 56%|█████▋    | 1754/3108 [42:11<31:39,  1.40s/it] 56%|█████▋    | 1755/3108 [42:12<31:37,  1.40s/it] 56%|█████▋    | 1756/3108 [42:13<31:36,  1.40s/it] 57%|█████▋    | 1757/3108 [42:15<31:39,  1.41s/it] 57%|█████▋    | 1758/3108 [42:16<31:32,  1.40s/it] 57%|█████▋    | 1759/3108 [42:18<31:26,  1.40s/it] 57%|█████▋    | 1760/3108 [42:19<31:25,  1.40s/it] 57%|█████▋    | 1761/3108 [42:20<31:27,  1.40s/it] 57%|█████▋    | 1762/3108 [42:22<31:24,  1.40s/it] 57%|█████▋    | 1763/3108 [42:23<31:23,  1.40s/it] 57%|█████▋    | 1764/3108 [42:25<32:04,  1.43s/it] 57%|█████▋    | 1765/3108 [42:26<31:50,  1.42s/it] 57%|█████▋    | 1766/3108 [42:28<31:39,  1.42s/it] 57%|█████▋    | 1767/3108 [42:29<31:32,  1.41s/it] 57%|█████▋    | 1768/3108 [42:30<31:28,  1.41s/it] 57%|█████▋    | 1769/3108 [42:32<31:26,  1.41s/it] 57%|█████▋    | 1770/3108 [42:33<32:05,  1.44s/it] 57%|█████▋    | 1771/3108 [42:35<32:24,  1.45s/it] 57%|█████▋    | 1772/3108 [42:36<31:58,  1.44s/it] 57%|█████▋    | 1773/3108 [42:38<31:40,  1.42s/it] 57%|█████▋    | 1774/3108 [42:39<31:30,  1.42s/it] 57%|█████▋    | 1775/3108 [42:40<31:25,  1.41s/it] 57%|█████▋    | 1776/3108 [42:42<31:19,  1.41s/it] 57%|█████▋    | 1777/3108 [42:43<31:16,  1.41s/it] 57%|█████▋    | 1778/3108 [42:45<31:14,  1.41s/it] 57%|█████▋    | 1779/3108 [42:46<31:10,  1.41s/it] 57%|█████▋    | 1780/3108 [42:47<31:05,  1.40s/it] 57%|█████▋    | 1781/3108 [42:49<31:05,  1.41s/it] 57%|█████▋    | 1782/3108 [42:50<31:02,  1.40s/it] 57%|█████▋    | 1783/3108 [42:52<31:00,  1.40s/it] 57%|█████▋    | 1784/3108 [42:53<30:56,  1.40s/it] 57%|█████▋    | 1785/3108 [42:54<30:52,  1.40s/it] 57%|█████▋    | 1786/3108 [42:56<30:54,  1.40s/it] 57%|█████▋    | 1787/3108 [42:57<30:54,  1.40s/it] 58%|█████▊    | 1788/3108 [42:59<30:50,  1.40s/it] 58%|█████▊    | 1789/3108 [43:00<30:48,  1.40s/it] 58%|█████▊    | 1790/3108 [43:02<31:29,  1.43s/it] 58%|█████▊    | 1791/3108 [43:03<31:15,  1.42s/it] 58%|█████▊    | 1792/3108 [43:04<31:03,  1.42s/it] 58%|█████▊    | 1793/3108 [43:06<30:56,  1.41s/it] 58%|█████▊    | 1794/3108 [43:07<30:49,  1.41s/it] 58%|█████▊    | 1795/3108 [43:09<30:43,  1.40s/it] 58%|█████▊    | 1796/3108 [43:10<31:18,  1.43s/it] 58%|█████▊    | 1797/3108 [43:11<31:34,  1.45s/it] 58%|█████▊    | 1798/3108 [43:13<31:16,  1.43s/it] 58%|█████▊    | 1799/3108 [43:14<31:06,  1.43s/it] 58%|█████▊    | 1800/3108 [43:16<30:54,  1.42s/it] 58%|█████▊    | 1801/3108 [43:17<30:42,  1.41s/it] 58%|█████▊    | 1802/3108 [43:18<30:34,  1.40s/it] 58%|█████▊    | 1803/3108 [43:20<30:35,  1.41s/it] 58%|█████▊    | 1804/3108 [43:21<30:29,  1.40s/it] 58%|█████▊    | 1805/3108 [43:23<30:33,  1.41s/it] 58%|█████▊    | 1806/3108 [43:24<30:30,  1.41s/it] 58%|█████▊    | 1807/3108 [43:26<30:25,  1.40s/it] 58%|█████▊    | 1808/3108 [43:27<30:21,  1.40s/it] 58%|█████▊    | 1809/3108 [43:28<30:18,  1.40s/it] 58%|█████▊    | 1810/3108 [43:30<30:18,  1.40s/it] 58%|█████▊    | 1811/3108 [43:31<30:17,  1.40s/it] 58%|█████▊    | 1812/3108 [43:33<30:19,  1.40s/it] 58%|█████▊    | 1813/3108 [43:34<30:19,  1.41s/it] 58%|█████▊    | 1814/3108 [43:35<30:16,  1.40s/it] 58%|█████▊    | 1815/3108 [43:37<30:51,  1.43s/it] 58%|█████▊    | 1816/3108 [43:38<30:35,  1.42s/it] 58%|█████▊    | 1817/3108 [43:40<30:24,  1.41s/it] 58%|█████▊    | 1818/3108 [43:41<30:15,  1.41s/it] 59%|█████▊    | 1819/3108 [43:42<30:08,  1.40s/it] 59%|█████▊    | 1820/3108 [43:44<30:03,  1.40s/it] 59%|█████▊    | 1821/3108 [43:45<30:05,  1.40s/it] 59%|█████▊    | 1822/3108 [43:47<30:43,  1.43s/it] 59%|█████▊    | 1823/3108 [43:48<30:59,  1.45s/it] 59%|█████▊    | 1824/3108 [43:50<30:33,  1.43s/it] 59%|█████▊    | 1825/3108 [43:51<30:19,  1.42s/it] 59%|█████▉    | 1826/3108 [43:52<30:12,  1.41s/it] 59%|█████▉    | 1827/3108 [43:54<30:07,  1.41s/it] 59%|█████▉    | 1828/3108 [43:55<30:00,  1.41s/it] 59%|█████▉    | 1829/3108 [43:57<30:01,  1.41s/it] 59%|█████▉    | 1830/3108 [43:58<29:58,  1.41s/it] 59%|█████▉    | 1831/3108 [43:59<29:52,  1.40s/it] 59%|█████▉    | 1832/3108 [44:01<29:48,  1.40s/it] 59%|█████▉    | 1833/3108 [44:02<29:48,  1.40s/it] 59%|█████▉    | 1834/3108 [44:04<29:49,  1.40s/it] 59%|█████▉    | 1835/3108 [44:05<29:46,  1.40s/it] 59%|█████▉    | 1836/3108 [44:06<29:47,  1.41s/it] 59%|█████▉    | 1837/3108 [44:08<29:43,  1.40s/it] 59%|█████▉    | 1838/3108 [44:09<29:41,  1.40s/it] 59%|█████▉    | 1839/3108 [44:11<29:37,  1.40s/it] 59%|█████▉    | 1840/3108 [44:12<30:21,  1.44s/it] 59%|█████▉    | 1841/3108 [44:14<30:07,  1.43s/it] 59%|█████▉    | 1842/3108 [44:15<29:52,  1.42s/it] 59%|█████▉    | 1843/3108 [44:16<29:43,  1.41s/it] 59%|█████▉    | 1844/3108 [44:18<29:38,  1.41s/it] 59%|█████▉    | 1845/3108 [44:19<29:34,  1.41s/it] 59%|█████▉    | 1846/3108 [44:21<29:30,  1.40s/it] 59%|█████▉    | 1847/3108 [44:22<30:09,  1.43s/it] 59%|█████▉    | 1848/3108 [44:23<30:19,  1.44s/it] 59%|█████▉    | 1849/3108 [44:25<30:03,  1.43s/it] 60%|█████▉    | 1850/3108 [44:26<29:49,  1.42s/it] 60%|█████▉    | 1851/3108 [44:28<29:41,  1.42s/it] 60%|█████▉    | 1852/3108 [44:29<29:32,  1.41s/it] 60%|█████▉    | 1853/3108 [44:30<29:28,  1.41s/it] 60%|█████▉    | 1854/3108 [44:32<29:23,  1.41s/it] 60%|█████▉    | 1855/3108 [44:33<29:17,  1.40s/it] 60%|█████▉    | 1856/3108 [44:35<29:14,  1.40s/it] 60%|█████▉    | 1857/3108 [44:36<29:14,  1.40s/it] 60%|█████▉    | 1858/3108 [44:37<29:16,  1.41s/it] 60%|█████▉    | 1859/3108 [44:39<29:13,  1.40s/it] 60%|█████▉    | 1860/3108 [44:40<29:13,  1.41s/it] 60%|█████▉    | 1861/3108 [44:42<29:08,  1.40s/it] 60%|█████▉    | 1862/3108 [44:43<29:07,  1.40s/it] 60%|█████▉    | 1863/3108 [44:45<29:04,  1.40s/it] 60%|█████▉    | 1864/3108 [44:46<29:01,  1.40s/it] 60%|██████    | 1865/3108 [44:47<29:01,  1.40s/it] 60%|██████    | 1866/3108 [44:49<29:42,  1.44s/it] 60%|██████    | 1867/3108 [44:50<29:27,  1.42s/it] 60%|██████    | 1868/3108 [44:52<29:18,  1.42s/it] 60%|██████    | 1869/3108 [44:53<29:07,  1.41s/it] 60%|██████    | 1870/3108 [44:54<29:06,  1.41s/it] 60%|██████    | 1871/3108 [44:56<28:59,  1.41s/it] 60%|██████    | 1872/3108 [44:57<29:32,  1.43s/it] 60%|██████    | 1873/3108 [44:59<29:44,  1.45s/it] 60%|██████    | 1874/3108 [45:00<29:28,  1.43s/it] 60%|██████    | 1875/3108 [45:02<29:15,  1.42s/it] 60%|██████    | 1876/3108 [45:03<29:04,  1.42s/it] 60%|██████    | 1877/3108 [45:04<28:59,  1.41s/it] 60%|██████    | 1878/3108 [45:06<28:51,  1.41s/it] 60%|██████    | 1879/3108 [45:07<28:44,  1.40s/it] 60%|██████    | 1880/3108 [45:09<28:41,  1.40s/it] 61%|██████    | 1881/3108 [45:10<28:42,  1.40s/it] 61%|██████    | 1882/3108 [45:11<28:36,  1.40s/it] 61%|██████    | 1883/3108 [45:13<28:33,  1.40s/it] 61%|██████    | 1884/3108 [45:14<28:33,  1.40s/it] 61%|██████    | 1885/3108 [45:16<28:33,  1.40s/it] 61%|██████    | 1886/3108 [45:17<28:32,  1.40s/it] 61%|██████    | 1887/3108 [45:18<28:29,  1.40s/it] 61%|██████    | 1888/3108 [45:20<28:30,  1.40s/it] 61%|██████    | 1889/3108 [45:21<28:29,  1.40s/it] 61%|██████    | 1890/3108 [45:23<28:29,  1.40s/it] 61%|██████    | 1891/3108 [45:24<28:26,  1.40s/it] 61%|██████    | 1892/3108 [45:26<29:05,  1.44s/it] 61%|██████    | 1893/3108 [45:27<28:51,  1.43s/it] 61%|██████    | 1894/3108 [45:28<28:40,  1.42s/it] 61%|██████    | 1895/3108 [45:30<28:37,  1.42s/it] 61%|██████    | 1896/3108 [45:31<28:36,  1.42s/it] 61%|██████    | 1897/3108 [45:33<28:32,  1.41s/it] 61%|██████    | 1898/3108 [45:34<28:59,  1.44s/it] 61%|██████    | 1899/3108 [45:36<29:09,  1.45s/it] 61%|██████    | 1900/3108 [45:37<28:52,  1.43s/it] 61%|██████    | 1901/3108 [45:38<28:36,  1.42s/it] 61%|██████    | 1902/3108 [45:40<28:29,  1.42s/it] 61%|██████    | 1903/3108 [45:41<28:23,  1.41s/it] 61%|██████▏   | 1904/3108 [45:43<28:16,  1.41s/it] 61%|██████▏   | 1905/3108 [45:44<28:13,  1.41s/it] 61%|██████▏   | 1906/3108 [45:45<28:13,  1.41s/it] 61%|██████▏   | 1907/3108 [45:47<28:10,  1.41s/it] 61%|██████▏   | 1908/3108 [45:48<28:06,  1.41s/it] 61%|██████▏   | 1909/3108 [45:50<28:04,  1.40s/it] 61%|██████▏   | 1910/3108 [45:51<28:02,  1.40s/it] 61%|██████▏   | 1911/3108 [45:52<28:04,  1.41s/it] 62%|██████▏   | 1912/3108 [45:54<28:04,  1.41s/it] 62%|██████▏   | 1913/3108 [45:55<28:00,  1.41s/it] 62%|██████▏   | 1914/3108 [45:57<27:58,  1.41s/it] 62%|██████▏   | 1915/3108 [45:58<27:59,  1.41s/it] 62%|██████▏   | 1916/3108 [45:59<27:57,  1.41s/it] 62%|██████▏   | 1917/3108 [46:01<28:34,  1.44s/it] 62%|██████▏   | 1918/3108 [46:02<28:23,  1.43s/it] 62%|██████▏   | 1919/3108 [46:04<28:09,  1.42s/it] 62%|██████▏   | 1920/3108 [46:05<28:03,  1.42s/it] 62%|██████▏   | 1921/3108 [46:07<27:55,  1.41s/it] 62%|██████▏   | 1922/3108 [46:08<27:51,  1.41s/it] 62%|██████▏   | 1923/3108 [46:09<27:46,  1.41s/it] 62%|██████▏   | 1924/3108 [46:11<28:15,  1.43s/it] 62%|██████▏   | 1925/3108 [46:12<28:32,  1.45s/it] 62%|██████▏   | 1926/3108 [46:14<28:14,  1.43s/it] 62%|██████▏   | 1927/3108 [46:15<28:02,  1.42s/it] 62%|██████▏   | 1928/3108 [46:17<27:51,  1.42s/it] 62%|██████▏   | 1929/3108 [46:18<27:47,  1.41s/it] 62%|██████▏   | 1930/3108 [46:19<27:45,  1.41s/it] 62%|██████▏   | 1931/3108 [46:21<27:43,  1.41s/it] 62%|██████▏   | 1932/3108 [46:22<27:35,  1.41s/it] 62%|██████▏   | 1933/3108 [46:24<27:34,  1.41s/it] 62%|██████▏   | 1934/3108 [46:25<27:32,  1.41s/it] 62%|██████▏   | 1935/3108 [46:26<27:29,  1.41s/it] 62%|██████▏   | 1936/3108 [46:28<27:28,  1.41s/it] 62%|██████▏   | 1937/3108 [46:29<27:27,  1.41s/it] 62%|██████▏   | 1938/3108 [46:31<27:23,  1.41s/it] 62%|██████▏   | 1939/3108 [46:32<27:23,  1.41s/it] 62%|██████▏   | 1940/3108 [46:33<27:22,  1.41s/it] 62%|██████▏   | 1941/3108 [46:35<27:19,  1.40s/it] 62%|██████▏   | 1942/3108 [46:36<27:55,  1.44s/it] 63%|██████▎   | 1943/3108 [46:38<27:44,  1.43s/it] 63%|██████▎   | 1944/3108 [46:39<27:32,  1.42s/it] 63%|██████▎   | 1945/3108 [46:41<27:28,  1.42s/it] 63%|██████▎   | 1946/3108 [46:42<27:25,  1.42s/it] 63%|██████▎   | 1947/3108 [46:43<27:18,  1.41s/it] 63%|██████▎   | 1948/3108 [46:45<27:14,  1.41s/it] 63%|██████▎   | 1949/3108 [46:46<27:47,  1.44s/it] 63%|██████▎   | 1950/3108 [46:48<27:59,  1.45s/it] 63%|██████▎   | 1951/3108 [46:49<27:39,  1.43s/it] 63%|██████▎   | 1952/3108 [46:51<27:20,  1.42s/it] 63%|██████▎   | 1953/3108 [46:52<27:12,  1.41s/it] 63%|██████▎   | 1954/3108 [46:53<27:05,  1.41s/it] 63%|██████▎   | 1955/3108 [46:55<27:04,  1.41s/it] 63%|██████▎   | 1956/3108 [46:56<26:58,  1.40s/it] 63%|██████▎   | 1957/3108 [46:58<26:58,  1.41s/it] 63%|██████▎   | 1958/3108 [46:59<26:54,  1.40s/it] 63%|██████▎   | 1959/3108 [47:00<26:54,  1.41s/it] 63%|██████▎   | 1960/3108 [47:02<26:53,  1.41s/it] 63%|██████▎   | 1961/3108 [47:03<26:51,  1.41s/it] 63%|██████▎   | 1962/3108 [47:05<26:49,  1.40s/it] 63%|██████▎   | 1963/3108 [47:06<26:47,  1.40s/it] 63%|██████▎   | 1964/3108 [47:07<26:46,  1.40s/it] 63%|██████▎   | 1965/3108 [47:09<26:46,  1.41s/it] 63%|██████▎   | 1966/3108 [47:10<26:47,  1.41s/it] 63%|██████▎   | 1967/3108 [47:12<26:46,  1.41s/it] 63%|██████▎   | 1968/3108 [47:13<27:19,  1.44s/it] 63%|██████▎   | 1969/3108 [47:14<27:05,  1.43s/it] 63%|██████▎   | 1970/3108 [47:16<26:56,  1.42s/it] 63%|██████▎   | 1971/3108 [47:17<26:48,  1.41s/it] 63%|██████▎   | 1972/3108 [47:19<26:45,  1.41s/it] 63%|██████▎   | 1973/3108 [47:20<26:43,  1.41s/it] 64%|██████▎   | 1974/3108 [47:22<27:17,  1.44s/it] 64%|██████▎   | 1975/3108 [47:23<27:29,  1.46s/it] 64%|██████▎   | 1976/3108 [47:25<27:09,  1.44s/it] 64%|██████▎   | 1977/3108 [47:26<26:56,  1.43s/it] 64%|██████▎   | 1978/3108 [47:27<26:46,  1.42s/it] 64%|██████▎   | 1979/3108 [47:29<26:37,  1.41s/it] 64%|██████▎   | 1980/3108 [47:30<26:26,  1.41s/it] 64%|██████▎   | 1981/3108 [47:32<26:23,  1.41s/it] 64%|██████▍   | 1982/3108 [47:33<26:23,  1.41s/it] 64%|██████▍   | 1983/3108 [47:34<26:21,  1.41s/it] 64%|██████▍   | 1984/3108 [47:36<26:14,  1.40s/it] 64%|██████▍   | 1985/3108 [47:37<26:12,  1.40s/it] 64%|██████▍   | 1986/3108 [47:39<26:08,  1.40s/it] 64%|██████▍   | 1987/3108 [47:40<26:07,  1.40s/it] 64%|██████▍   | 1988/3108 [47:41<26:07,  1.40s/it] 64%|██████▍   | 1989/3108 [47:43<26:05,  1.40s/it] 64%|██████▍   | 1990/3108 [47:44<26:06,  1.40s/it] 64%|██████▍   | 1991/3108 [47:46<26:04,  1.40s/it] 64%|██████▍   | 1992/3108 [47:47<26:06,  1.40s/it] 64%|██████▍   | 1993/3108 [47:48<26:03,  1.40s/it] 64%|██████▍   | 1994/3108 [47:50<26:38,  1.43s/it] 64%|██████▍   | 1995/3108 [47:51<26:25,  1.42s/it] 64%|██████▍   | 1996/3108 [47:53<26:15,  1.42s/it] 64%|██████▍   | 1997/3108 [47:54<26:06,  1.41s/it] 64%|██████▍   | 1998/3108 [47:55<26:01,  1.41s/it] 64%|██████▍   | 1999/3108 [47:57<25:56,  1.40s/it] 64%|██████▍   | 2000/3108 [47:58<26:30,  1.44s/it]                                                   loss: 0.73367517, learning_rate: 7.504e-06, global_step: 2000, interval_runtime: 707.0896, interval_samples_per_second: 12.72823152609857, interval_steps_per_second: 0.7071239736721429, epoch: 2.574
 64%|██████▍   | 2000/3108 [47:58<26:30,  1.44s/it][32m[2023-11-10 09:55:21,951] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, token_to_orig_map, token_is_max_context, question_id, questions, tokens, id, start_labels. If end_labels, token_to_orig_map, token_is_max_context, question_id, questions, tokens, id, start_labels are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 09:55:22,393] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 09:55:22,393] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 09:55:22,393] [    INFO][0m -   Total prediction steps = 48[0m
[32m[2023-11-10 09:55:22,393] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 09:55:22,393] [    INFO][0m -   Total Batch size = 18[0m

  0%|          | 0/48 [00:00<?, ?it/s][A
  4%|▍         | 2/48 [00:00<00:18,  2.52it/s][A
  6%|▋         | 3/48 [00:01<00:25,  1.77it/s][A
  8%|▊         | 4/48 [00:02<00:28,  1.54it/s][A
 10%|█         | 5/48 [00:03<00:30,  1.42it/s][A
 12%|█▎        | 6/48 [00:03<00:30,  1.36it/s][A
 15%|█▍        | 7/48 [00:04<00:31,  1.32it/s][A
 17%|█▋        | 8/48 [00:05<00:30,  1.29it/s][A
 19%|█▉        | 9/48 [00:06<00:30,  1.28it/s][A
 21%|██        | 10/48 [00:07<00:30,  1.27it/s][A
 23%|██▎       | 11/48 [00:08<00:29,  1.26it/s][A
 25%|██▌       | 12/48 [00:08<00:28,  1.25it/s][A
 27%|██▋       | 13/48 [00:09<00:27,  1.25it/s][A
 29%|██▉       | 14/48 [00:10<00:27,  1.25it/s][A
 31%|███▏      | 15/48 [00:11<00:26,  1.26it/s][A
 33%|███▎      | 16/48 [00:12<00:25,  1.26it/s][A
 35%|███▌      | 17/48 [00:12<00:24,  1.25it/s][A
 38%|███▊      | 18/48 [00:13<00:23,  1.25it/s][A
 40%|███▉      | 19/48 [00:14<00:23,  1.25it/s][A
 42%|████▏     | 20/48 [00:15<00:22,  1.25it/s][A
 44%|████▍     | 21/48 [00:16<00:21,  1.25it/s][A
 46%|████▌     | 22/48 [00:16<00:20,  1.25it/s][A
 48%|████▊     | 23/48 [00:17<00:19,  1.26it/s][A
 50%|█████     | 24/48 [00:18<00:19,  1.25it/s][A
 52%|█████▏    | 25/48 [00:19<00:18,  1.24it/s][A
 54%|█████▍    | 26/48 [00:20<00:17,  1.24it/s][A
 56%|█████▋    | 27/48 [00:20<00:16,  1.25it/s][A
 58%|█████▊    | 28/48 [00:21<00:16,  1.20it/s][A
 60%|██████    | 29/48 [00:22<00:15,  1.21it/s][A
 62%|██████▎   | 30/48 [00:23<00:14,  1.22it/s][A
 65%|██████▍   | 31/48 [00:24<00:13,  1.23it/s][A
 67%|██████▋   | 32/48 [00:24<00:12,  1.23it/s][A
 69%|██████▉   | 33/48 [00:25<00:12,  1.24it/s][A
 71%|███████   | 34/48 [00:26<00:11,  1.25it/s][A
 73%|███████▎  | 35/48 [00:27<00:10,  1.25it/s][A
 75%|███████▌  | 36/48 [00:28<00:09,  1.24it/s][A
 77%|███████▋  | 37/48 [00:28<00:08,  1.24it/s][A
 79%|███████▉  | 38/48 [00:29<00:08,  1.24it/s][A
 81%|████████▏ | 39/48 [00:30<00:07,  1.25it/s][A
 83%|████████▎ | 40/48 [00:31<00:06,  1.24it/s][A
 85%|████████▌ | 41/48 [00:32<00:05,  1.25it/s][A
 88%|████████▊ | 42/48 [00:32<00:04,  1.25it/s][A
 90%|████████▉ | 43/48 [00:33<00:03,  1.44it/s][A
 92%|█████████▏| 44/48 [00:33<00:02,  1.75it/s][A
 94%|█████████▍| 45/48 [00:33<00:01,  2.03it/s][A
 96%|█████████▌| 46/48 [00:34<00:00,  2.30it/s][A
 98%|█████████▊| 47/48 [00:34<00:00,  2.54it/s][A
100%|██████████| 48/48 [00:34<00:00,  3.26it/s][A                                                   
                                               [Aeval_anls: 60.26329338104196, epoch: 2.574
 64%|██████▍   | 2000/3108 [48:37<26:30,  1.44s/it]
100%|██████████| 48/48 [00:35<00:00,  3.26it/s][A
                                               [A[32m[2023-11-10 09:56:00,880] [    INFO][0m - Saving model checkpoint to ./models/fidelity/checkpoint-2000[0m
[32m[2023-11-10 09:56:00,889] [    INFO][0m - Configuration saved in ./models/fidelity/checkpoint-2000/config.json[0m
[32m[2023-11-10 09:56:03,442] [    INFO][0m - Model weights saved in ./models/fidelity/checkpoint-2000/model_state.pdparams[0m
[32m[2023-11-10 09:56:03,442] [    INFO][0m - tokenizer config file saved in ./models/fidelity/checkpoint-2000/tokenizer_config.json[0m
[32m[2023-11-10 09:56:03,443] [    INFO][0m - Special tokens file saved in ./models/fidelity/checkpoint-2000/special_tokens_map.json[0m
 64%|██████▍   | 2001/3108 [48:46<4:44:28, 15.42s/it] 64%|██████▍   | 2002/3108 [48:48<3:26:39, 11.21s/it] 64%|██████▍   | 2003/3108 [48:49<2:32:13,  8.27s/it] 64%|██████▍   | 2004/3108 [48:51<1:54:11,  6.21s/it] 65%|██████▍   | 2005/3108 [48:52<1:27:36,  4.77s/it] 65%|██████▍   | 2006/3108 [48:53<1:08:59,  3.76s/it] 65%|██████▍   | 2007/3108 [48:55<56:00,  3.05s/it]   65%|██████▍   | 2008/3108 [48:56<46:56,  2.56s/it] 65%|██████▍   | 2009/3108 [48:58<40:55,  2.23s/it] 65%|██████▍   | 2010/3108 [48:59<36:18,  1.98s/it] 65%|██████▍   | 2011/3108 [49:00<33:07,  1.81s/it] 65%|██████▍   | 2012/3108 [49:02<30:48,  1.69s/it] 65%|██████▍   | 2013/3108 [49:03<29:14,  1.60s/it] 65%|██████▍   | 2014/3108 [49:05<28:45,  1.58s/it] 65%|██████▍   | 2015/3108 [49:06<27:46,  1.52s/it] 65%|██████▍   | 2016/3108 [49:08<27:36,  1.52s/it] 65%|██████▍   | 2017/3108 [49:09<26:55,  1.48s/it] 65%|██████▍   | 2018/3108 [49:10<26:28,  1.46s/it] 65%|██████▍   | 2019/3108 [49:12<26:10,  1.44s/it] 65%|██████▍   | 2020/3108 [49:13<25:57,  1.43s/it] 65%|██████▌   | 2021/3108 [49:15<25:47,  1.42s/it] 65%|██████▌   | 2022/3108 [49:16<25:35,  1.41s/it] 65%|██████▌   | 2023/3108 [49:18<25:29,  1.41s/it] 65%|██████▌   | 2024/3108 [49:19<25:27,  1.41s/it] 65%|██████▌   | 2025/3108 [49:20<25:21,  1.41s/it] 65%|██████▌   | 2026/3108 [49:22<25:19,  1.40s/it] 65%|██████▌   | 2027/3108 [49:23<25:15,  1.40s/it] 65%|██████▌   | 2028/3108 [49:25<25:12,  1.40s/it] 65%|██████▌   | 2029/3108 [49:26<25:13,  1.40s/it] 65%|██████▌   | 2030/3108 [49:27<25:10,  1.40s/it] 65%|██████▌   | 2031/3108 [49:29<25:10,  1.40s/it] 65%|██████▌   | 2032/3108 [49:30<25:11,  1.41s/it] 65%|██████▌   | 2033/3108 [49:32<25:10,  1.41s/it] 65%|██████▌   | 2034/3108 [49:33<25:31,  1.43s/it] 65%|██████▌   | 2035/3108 [49:34<25:21,  1.42s/it] 66%|██████▌   | 2036/3108 [49:36<25:17,  1.42s/it] 66%|██████▌   | 2037/3108 [49:37<25:08,  1.41s/it] 66%|██████▌   | 2038/3108 [49:39<25:06,  1.41s/it] 66%|██████▌   | 2039/3108 [49:40<25:04,  1.41s/it] 66%|██████▌   | 2040/3108 [49:42<25:33,  1.44s/it] 66%|██████▌   | 2041/3108 [49:43<25:18,  1.42s/it] 66%|██████▌   | 2042/3108 [49:44<25:12,  1.42s/it] 66%|██████▌   | 2043/3108 [49:46<25:02,  1.41s/it] 66%|██████▌   | 2044/3108 [49:47<25:01,  1.41s/it] 66%|██████▌   | 2045/3108 [49:49<24:57,  1.41s/it] 66%|██████▌   | 2046/3108 [49:50<24:48,  1.40s/it] 66%|██████▌   | 2047/3108 [49:51<25:20,  1.43s/it] 66%|██████▌   | 2048/3108 [49:53<25:07,  1.42s/it] 66%|██████▌   | 2049/3108 [49:54<25:01,  1.42s/it] 66%|██████▌   | 2050/3108 [49:56<24:59,  1.42s/it] 66%|██████▌   | 2051/3108 [49:57<24:54,  1.41s/it] 66%|██████▌   | 2052/3108 [49:58<24:47,  1.41s/it] 66%|██████▌   | 2053/3108 [50:00<24:45,  1.41s/it] 66%|██████▌   | 2054/3108 [50:01<24:41,  1.41s/it] 66%|██████▌   | 2055/3108 [50:03<24:37,  1.40s/it] 66%|██████▌   | 2056/3108 [50:04<24:37,  1.40s/it] 66%|██████▌   | 2057/3108 [50:05<24:40,  1.41s/it] 66%|██████▌   | 2058/3108 [50:07<24:39,  1.41s/it] 66%|██████▌   | 2059/3108 [50:08<25:02,  1.43s/it] 66%|██████▋   | 2060/3108 [50:10<24:51,  1.42s/it] 66%|██████▋   | 2061/3108 [50:11<24:44,  1.42s/it] 66%|██████▋   | 2062/3108 [50:13<24:38,  1.41s/it] 66%|██████▋   | 2063/3108 [50:14<24:36,  1.41s/it] 66%|██████▋   | 2064/3108 [50:15<24:29,  1.41s/it] 66%|██████▋   | 2065/3108 [50:17<24:23,  1.40s/it] 66%|██████▋   | 2066/3108 [50:18<24:55,  1.43s/it] 67%|██████▋   | 2067/3108 [50:20<24:41,  1.42s/it] 67%|██████▋   | 2068/3108 [50:21<24:35,  1.42s/it] 67%|██████▋   | 2069/3108 [50:23<24:30,  1.41s/it] 67%|██████▋   | 2070/3108 [50:24<24:22,  1.41s/it] 67%|██████▋   | 2071/3108 [50:25<24:18,  1.41s/it] 67%|██████▋   | 2072/3108 [50:27<24:17,  1.41s/it] 67%|██████▋   | 2073/3108 [50:28<24:17,  1.41s/it] 67%|██████▋   | 2074/3108 [50:30<24:15,  1.41s/it] 67%|██████▋   | 2075/3108 [50:31<24:10,  1.40s/it] 67%|██████▋   | 2076/3108 [50:32<24:06,  1.40s/it] 67%|██████▋   | 2077/3108 [50:34<24:04,  1.40s/it] 67%|██████▋   | 2078/3108 [50:35<24:02,  1.40s/it] 67%|██████▋   | 2079/3108 [50:37<24:00,  1.40s/it] 67%|██████▋   | 2080/3108 [50:38<24:30,  1.43s/it] 67%|██████▋   | 2081/3108 [50:39<24:20,  1.42s/it] 67%|██████▋   | 2082/3108 [50:41<24:14,  1.42s/it] 67%|██████▋   | 2083/3108 [50:42<24:09,  1.41s/it] 67%|██████▋   | 2084/3108 [50:44<24:03,  1.41s/it] 67%|██████▋   | 2085/3108 [50:45<24:22,  1.43s/it] 67%|██████▋   | 2086/3108 [50:47<24:12,  1.42s/it] 67%|██████▋   | 2087/3108 [50:48<24:04,  1.41s/it] 67%|██████▋   | 2088/3108 [50:49<24:00,  1.41s/it] 67%|██████▋   | 2089/3108 [50:51<23:55,  1.41s/it] 67%|██████▋   | 2090/3108 [50:52<23:53,  1.41s/it] 67%|██████▋   | 2091/3108 [50:54<24:21,  1.44s/it] 67%|██████▋   | 2092/3108 [50:55<24:09,  1.43s/it] 67%|██████▋   | 2093/3108 [50:56<24:03,  1.42s/it] 67%|██████▋   | 2094/3108 [50:58<23:56,  1.42s/it] 67%|██████▋   | 2095/3108 [50:59<23:49,  1.41s/it] 67%|██████▋   | 2096/3108 [51:01<23:45,  1.41s/it] 67%|██████▋   | 2097/3108 [51:02<23:42,  1.41s/it] 68%|██████▊   | 2098/3108 [51:03<23:42,  1.41s/it] 68%|██████▊   | 2099/3108 [51:05<23:41,  1.41s/it] 68%|██████▊   | 2100/3108 [51:06<23:36,  1.41s/it] 68%|██████▊   | 2101/3108 [51:08<23:36,  1.41s/it] 68%|██████▊   | 2102/3108 [51:09<23:32,  1.40s/it] 68%|██████▊   | 2103/3108 [51:10<23:33,  1.41s/it] 68%|██████▊   | 2104/3108 [51:12<23:32,  1.41s/it] 68%|██████▊   | 2105/3108 [51:13<23:32,  1.41s/it] 68%|██████▊   | 2106/3108 [51:15<23:27,  1.41s/it] 68%|██████▊   | 2107/3108 [51:16<23:28,  1.41s/it] 68%|██████▊   | 2108/3108 [51:18<23:23,  1.40s/it] 68%|██████▊   | 2109/3108 [51:19<23:24,  1.41s/it] 68%|██████▊   | 2110/3108 [51:20<23:46,  1.43s/it] 68%|██████▊   | 2111/3108 [51:22<23:59,  1.44s/it] 68%|██████▊   | 2112/3108 [51:23<23:44,  1.43s/it] 68%|██████▊   | 2113/3108 [51:25<23:31,  1.42s/it] 68%|██████▊   | 2114/3108 [51:26<23:25,  1.41s/it] 68%|██████▊   | 2115/3108 [51:27<23:18,  1.41s/it] 68%|██████▊   | 2116/3108 [51:29<23:50,  1.44s/it] 68%|██████▊   | 2117/3108 [51:30<23:38,  1.43s/it] 68%|██████▊   | 2118/3108 [51:32<23:26,  1.42s/it] 68%|██████▊   | 2119/3108 [51:33<23:21,  1.42s/it] 68%|██████▊   | 2120/3108 [51:35<23:13,  1.41s/it] 68%|██████▊   | 2121/3108 [51:36<23:11,  1.41s/it] 68%|██████▊   | 2122/3108 [51:37<23:04,  1.40s/it] 68%|██████▊   | 2123/3108 [51:39<23:06,  1.41s/it] 68%|██████▊   | 2124/3108 [51:40<23:01,  1.40s/it] 68%|██████▊   | 2125/3108 [51:42<23:00,  1.40s/it] 68%|██████▊   | 2126/3108 [51:43<22:56,  1.40s/it] 68%|██████▊   | 2127/3108 [51:44<22:57,  1.40s/it] 68%|██████▊   | 2128/3108 [51:46<22:57,  1.41s/it] 69%|██████▊   | 2129/3108 [51:47<22:59,  1.41s/it] 69%|██████▊   | 2130/3108 [51:49<22:56,  1.41s/it] 69%|██████▊   | 2131/3108 [51:50<22:52,  1.41s/it] 69%|██████▊   | 2132/3108 [51:51<22:48,  1.40s/it] 69%|██████▊   | 2133/3108 [51:53<22:49,  1.40s/it] 69%|██████▊   | 2134/3108 [51:54<22:45,  1.40s/it] 69%|██████▊   | 2135/3108 [51:56<22:45,  1.40s/it] 69%|██████▊   | 2136/3108 [51:57<23:04,  1.42s/it] 69%|██████▉   | 2137/3108 [51:59<22:59,  1.42s/it] 69%|██████▉   | 2138/3108 [52:00<22:53,  1.42s/it] 69%|██████▉   | 2139/3108 [52:01<22:47,  1.41s/it] 69%|██████▉   | 2140/3108 [52:03<23:12,  1.44s/it] 69%|██████▉   | 2141/3108 [52:04<23:06,  1.43s/it] 69%|██████▉   | 2142/3108 [52:06<23:26,  1.46s/it] 69%|██████▉   | 2143/3108 [52:07<23:11,  1.44s/it] 69%|██████▉   | 2144/3108 [52:09<22:59,  1.43s/it] 69%|██████▉   | 2145/3108 [52:10<22:51,  1.42s/it] 69%|██████▉   | 2146/3108 [52:11<22:45,  1.42s/it] 69%|██████▉   | 2147/3108 [52:13<22:43,  1.42s/it] 69%|██████▉   | 2148/3108 [52:14<22:39,  1.42s/it] 69%|██████▉   | 2149/3108 [52:16<22:31,  1.41s/it] 69%|██████▉   | 2150/3108 [52:17<22:29,  1.41s/it] 69%|██████▉   | 2151/3108 [52:18<22:26,  1.41s/it] 69%|██████▉   | 2152/3108 [52:20<22:22,  1.40s/it] 69%|██████▉   | 2153/3108 [52:21<22:19,  1.40s/it] 69%|██████▉   | 2154/3108 [52:23<22:17,  1.40s/it] 69%|██████▉   | 2155/3108 [52:24<22:18,  1.40s/it] 69%|██████▉   | 2156/3108 [52:25<22:12,  1.40s/it] 69%|██████▉   | 2157/3108 [52:27<22:09,  1.40s/it] 69%|██████▉   | 2158/3108 [52:28<22:08,  1.40s/it] 69%|██████▉   | 2159/3108 [52:30<22:11,  1.40s/it] 69%|██████▉   | 2160/3108 [52:31<22:07,  1.40s/it] 70%|██████▉   | 2161/3108 [52:33<22:29,  1.43s/it] 70%|██████▉   | 2162/3108 [52:34<22:16,  1.41s/it] 70%|██████▉   | 2163/3108 [52:35<22:11,  1.41s/it] 70%|██████▉   | 2164/3108 [52:37<22:05,  1.40s/it] 70%|██████▉   | 2165/3108 [52:38<22:04,  1.40s/it] 70%|██████▉   | 2166/3108 [52:40<22:03,  1.40s/it] 70%|██████▉   | 2167/3108 [52:41<22:02,  1.41s/it] 70%|██████▉   | 2168/3108 [52:42<22:33,  1.44s/it] 70%|██████▉   | 2169/3108 [52:44<22:22,  1.43s/it] 70%|██████▉   | 2170/3108 [52:45<22:40,  1.45s/it] 70%|██████▉   | 2171/3108 [52:47<22:24,  1.43s/it] 70%|██████▉   | 2172/3108 [52:48<22:12,  1.42s/it] 70%|██████▉   | 2173/3108 [52:50<22:04,  1.42s/it] 70%|██████▉   | 2174/3108 [52:51<22:00,  1.41s/it] 70%|██████▉   | 2175/3108 [52:52<21:54,  1.41s/it] 70%|███████   | 2176/3108 [52:54<21:52,  1.41s/it] 70%|███████   | 2177/3108 [52:55<21:51,  1.41s/it] 70%|███████   | 2178/3108 [52:57<21:47,  1.41s/it] 70%|███████   | 2179/3108 [52:58<21:46,  1.41s/it] 70%|███████   | 2180/3108 [52:59<21:46,  1.41s/it] 70%|███████   | 2181/3108 [53:01<21:44,  1.41s/it] 70%|███████   | 2182/3108 [53:02<21:44,  1.41s/it] 70%|███████   | 2183/3108 [53:04<21:40,  1.41s/it] 70%|███████   | 2184/3108 [53:05<21:39,  1.41s/it] 70%|███████   | 2185/3108 [53:06<21:34,  1.40s/it] 70%|███████   | 2186/3108 [53:08<21:29,  1.40s/it] 70%|███████   | 2187/3108 [53:09<21:48,  1.42s/it] 70%|███████   | 2188/3108 [53:11<21:44,  1.42s/it] 70%|███████   | 2189/3108 [53:12<21:41,  1.42s/it] 70%|███████   | 2190/3108 [53:13<21:34,  1.41s/it] 70%|███████   | 2191/3108 [53:15<21:31,  1.41s/it] 71%|███████   | 2192/3108 [53:16<21:28,  1.41s/it] 71%|███████   | 2193/3108 [53:18<21:55,  1.44s/it] 71%|███████   | 2194/3108 [53:19<21:46,  1.43s/it] 71%|███████   | 2195/3108 [53:21<21:40,  1.42s/it] 71%|███████   | 2196/3108 [53:22<21:34,  1.42s/it] 71%|███████   | 2197/3108 [53:23<21:27,  1.41s/it] 71%|███████   | 2198/3108 [53:25<21:24,  1.41s/it] 71%|███████   | 2199/3108 [53:26<21:20,  1.41s/it] 71%|███████   | 2200/3108 [53:28<21:42,  1.43s/it] 71%|███████   | 2201/3108 [53:29<21:33,  1.43s/it] 71%|███████   | 2202/3108 [53:31<21:27,  1.42s/it] 71%|███████   | 2203/3108 [53:32<21:20,  1.42s/it] 71%|███████   | 2204/3108 [53:33<21:17,  1.41s/it] 71%|███████   | 2205/3108 [53:35<21:11,  1.41s/it] 71%|███████   | 2206/3108 [53:36<21:10,  1.41s/it] 71%|███████   | 2207/3108 [53:38<21:10,  1.41s/it] 71%|███████   | 2208/3108 [53:39<21:09,  1.41s/it] 71%|███████   | 2209/3108 [53:40<21:01,  1.40s/it] 71%|███████   | 2210/3108 [53:42<20:56,  1.40s/it] 71%|███████   | 2211/3108 [53:43<20:55,  1.40s/it] 71%|███████   | 2212/3108 [53:45<20:57,  1.40s/it] 71%|███████   | 2213/3108 [53:46<21:12,  1.42s/it] 71%|███████   | 2214/3108 [53:47<21:05,  1.42s/it] 71%|███████▏  | 2215/3108 [53:49<21:00,  1.41s/it] 71%|███████▏  | 2216/3108 [53:50<20:55,  1.41s/it] 71%|███████▏  | 2217/3108 [53:52<20:52,  1.41s/it] 71%|███████▏  | 2218/3108 [53:53<21:22,  1.44s/it] 71%|███████▏  | 2219/3108 [53:55<21:12,  1.43s/it] 71%|███████▏  | 2220/3108 [53:56<21:01,  1.42s/it] 71%|███████▏  | 2221/3108 [53:57<20:53,  1.41s/it] 71%|███████▏  | 2222/3108 [53:59<20:49,  1.41s/it] 72%|███████▏  | 2223/3108 [54:00<20:45,  1.41s/it] 72%|███████▏  | 2224/3108 [54:02<20:43,  1.41s/it] 72%|███████▏  | 2225/3108 [54:03<20:44,  1.41s/it] 72%|███████▏  | 2226/3108 [54:04<20:38,  1.40s/it] 72%|███████▏  | 2227/3108 [54:06<20:32,  1.40s/it] 72%|███████▏  | 2228/3108 [54:07<20:31,  1.40s/it] 72%|███████▏  | 2229/3108 [54:09<20:30,  1.40s/it] 72%|███████▏  | 2230/3108 [54:10<20:53,  1.43s/it] 72%|███████▏  | 2231/3108 [54:11<20:44,  1.42s/it] 72%|███████▏  | 2232/3108 [54:13<20:37,  1.41s/it] 72%|███████▏  | 2233/3108 [54:14<20:33,  1.41s/it] 72%|███████▏  | 2234/3108 [54:16<20:28,  1.41s/it] 72%|███████▏  | 2235/3108 [54:17<20:28,  1.41s/it] 72%|███████▏  | 2236/3108 [54:18<20:24,  1.40s/it] 72%|███████▏  | 2237/3108 [54:20<20:21,  1.40s/it] 72%|███████▏  | 2238/3108 [54:21<20:40,  1.43s/it] 72%|███████▏  | 2239/3108 [54:23<20:31,  1.42s/it] 72%|███████▏  | 2240/3108 [54:24<20:28,  1.42s/it] 72%|███████▏  | 2241/3108 [54:26<20:26,  1.41s/it] 72%|███████▏  | 2242/3108 [54:27<20:23,  1.41s/it] 72%|███████▏  | 2243/3108 [54:28<20:22,  1.41s/it] 72%|███████▏  | 2244/3108 [54:30<20:46,  1.44s/it] 72%|███████▏  | 2245/3108 [54:31<20:32,  1.43s/it] 72%|███████▏  | 2246/3108 [54:33<20:21,  1.42s/it] 72%|███████▏  | 2247/3108 [54:34<20:15,  1.41s/it] 72%|███████▏  | 2248/3108 [54:35<20:11,  1.41s/it] 72%|███████▏  | 2249/3108 [54:37<20:08,  1.41s/it] 72%|███████▏  | 2250/3108 [54:38<20:04,  1.40s/it] 72%|███████▏  | 2251/3108 [54:40<20:02,  1.40s/it] 72%|███████▏  | 2252/3108 [54:41<20:00,  1.40s/it] 72%|███████▏  | 2253/3108 [54:42<19:57,  1.40s/it] 73%|███████▎  | 2254/3108 [54:44<19:58,  1.40s/it] 73%|███████▎  | 2255/3108 [54:45<19:58,  1.40s/it] 73%|███████▎  | 2256/3108 [54:47<19:55,  1.40s/it] 73%|███████▎  | 2257/3108 [54:48<19:54,  1.40s/it] 73%|███████▎  | 2258/3108 [54:50<19:51,  1.40s/it] 73%|███████▎  | 2259/3108 [54:51<19:51,  1.40s/it] 73%|███████▎  | 2260/3108 [54:52<20:14,  1.43s/it] 73%|███████▎  | 2261/3108 [54:54<20:07,  1.43s/it] 73%|███████▎  | 2262/3108 [54:55<19:59,  1.42s/it] 73%|███████▎  | 2263/3108 [54:57<20:14,  1.44s/it] 73%|███████▎  | 2264/3108 [54:58<20:02,  1.42s/it] 73%|███████▎  | 2265/3108 [55:00<19:56,  1.42s/it] 73%|███████▎  | 2266/3108 [55:01<19:49,  1.41s/it] 73%|███████▎  | 2267/3108 [55:02<19:48,  1.41s/it] 73%|███████▎  | 2268/3108 [55:04<19:45,  1.41s/it] 73%|███████▎  | 2269/3108 [55:05<19:38,  1.41s/it] 73%|███████▎  | 2270/3108 [55:07<20:06,  1.44s/it] 73%|███████▎  | 2271/3108 [55:08<19:55,  1.43s/it] 73%|███████▎  | 2272/3108 [55:09<19:47,  1.42s/it] 73%|███████▎  | 2273/3108 [55:11<19:44,  1.42s/it] 73%|███████▎  | 2274/3108 [55:12<19:40,  1.42s/it] 73%|███████▎  | 2275/3108 [55:14<19:36,  1.41s/it] 73%|███████▎  | 2276/3108 [55:15<19:31,  1.41s/it] 73%|███████▎  | 2277/3108 [55:16<19:30,  1.41s/it] 73%|███████▎  | 2278/3108 [55:18<19:28,  1.41s/it] 73%|███████▎  | 2279/3108 [55:19<19:22,  1.40s/it] 73%|███████▎  | 2280/3108 [55:21<19:21,  1.40s/it] 73%|███████▎  | 2281/3108 [55:22<19:18,  1.40s/it] 73%|███████▎  | 2282/3108 [55:23<19:17,  1.40s/it] 73%|███████▎  | 2283/3108 [55:25<19:20,  1.41s/it] 73%|███████▎  | 2284/3108 [55:26<19:17,  1.40s/it] 74%|███████▎  | 2285/3108 [55:28<19:15,  1.40s/it] 74%|███████▎  | 2286/3108 [55:29<19:15,  1.41s/it] 74%|███████▎  | 2287/3108 [55:31<19:10,  1.40s/it] 74%|███████▎  | 2288/3108 [55:32<19:10,  1.40s/it] 74%|███████▎  | 2289/3108 [55:33<19:25,  1.42s/it] 74%|███████▎  | 2290/3108 [55:35<19:44,  1.45s/it] 74%|███████▎  | 2291/3108 [55:36<19:32,  1.44s/it] 74%|███████▎  | 2292/3108 [55:38<19:22,  1.42s/it] 74%|███████▍  | 2293/3108 [55:39<19:16,  1.42s/it] 74%|███████▍  | 2294/3108 [55:40<19:10,  1.41s/it] 74%|███████▍  | 2295/3108 [55:42<19:33,  1.44s/it] 74%|███████▍  | 2296/3108 [55:43<19:19,  1.43s/it] 74%|███████▍  | 2297/3108 [55:45<19:14,  1.42s/it] 74%|███████▍  | 2298/3108 [55:46<19:05,  1.41s/it] 74%|███████▍  | 2299/3108 [55:48<19:01,  1.41s/it] 74%|███████▍  | 2300/3108 [55:49<18:58,  1.41s/it] 74%|███████▍  | 2301/3108 [55:50<18:56,  1.41s/it] 74%|███████▍  | 2302/3108 [55:52<18:52,  1.40s/it] 74%|███████▍  | 2303/3108 [55:53<18:54,  1.41s/it] 74%|███████▍  | 2304/3108 [55:55<18:52,  1.41s/it] 74%|███████▍  | 2305/3108 [55:56<18:47,  1.40s/it] 74%|███████▍  | 2306/3108 [55:57<18:47,  1.41s/it] 74%|███████▍  | 2307/3108 [55:59<18:44,  1.40s/it] 74%|███████▍  | 2308/3108 [56:00<18:43,  1.40s/it] 74%|███████▍  | 2309/3108 [56:02<18:43,  1.41s/it] 74%|███████▍  | 2310/3108 [56:03<18:41,  1.41s/it] 74%|███████▍  | 2311/3108 [56:04<18:39,  1.40s/it] 74%|███████▍  | 2312/3108 [56:06<18:34,  1.40s/it] 74%|███████▍  | 2313/3108 [56:07<18:33,  1.40s/it] 74%|███████▍  | 2314/3108 [56:09<18:32,  1.40s/it] 74%|███████▍  | 2315/3108 [56:10<18:47,  1.42s/it] 75%|███████▍  | 2316/3108 [56:12<18:41,  1.42s/it] 75%|███████▍  | 2317/3108 [56:13<18:36,  1.41s/it] 75%|███████▍  | 2318/3108 [56:14<18:32,  1.41s/it] 75%|███████▍  | 2319/3108 [56:16<18:31,  1.41s/it] 75%|███████▍  | 2320/3108 [56:17<18:57,  1.44s/it] 75%|███████▍  | 2321/3108 [56:19<18:45,  1.43s/it] 75%|███████▍  | 2322/3108 [56:20<18:39,  1.42s/it] 75%|███████▍  | 2323/3108 [56:21<18:32,  1.42s/it] 75%|███████▍  | 2324/3108 [56:23<18:26,  1.41s/it] 75%|███████▍  | 2325/3108 [56:24<18:22,  1.41s/it] 75%|███████▍  | 2326/3108 [56:26<17:49,  1.37s/it] 75%|███████▍  | 2327/3108 [56:27<16:34,  1.27s/it] 75%|███████▍  | 2328/3108 [56:28<15:44,  1.21s/it] 75%|███████▍  | 2329/3108 [56:29<15:06,  1.16s/it] 75%|███████▍  | 2330/3108 [56:30<14:39,  1.13s/it] 75%|███████▌  | 2331/3108 [56:31<13:26,  1.04s/it] 75%|███████▌  | 2332/3108 [56:34<22:35,  1.75s/it] 75%|███████▌  | 2333/3108 [56:35<21:10,  1.64s/it] 75%|███████▌  | 2334/3108 [56:37<20:13,  1.57s/it] 75%|███████▌  | 2335/3108 [56:38<19:35,  1.52s/it] 75%|███████▌  | 2336/3108 [56:40<19:08,  1.49s/it] 75%|███████▌  | 2337/3108 [56:41<18:48,  1.46s/it] 75%|███████▌  | 2338/3108 [56:42<18:31,  1.44s/it] 75%|███████▌  | 2339/3108 [56:44<18:23,  1.44s/it] 75%|███████▌  | 2340/3108 [56:45<18:34,  1.45s/it] 75%|███████▌  | 2341/3108 [56:47<18:20,  1.43s/it] 75%|███████▌  | 2342/3108 [56:48<18:09,  1.42s/it] 75%|███████▌  | 2343/3108 [56:50<18:04,  1.42s/it] 75%|███████▌  | 2344/3108 [56:51<17:58,  1.41s/it] 75%|███████▌  | 2345/3108 [56:52<17:56,  1.41s/it] 75%|███████▌  | 2346/3108 [56:54<18:18,  1.44s/it] 76%|███████▌  | 2347/3108 [56:55<18:12,  1.44s/it] 76%|███████▌  | 2348/3108 [56:57<18:06,  1.43s/it] 76%|███████▌  | 2349/3108 [56:58<18:21,  1.45s/it] 76%|███████▌  | 2350/3108 [57:00<18:09,  1.44s/it] 76%|███████▌  | 2351/3108 [57:01<18:01,  1.43s/it] 76%|███████▌  | 2352/3108 [57:02<17:52,  1.42s/it] 76%|███████▌  | 2353/3108 [57:04<17:50,  1.42s/it] 76%|███████▌  | 2354/3108 [57:05<17:46,  1.41s/it] 76%|███████▌  | 2355/3108 [57:07<17:39,  1.41s/it] 76%|███████▌  | 2356/3108 [57:08<17:35,  1.40s/it] 76%|███████▌  | 2357/3108 [57:09<17:34,  1.40s/it] 76%|███████▌  | 2358/3108 [57:11<17:32,  1.40s/it] 76%|███████▌  | 2359/3108 [57:12<17:29,  1.40s/it] 76%|███████▌  | 2360/3108 [57:14<17:28,  1.40s/it] 76%|███████▌  | 2361/3108 [57:15<17:26,  1.40s/it] 76%|███████▌  | 2362/3108 [57:16<17:26,  1.40s/it] 76%|███████▌  | 2363/3108 [57:18<17:28,  1.41s/it] 76%|███████▌  | 2364/3108 [57:19<17:24,  1.40s/it] 76%|███████▌  | 2365/3108 [57:21<17:38,  1.43s/it] 76%|███████▌  | 2366/3108 [57:22<17:33,  1.42s/it] 76%|███████▌  | 2367/3108 [57:24<17:29,  1.42s/it] 76%|███████▌  | 2368/3108 [57:25<17:26,  1.41s/it] 76%|███████▌  | 2369/3108 [57:26<17:24,  1.41s/it] 76%|███████▋  | 2370/3108 [57:28<17:19,  1.41s/it] 76%|███████▋  | 2371/3108 [57:29<17:18,  1.41s/it] 76%|███████▋  | 2372/3108 [57:31<17:36,  1.44s/it] 76%|███████▋  | 2373/3108 [57:32<17:32,  1.43s/it] 76%|███████▋  | 2374/3108 [57:33<17:27,  1.43s/it] 76%|███████▋  | 2375/3108 [57:35<17:21,  1.42s/it] 76%|███████▋  | 2376/3108 [57:36<17:15,  1.41s/it] 76%|███████▋  | 2377/3108 [57:38<17:35,  1.44s/it] 77%|███████▋  | 2378/3108 [57:39<17:26,  1.43s/it] 77%|███████▋  | 2379/3108 [57:41<17:19,  1.43s/it] 77%|███████▋  | 2380/3108 [57:42<17:13,  1.42s/it] 77%|███████▋  | 2381/3108 [57:43<17:06,  1.41s/it] 77%|███████▋  | 2382/3108 [57:45<17:02,  1.41s/it] 77%|███████▋  | 2383/3108 [57:46<17:01,  1.41s/it] 77%|███████▋  | 2384/3108 [57:48<16:56,  1.40s/it] 77%|███████▋  | 2385/3108 [57:49<16:53,  1.40s/it] 77%|███████▋  | 2386/3108 [57:50<16:53,  1.40s/it] 77%|███████▋  | 2387/3108 [57:52<16:54,  1.41s/it] 77%|███████▋  | 2388/3108 [57:53<16:53,  1.41s/it] 77%|███████▋  | 2389/3108 [57:55<16:50,  1.41s/it] 77%|███████▋  | 2390/3108 [57:56<16:45,  1.40s/it] 77%|███████▋  | 2391/3108 [57:58<17:00,  1.42s/it] 77%|███████▋  | 2392/3108 [57:59<16:53,  1.42s/it] 77%|███████▋  | 2393/3108 [58:00<16:49,  1.41s/it] 77%|███████▋  | 2394/3108 [58:02<16:45,  1.41s/it] 77%|███████▋  | 2395/3108 [58:03<16:41,  1.40s/it] 77%|███████▋  | 2396/3108 [58:05<16:41,  1.41s/it] 77%|███████▋  | 2397/3108 [58:06<17:01,  1.44s/it] 77%|███████▋  | 2398/3108 [58:07<16:51,  1.42s/it] 77%|███████▋  | 2399/3108 [58:09<16:46,  1.42s/it] 77%|███████▋  | 2400/3108 [58:10<16:41,  1.41s/it] 77%|███████▋  | 2401/3108 [58:12<16:38,  1.41s/it] 77%|███████▋  | 2402/3108 [58:13<16:33,  1.41s/it] 77%|███████▋  | 2403/3108 [58:14<16:31,  1.41s/it] 77%|███████▋  | 2404/3108 [58:16<16:28,  1.40s/it] 77%|███████▋  | 2405/3108 [58:17<16:27,  1.41s/it] 77%|███████▋  | 2406/3108 [58:19<16:24,  1.40s/it] 77%|███████▋  | 2407/3108 [58:20<16:21,  1.40s/it] 77%|███████▋  | 2408/3108 [58:21<16:22,  1.40s/it] 78%|███████▊  | 2409/3108 [58:23<16:22,  1.41s/it] 78%|███████▊  | 2410/3108 [58:24<16:39,  1.43s/it] 78%|███████▊  | 2411/3108 [58:26<16:34,  1.43s/it] 78%|███████▊  | 2412/3108 [58:27<16:29,  1.42s/it] 78%|███████▊  | 2413/3108 [58:29<16:22,  1.41s/it] 78%|███████▊  | 2414/3108 [58:30<16:19,  1.41s/it] 78%|███████▊  | 2415/3108 [58:31<16:13,  1.41s/it] 78%|███████▊  | 2416/3108 [58:33<16:11,  1.40s/it] 78%|███████▊  | 2417/3108 [58:34<16:28,  1.43s/it] 78%|███████▊  | 2418/3108 [58:36<16:22,  1.42s/it] 78%|███████▊  | 2419/3108 [58:37<16:14,  1.41s/it] 78%|███████▊  | 2420/3108 [58:38<16:10,  1.41s/it] 78%|███████▊  | 2421/3108 [58:40<16:07,  1.41s/it] 78%|███████▊  | 2422/3108 [58:41<16:28,  1.44s/it] 78%|███████▊  | 2423/3108 [58:43<16:19,  1.43s/it] 78%|███████▊  | 2424/3108 [58:44<16:10,  1.42s/it] 78%|███████▊  | 2425/3108 [58:46<16:03,  1.41s/it] 78%|███████▊  | 2426/3108 [58:47<15:59,  1.41s/it] 78%|███████▊  | 2427/3108 [58:48<15:59,  1.41s/it] 78%|███████▊  | 2428/3108 [58:50<15:57,  1.41s/it] 78%|███████▊  | 2429/3108 [58:51<15:53,  1.40s/it] 78%|███████▊  | 2430/3108 [58:53<15:53,  1.41s/it] 78%|███████▊  | 2431/3108 [58:54<15:51,  1.41s/it] 78%|███████▊  | 2432/3108 [58:55<15:48,  1.40s/it] 78%|███████▊  | 2433/3108 [58:57<15:45,  1.40s/it] 78%|███████▊  | 2434/3108 [58:58<15:45,  1.40s/it] 78%|███████▊  | 2435/3108 [59:00<15:41,  1.40s/it] 78%|███████▊  | 2436/3108 [59:01<15:39,  1.40s/it] 78%|███████▊  | 2437/3108 [59:02<15:37,  1.40s/it] 78%|███████▊  | 2438/3108 [59:04<15:38,  1.40s/it] 78%|███████▊  | 2439/3108 [59:05<15:35,  1.40s/it] 79%|███████▊  | 2440/3108 [59:07<15:55,  1.43s/it] 79%|███████▊  | 2441/3108 [59:08<15:48,  1.42s/it] 79%|███████▊  | 2442/3108 [59:10<16:00,  1.44s/it] 79%|███████▊  | 2443/3108 [59:11<15:49,  1.43s/it] 79%|███████▊  | 2444/3108 [59:12<15:42,  1.42s/it] 79%|███████▊  | 2445/3108 [59:14<15:38,  1.42s/it] 79%|███████▊  | 2446/3108 [59:15<15:33,  1.41s/it] 79%|███████▊  | 2447/3108 [59:17<15:32,  1.41s/it] 79%|███████▉  | 2448/3108 [59:18<15:50,  1.44s/it] 79%|███████▉  | 2449/3108 [59:20<15:43,  1.43s/it] 79%|███████▉  | 2450/3108 [59:21<15:36,  1.42s/it] 79%|███████▉  | 2451/3108 [59:22<15:29,  1.41s/it] 79%|███████▉  | 2452/3108 [59:24<15:25,  1.41s/it] 79%|███████▉  | 2453/3108 [59:25<15:24,  1.41s/it] 79%|███████▉  | 2454/3108 [59:27<15:22,  1.41s/it] 79%|███████▉  | 2455/3108 [59:28<15:17,  1.41s/it] 79%|███████▉  | 2456/3108 [59:29<15:13,  1.40s/it] 79%|███████▉  | 2457/3108 [59:31<15:11,  1.40s/it] 79%|███████▉  | 2458/3108 [59:32<15:12,  1.40s/it] 79%|███████▉  | 2459/3108 [59:34<15:12,  1.41s/it] 79%|███████▉  | 2460/3108 [59:35<15:08,  1.40s/it] 79%|███████▉  | 2461/3108 [59:36<15:04,  1.40s/it] 79%|███████▉  | 2462/3108 [59:38<15:05,  1.40s/it] 79%|███████▉  | 2463/3108 [59:39<15:04,  1.40s/it] 79%|███████▉  | 2464/3108 [59:41<15:01,  1.40s/it] 79%|███████▉  | 2465/3108 [59:42<15:00,  1.40s/it] 79%|███████▉  | 2466/3108 [59:43<15:00,  1.40s/it] 79%|███████▉  | 2467/3108 [59:45<15:13,  1.42s/it] 79%|███████▉  | 2468/3108 [59:46<15:07,  1.42s/it] 79%|███████▉  | 2469/3108 [59:48<15:01,  1.41s/it] 79%|███████▉  | 2470/3108 [59:49<15:16,  1.44s/it] 80%|███████▉  | 2471/3108 [59:51<15:09,  1.43s/it] 80%|███████▉  | 2472/3108 [59:52<15:01,  1.42s/it] 80%|███████▉  | 2473/3108 [59:53<14:58,  1.42s/it] 80%|███████▉  | 2474/3108 [59:55<15:16,  1.45s/it] 80%|███████▉  | 2475/3108 [59:56<15:08,  1.44s/it] 80%|███████▉  | 2476/3108 [59:58<15:01,  1.43s/it] 80%|███████▉  | 2477/3108 [59:59<14:57,  1.42s/it] 80%|███████▉  | 2478/3108 [1:00:00<14:50,  1.41s/it] 80%|███████▉  | 2479/3108 [1:00:02<14:45,  1.41s/it] 80%|███████▉  | 2480/3108 [1:00:03<14:41,  1.40s/it] 80%|███████▉  | 2481/3108 [1:00:05<14:37,  1.40s/it] 80%|███████▉  | 2482/3108 [1:00:06<14:35,  1.40s/it] 80%|███████▉  | 2483/3108 [1:00:07<14:36,  1.40s/it] 80%|███████▉  | 2484/3108 [1:00:09<14:33,  1.40s/it] 80%|███████▉  | 2485/3108 [1:00:10<14:33,  1.40s/it] 80%|███████▉  | 2486/3108 [1:00:12<14:30,  1.40s/it] 80%|████████  | 2487/3108 [1:00:13<14:29,  1.40s/it] 80%|████████  | 2488/3108 [1:00:14<14:29,  1.40s/it] 80%|████████  | 2489/3108 [1:00:16<14:30,  1.41s/it] 80%|████████  | 2490/3108 [1:00:17<14:27,  1.40s/it] 80%|████████  | 2491/3108 [1:00:19<14:26,  1.40s/it] 80%|████████  | 2492/3108 [1:00:20<14:23,  1.40s/it] 80%|████████  | 2493/3108 [1:00:22<14:35,  1.42s/it] 80%|████████  | 2494/3108 [1:00:23<14:31,  1.42s/it] 80%|████████  | 2495/3108 [1:00:24<14:29,  1.42s/it] 80%|████████  | 2496/3108 [1:00:26<14:26,  1.42s/it] 80%|████████  | 2497/3108 [1:00:27<14:24,  1.41s/it] 80%|████████  | 2498/3108 [1:00:29<14:22,  1.41s/it] 80%|████████  | 2499/3108 [1:00:30<14:39,  1.44s/it] 80%|████████  | 2500/3108 [1:00:32<14:48,  1.46s/it]                                                     loss: 0.64385095, learning_rate: 4.118e-06, global_step: 2500, interval_runtime: 753.2783, interval_samples_per_second: 11.94777559713813, interval_steps_per_second: 0.6637653109521183, epoch: 3.2175
 80%|████████  | 2500/3108 [1:00:32<14:48,  1.46s/it] 80%|████████  | 2501/3108 [1:00:33<14:46,  1.46s/it] 81%|████████  | 2502/3108 [1:00:34<14:34,  1.44s/it] 81%|████████  | 2503/3108 [1:00:36<14:25,  1.43s/it] 81%|████████  | 2504/3108 [1:00:37<14:20,  1.42s/it] 81%|████████  | 2505/3108 [1:00:39<14:16,  1.42s/it] 81%|████████  | 2506/3108 [1:00:40<14:12,  1.42s/it] 81%|████████  | 2507/3108 [1:00:42<14:08,  1.41s/it] 81%|████████  | 2508/3108 [1:00:43<14:06,  1.41s/it] 81%|████████  | 2509/3108 [1:00:44<14:04,  1.41s/it] 81%|████████  | 2510/3108 [1:00:46<14:00,  1.41s/it] 81%|████████  | 2511/3108 [1:00:47<13:58,  1.41s/it] 81%|████████  | 2512/3108 [1:00:49<13:56,  1.40s/it] 81%|████████  | 2513/3108 [1:00:50<13:56,  1.41s/it] 81%|████████  | 2514/3108 [1:00:51<13:53,  1.40s/it] 81%|████████  | 2515/3108 [1:00:53<13:52,  1.40s/it] 81%|████████  | 2516/3108 [1:00:54<13:52,  1.41s/it] 81%|████████  | 2517/3108 [1:00:56<13:49,  1.40s/it] 81%|████████  | 2518/3108 [1:00:57<13:47,  1.40s/it] 81%|████████  | 2519/3108 [1:00:58<13:58,  1.42s/it] 81%|████████  | 2520/3108 [1:01:00<13:53,  1.42s/it] 81%|████████  | 2521/3108 [1:01:01<13:49,  1.41s/it] 81%|████████  | 2522/3108 [1:01:03<13:45,  1.41s/it] 81%|████████  | 2523/3108 [1:01:04<13:43,  1.41s/it] 81%|████████  | 2524/3108 [1:01:06<14:00,  1.44s/it] 81%|████████  | 2525/3108 [1:01:07<13:54,  1.43s/it] 81%|████████▏ | 2526/3108 [1:01:08<13:46,  1.42s/it] 81%|████████▏ | 2527/3108 [1:01:10<13:40,  1.41s/it] 81%|████████▏ | 2528/3108 [1:01:11<13:38,  1.41s/it] 81%|████████▏ | 2529/3108 [1:01:13<13:37,  1.41s/it] 81%|████████▏ | 2530/3108 [1:01:14<13:48,  1.43s/it] 81%|████████▏ | 2531/3108 [1:01:15<13:41,  1.42s/it] 81%|████████▏ | 2532/3108 [1:01:17<13:35,  1.42s/it] 81%|████████▏ | 2533/3108 [1:01:18<13:34,  1.42s/it] 82%|████████▏ | 2534/3108 [1:01:20<13:30,  1.41s/it] 82%|████████▏ | 2535/3108 [1:01:21<13:27,  1.41s/it] 82%|████████▏ | 2536/3108 [1:01:22<13:24,  1.41s/it] 82%|████████▏ | 2537/3108 [1:01:24<13:23,  1.41s/it] 82%|████████▏ | 2538/3108 [1:01:25<13:21,  1.41s/it] 82%|████████▏ | 2539/3108 [1:01:27<13:18,  1.40s/it] 82%|████████▏ | 2540/3108 [1:01:28<13:16,  1.40s/it] 82%|████████▏ | 2541/3108 [1:01:29<13:14,  1.40s/it] 82%|████████▏ | 2542/3108 [1:01:31<13:12,  1.40s/it] 82%|████████▏ | 2543/3108 [1:01:32<13:13,  1.41s/it] 82%|████████▏ | 2544/3108 [1:01:34<13:26,  1.43s/it] 82%|████████▏ | 2545/3108 [1:01:35<13:21,  1.42s/it] 82%|████████▏ | 2546/3108 [1:01:37<13:16,  1.42s/it] 82%|████████▏ | 2547/3108 [1:01:38<13:12,  1.41s/it] 82%|████████▏ | 2548/3108 [1:01:39<13:08,  1.41s/it] 82%|████████▏ | 2549/3108 [1:01:41<13:06,  1.41s/it] 82%|████████▏ | 2550/3108 [1:01:42<13:20,  1.43s/it] 82%|████████▏ | 2551/3108 [1:01:44<13:14,  1.43s/it] 82%|████████▏ | 2552/3108 [1:01:45<13:08,  1.42s/it] 82%|████████▏ | 2553/3108 [1:01:46<13:03,  1.41s/it] 82%|████████▏ | 2554/3108 [1:01:48<13:01,  1.41s/it] 82%|████████▏ | 2555/3108 [1:01:49<13:00,  1.41s/it] 82%|████████▏ | 2556/3108 [1:01:51<12:58,  1.41s/it] 82%|████████▏ | 2557/3108 [1:01:52<12:55,  1.41s/it] 82%|████████▏ | 2558/3108 [1:01:54<12:52,  1.41s/it] 82%|████████▏ | 2559/3108 [1:01:55<12:51,  1.40s/it] 82%|████████▏ | 2560/3108 [1:01:56<13:02,  1.43s/it] 82%|████████▏ | 2561/3108 [1:01:58<12:58,  1.42s/it] 82%|████████▏ | 2562/3108 [1:01:59<12:53,  1.42s/it] 82%|████████▏ | 2563/3108 [1:02:01<12:50,  1.41s/it] 82%|████████▏ | 2564/3108 [1:02:02<12:48,  1.41s/it] 83%|████████▎ | 2565/3108 [1:02:03<12:43,  1.41s/it] 83%|████████▎ | 2566/3108 [1:02:05<12:44,  1.41s/it] 83%|████████▎ | 2567/3108 [1:02:06<12:41,  1.41s/it] 83%|████████▎ | 2568/3108 [1:02:08<12:39,  1.41s/it] 83%|████████▎ | 2569/3108 [1:02:09<12:50,  1.43s/it] 83%|████████▎ | 2570/3108 [1:02:11<12:44,  1.42s/it] 83%|████████▎ | 2571/3108 [1:02:12<12:40,  1.42s/it] 83%|████████▎ | 2572/3108 [1:02:13<12:37,  1.41s/it] 83%|████████▎ | 2573/3108 [1:02:15<12:33,  1.41s/it] 83%|████████▎ | 2574/3108 [1:02:16<12:30,  1.41s/it] 83%|████████▎ | 2575/3108 [1:02:18<12:29,  1.41s/it] 83%|████████▎ | 2576/3108 [1:02:19<12:45,  1.44s/it] 83%|████████▎ | 2577/3108 [1:02:20<12:40,  1.43s/it] 83%|████████▎ | 2578/3108 [1:02:22<12:34,  1.42s/it] 83%|████████▎ | 2579/3108 [1:02:23<12:29,  1.42s/it] 83%|████████▎ | 2580/3108 [1:02:25<12:27,  1.42s/it] 83%|████████▎ | 2581/3108 [1:02:26<12:24,  1.41s/it] 83%|████████▎ | 2582/3108 [1:02:28<12:20,  1.41s/it] 83%|████████▎ | 2583/3108 [1:02:29<12:19,  1.41s/it] 83%|████████▎ | 2584/3108 [1:02:30<12:15,  1.40s/it] 83%|████████▎ | 2585/3108 [1:02:32<12:14,  1.40s/it] 83%|████████▎ | 2586/3108 [1:02:33<12:12,  1.40s/it] 83%|████████▎ | 2587/3108 [1:02:35<12:11,  1.40s/it] 83%|████████▎ | 2588/3108 [1:02:36<12:09,  1.40s/it] 83%|████████▎ | 2589/3108 [1:02:37<12:10,  1.41s/it] 83%|████████▎ | 2590/3108 [1:02:39<12:22,  1.43s/it] 83%|████████▎ | 2591/3108 [1:02:40<12:16,  1.42s/it] 83%|████████▎ | 2592/3108 [1:02:42<12:10,  1.42s/it] 83%|████████▎ | 2593/3108 [1:02:43<12:06,  1.41s/it] 83%|████████▎ | 2594/3108 [1:02:44<12:03,  1.41s/it] 83%|████████▎ | 2595/3108 [1:02:46<12:13,  1.43s/it] 84%|████████▎ | 2596/3108 [1:02:47<12:09,  1.42s/it] 84%|████████▎ | 2597/3108 [1:02:49<12:04,  1.42s/it] 84%|████████▎ | 2598/3108 [1:02:50<12:00,  1.41s/it] 84%|████████▎ | 2599/3108 [1:02:52<11:58,  1.41s/it] 84%|████████▎ | 2600/3108 [1:02:53<11:54,  1.41s/it] 84%|████████▎ | 2601/3108 [1:02:54<12:08,  1.44s/it] 84%|████████▎ | 2602/3108 [1:02:56<11:59,  1.42s/it] 84%|████████▍ | 2603/3108 [1:02:57<11:56,  1.42s/it] 84%|████████▍ | 2604/3108 [1:02:59<11:52,  1.41s/it] 84%|████████▍ | 2605/3108 [1:03:00<11:50,  1.41s/it] 84%|████████▍ | 2606/3108 [1:03:01<11:48,  1.41s/it] 84%|████████▍ | 2607/3108 [1:03:03<11:45,  1.41s/it] 84%|████████▍ | 2608/3108 [1:03:04<11:44,  1.41s/it] 84%|████████▍ | 2609/3108 [1:03:06<11:42,  1.41s/it] 84%|████████▍ | 2610/3108 [1:03:07<11:40,  1.41s/it] 84%|████████▍ | 2611/3108 [1:03:08<11:37,  1.40s/it] 84%|████████▍ | 2612/3108 [1:03:10<11:35,  1.40s/it] 84%|████████▍ | 2613/3108 [1:03:11<11:35,  1.40s/it] 84%|████████▍ | 2614/3108 [1:03:13<11:34,  1.41s/it] 84%|████████▍ | 2615/3108 [1:03:14<11:33,  1.41s/it] 84%|████████▍ | 2616/3108 [1:03:16<11:31,  1.41s/it] 84%|████████▍ | 2617/3108 [1:03:17<11:29,  1.40s/it] 84%|████████▍ | 2618/3108 [1:03:18<11:26,  1.40s/it] 84%|████████▍ | 2619/3108 [1:03:20<11:27,  1.41s/it] 84%|████████▍ | 2620/3108 [1:03:21<11:37,  1.43s/it] 84%|████████▍ | 2621/3108 [1:03:23<11:43,  1.44s/it] 84%|████████▍ | 2622/3108 [1:03:24<11:36,  1.43s/it] 84%|████████▍ | 2623/3108 [1:03:25<11:30,  1.42s/it] 84%|████████▍ | 2624/3108 [1:03:27<11:26,  1.42s/it] 84%|████████▍ | 2625/3108 [1:03:28<11:24,  1.42s/it] 84%|████████▍ | 2626/3108 [1:03:30<11:35,  1.44s/it] 85%|████████▍ | 2627/3108 [1:03:31<11:27,  1.43s/it] 85%|████████▍ | 2628/3108 [1:03:33<11:20,  1.42s/it] 85%|████████▍ | 2629/3108 [1:03:34<11:15,  1.41s/it] 85%|████████▍ | 2630/3108 [1:03:35<11:11,  1.40s/it] 85%|████████▍ | 2631/3108 [1:03:37<11:09,  1.40s/it] 85%|████████▍ | 2632/3108 [1:03:38<11:06,  1.40s/it] 85%|████████▍ | 2633/3108 [1:03:40<11:07,  1.40s/it] 85%|████████▍ | 2634/3108 [1:03:41<11:04,  1.40s/it] 85%|████████▍ | 2635/3108 [1:03:42<11:03,  1.40s/it] 85%|████████▍ | 2636/3108 [1:03:44<11:01,  1.40s/it] 85%|████████▍ | 2637/3108 [1:03:45<11:01,  1.40s/it] 85%|████████▍ | 2638/3108 [1:03:47<11:01,  1.41s/it] 85%|████████▍ | 2639/3108 [1:03:48<10:59,  1.41s/it] 85%|████████▍ | 2640/3108 [1:03:49<10:56,  1.40s/it] 85%|████████▍ | 2641/3108 [1:03:51<10:54,  1.40s/it] 85%|████████▌ | 2642/3108 [1:03:52<10:53,  1.40s/it] 85%|████████▌ | 2643/3108 [1:03:54<10:51,  1.40s/it] 85%|████████▌ | 2644/3108 [1:03:55<10:51,  1.40s/it] 85%|████████▌ | 2645/3108 [1:03:56<10:51,  1.41s/it] 85%|████████▌ | 2646/3108 [1:03:58<11:00,  1.43s/it] 85%|████████▌ | 2647/3108 [1:03:59<10:55,  1.42s/it] 85%|████████▌ | 2648/3108 [1:04:01<10:52,  1.42s/it] 85%|████████▌ | 2649/3108 [1:04:02<10:49,  1.42s/it] 85%|████████▌ | 2650/3108 [1:04:04<10:58,  1.44s/it] 85%|████████▌ | 2651/3108 [1:04:05<10:53,  1.43s/it] 85%|████████▌ | 2652/3108 [1:04:07<11:04,  1.46s/it] 85%|████████▌ | 2653/3108 [1:04:08<10:55,  1.44s/it] 85%|████████▌ | 2654/3108 [1:04:09<10:48,  1.43s/it] 85%|████████▌ | 2655/3108 [1:04:11<10:43,  1.42s/it] 85%|████████▌ | 2656/3108 [1:04:12<10:39,  1.42s/it] 85%|████████▌ | 2657/3108 [1:04:14<10:37,  1.41s/it] 86%|████████▌ | 2658/3108 [1:04:15<10:33,  1.41s/it] 86%|████████▌ | 2659/3108 [1:04:16<10:31,  1.41s/it] 86%|████████▌ | 2660/3108 [1:04:18<10:29,  1.40s/it] 86%|████████▌ | 2661/3108 [1:04:19<10:28,  1.40s/it] 86%|████████▌ | 2662/3108 [1:04:21<10:27,  1.41s/it] 86%|████████▌ | 2663/3108 [1:04:22<10:26,  1.41s/it] 86%|████████▌ | 2664/3108 [1:04:23<10:23,  1.40s/it] 86%|████████▌ | 2665/3108 [1:04:25<10:20,  1.40s/it] 86%|████████▌ | 2666/3108 [1:04:26<10:20,  1.40s/it] 86%|████████▌ | 2667/3108 [1:04:28<10:17,  1.40s/it] 86%|████████▌ | 2668/3108 [1:04:29<10:18,  1.41s/it] 86%|████████▌ | 2669/3108 [1:04:30<10:16,  1.40s/it] 86%|████████▌ | 2670/3108 [1:04:32<10:14,  1.40s/it] 86%|████████▌ | 2671/3108 [1:04:33<10:21,  1.42s/it] 86%|████████▌ | 2672/3108 [1:04:35<10:16,  1.41s/it] 86%|████████▌ | 2673/3108 [1:04:36<10:12,  1.41s/it] 86%|████████▌ | 2674/3108 [1:04:37<10:09,  1.40s/it] 86%|████████▌ | 2675/3108 [1:04:39<10:08,  1.41s/it] 86%|████████▌ | 2676/3108 [1:04:40<10:05,  1.40s/it] 86%|████████▌ | 2677/3108 [1:04:42<10:03,  1.40s/it] 86%|████████▌ | 2678/3108 [1:04:43<10:16,  1.43s/it] 86%|████████▌ | 2679/3108 [1:04:45<10:10,  1.42s/it] 86%|████████▌ | 2680/3108 [1:04:46<10:19,  1.45s/it] 86%|████████▋ | 2681/3108 [1:04:47<10:12,  1.43s/it] 86%|████████▋ | 2682/3108 [1:04:49<10:06,  1.42s/it] 86%|████████▋ | 2683/3108 [1:04:50<10:01,  1.42s/it] 86%|████████▋ | 2684/3108 [1:04:52<09:59,  1.41s/it] 86%|████████▋ | 2685/3108 [1:04:53<09:56,  1.41s/it] 86%|████████▋ | 2686/3108 [1:04:55<09:54,  1.41s/it] 86%|████████▋ | 2687/3108 [1:04:56<09:51,  1.40s/it] 86%|████████▋ | 2688/3108 [1:04:57<09:49,  1.40s/it] 87%|████████▋ | 2689/3108 [1:04:59<09:47,  1.40s/it] 87%|████████▋ | 2690/3108 [1:05:00<09:46,  1.40s/it] 87%|████████▋ | 2691/3108 [1:05:02<09:45,  1.40s/it] 87%|████████▋ | 2692/3108 [1:05:03<09:44,  1.41s/it] 87%|████████▋ | 2693/3108 [1:05:04<09:43,  1.41s/it] 87%|████████▋ | 2694/3108 [1:05:06<09:40,  1.40s/it] 87%|████████▋ | 2695/3108 [1:05:07<09:39,  1.40s/it] 87%|████████▋ | 2696/3108 [1:05:09<09:37,  1.40s/it] 87%|████████▋ | 2697/3108 [1:05:10<09:44,  1.42s/it] 87%|████████▋ | 2698/3108 [1:05:11<09:40,  1.42s/it] 87%|████████▋ | 2699/3108 [1:05:13<09:38,  1.41s/it] 87%|████████▋ | 2700/3108 [1:05:14<09:34,  1.41s/it] 87%|████████▋ | 2701/3108 [1:05:16<09:31,  1.40s/it] 87%|████████▋ | 2702/3108 [1:05:17<09:29,  1.40s/it] 87%|████████▋ | 2703/3108 [1:05:19<09:41,  1.43s/it] 87%|████████▋ | 2704/3108 [1:05:20<09:35,  1.43s/it] 87%|████████▋ | 2705/3108 [1:05:21<09:30,  1.42s/it] 87%|████████▋ | 2706/3108 [1:05:23<09:28,  1.41s/it] 87%|████████▋ | 2707/3108 [1:05:24<09:25,  1.41s/it] 87%|████████▋ | 2708/3108 [1:05:26<09:23,  1.41s/it] 87%|████████▋ | 2709/3108 [1:05:27<09:21,  1.41s/it] 87%|████████▋ | 2710/3108 [1:05:28<09:30,  1.43s/it] 87%|████████▋ | 2711/3108 [1:05:30<09:25,  1.43s/it] 87%|████████▋ | 2712/3108 [1:05:31<09:20,  1.42s/it] 87%|████████▋ | 2713/3108 [1:05:33<09:17,  1.41s/it] 87%|████████▋ | 2714/3108 [1:05:34<09:14,  1.41s/it] 87%|████████▋ | 2715/3108 [1:05:35<09:13,  1.41s/it] 87%|████████▋ | 2716/3108 [1:05:37<09:12,  1.41s/it] 87%|████████▋ | 2717/3108 [1:05:38<09:10,  1.41s/it] 87%|████████▋ | 2718/3108 [1:05:40<09:07,  1.40s/it] 87%|████████▋ | 2719/3108 [1:05:41<09:05,  1.40s/it] 88%|████████▊ | 2720/3108 [1:05:42<09:03,  1.40s/it] 88%|████████▊ | 2721/3108 [1:05:44<09:02,  1.40s/it] 88%|████████▊ | 2722/3108 [1:05:45<09:00,  1.40s/it] 88%|████████▊ | 2723/3108 [1:05:47<09:07,  1.42s/it] 88%|████████▊ | 2724/3108 [1:05:48<09:05,  1.42s/it] 88%|████████▊ | 2725/3108 [1:05:50<09:02,  1.42s/it] 88%|████████▊ | 2726/3108 [1:05:51<09:00,  1.42s/it] 88%|████████▊ | 2727/3108 [1:05:52<08:58,  1.41s/it] 88%|████████▊ | 2728/3108 [1:05:54<09:08,  1.44s/it] 88%|████████▊ | 2729/3108 [1:05:55<09:04,  1.44s/it] 88%|████████▊ | 2730/3108 [1:05:57<08:58,  1.42s/it] 88%|████████▊ | 2731/3108 [1:05:58<08:55,  1.42s/it] 88%|████████▊ | 2732/3108 [1:05:59<08:51,  1.41s/it] 88%|████████▊ | 2733/3108 [1:06:01<08:49,  1.41s/it] 88%|████████▊ | 2734/3108 [1:06:02<08:47,  1.41s/it] 88%|████████▊ | 2735/3108 [1:06:04<08:46,  1.41s/it] 88%|████████▊ | 2736/3108 [1:06:05<08:44,  1.41s/it] 88%|████████▊ | 2737/3108 [1:06:07<08:41,  1.41s/it] 88%|████████▊ | 2738/3108 [1:06:08<08:39,  1.40s/it] 88%|████████▊ | 2739/3108 [1:06:09<08:38,  1.40s/it] 88%|████████▊ | 2740/3108 [1:06:11<08:46,  1.43s/it] 88%|████████▊ | 2741/3108 [1:06:12<08:43,  1.43s/it] 88%|████████▊ | 2742/3108 [1:06:14<08:38,  1.42s/it] 88%|████████▊ | 2743/3108 [1:06:15<08:35,  1.41s/it] 88%|████████▊ | 2744/3108 [1:06:16<08:32,  1.41s/it] 88%|████████▊ | 2745/3108 [1:06:18<08:29,  1.40s/it] 88%|████████▊ | 2746/3108 [1:06:19<08:28,  1.41s/it] 88%|████████▊ | 2747/3108 [1:06:21<08:28,  1.41s/it] 88%|████████▊ | 2748/3108 [1:06:22<08:34,  1.43s/it] 88%|████████▊ | 2749/3108 [1:06:24<08:30,  1.42s/it] 88%|████████▊ | 2750/3108 [1:06:25<08:26,  1.42s/it] 89%|████████▊ | 2751/3108 [1:06:26<08:23,  1.41s/it] 89%|████████▊ | 2752/3108 [1:06:28<08:22,  1.41s/it] 89%|████████▊ | 2753/3108 [1:06:29<08:21,  1.41s/it] 89%|████████▊ | 2754/3108 [1:06:31<08:30,  1.44s/it] 89%|████████▊ | 2755/3108 [1:06:32<08:25,  1.43s/it] 89%|████████▊ | 2756/3108 [1:06:33<08:20,  1.42s/it] 89%|████████▊ | 2757/3108 [1:06:35<08:16,  1.42s/it] 89%|████████▊ | 2758/3108 [1:06:36<08:13,  1.41s/it] 89%|████████▉ | 2759/3108 [1:06:38<08:12,  1.41s/it] 89%|████████▉ | 2760/3108 [1:06:39<08:10,  1.41s/it] 89%|████████▉ | 2761/3108 [1:06:40<08:07,  1.41s/it] 89%|████████▉ | 2762/3108 [1:06:42<08:05,  1.40s/it] 89%|████████▉ | 2763/3108 [1:06:43<08:03,  1.40s/it] 89%|████████▉ | 2764/3108 [1:06:45<08:02,  1.40s/it] 89%|████████▉ | 2765/3108 [1:06:46<08:01,  1.40s/it] 89%|████████▉ | 2766/3108 [1:06:47<07:59,  1.40s/it] 89%|████████▉ | 2767/3108 [1:06:49<07:56,  1.40s/it] 89%|████████▉ | 2768/3108 [1:06:50<07:55,  1.40s/it] 89%|████████▉ | 2769/3108 [1:06:52<07:55,  1.40s/it] 89%|████████▉ | 2770/3108 [1:06:53<08:02,  1.43s/it] 89%|████████▉ | 2771/3108 [1:06:55<07:58,  1.42s/it] 89%|████████▉ | 2772/3108 [1:06:56<07:54,  1.41s/it] 89%|████████▉ | 2773/3108 [1:06:57<08:00,  1.43s/it] 89%|████████▉ | 2774/3108 [1:06:59<07:55,  1.42s/it] 89%|████████▉ | 2775/3108 [1:07:00<07:51,  1.42s/it] 89%|████████▉ | 2776/3108 [1:07:02<07:49,  1.41s/it] 89%|████████▉ | 2777/3108 [1:07:03<07:48,  1.41s/it] 89%|████████▉ | 2778/3108 [1:07:04<07:44,  1.41s/it] 89%|████████▉ | 2779/3108 [1:07:06<07:42,  1.41s/it] 89%|████████▉ | 2780/3108 [1:07:07<07:52,  1.44s/it] 89%|████████▉ | 2781/3108 [1:07:09<07:47,  1.43s/it] 90%|████████▉ | 2782/3108 [1:07:10<07:43,  1.42s/it] 90%|████████▉ | 2783/3108 [1:07:12<07:40,  1.42s/it] 90%|████████▉ | 2784/3108 [1:07:13<07:37,  1.41s/it] 90%|████████▉ | 2785/3108 [1:07:14<07:35,  1.41s/it] 90%|████████▉ | 2786/3108 [1:07:16<07:34,  1.41s/it] 90%|████████▉ | 2787/3108 [1:07:17<07:32,  1.41s/it] 90%|████████▉ | 2788/3108 [1:07:19<07:31,  1.41s/it] 90%|████████▉ | 2789/3108 [1:07:20<07:30,  1.41s/it] 90%|████████▉ | 2790/3108 [1:07:21<07:27,  1.41s/it] 90%|████████▉ | 2791/3108 [1:07:23<07:26,  1.41s/it] 90%|████████▉ | 2792/3108 [1:07:24<07:22,  1.40s/it] 90%|████████▉ | 2793/3108 [1:07:26<07:20,  1.40s/it] 90%|████████▉ | 2794/3108 [1:07:27<07:19,  1.40s/it] 90%|████████▉ | 2795/3108 [1:07:28<07:18,  1.40s/it] 90%|████████▉ | 2796/3108 [1:07:30<07:17,  1.40s/it] 90%|████████▉ | 2797/3108 [1:07:31<07:16,  1.40s/it] 90%|█████████ | 2798/3108 [1:07:33<07:15,  1.41s/it] 90%|█████████ | 2799/3108 [1:07:34<07:20,  1.43s/it] 90%|█████████ | 2800/3108 [1:07:36<07:26,  1.45s/it] 90%|█████████ | 2801/3108 [1:07:37<07:20,  1.44s/it] 90%|█████████ | 2802/3108 [1:07:38<07:16,  1.43s/it] 90%|█████████ | 2803/3108 [1:07:40<07:12,  1.42s/it] 90%|█████████ | 2804/3108 [1:07:41<07:09,  1.41s/it] 90%|█████████ | 2805/3108 [1:07:43<07:17,  1.45s/it] 90%|█████████ | 2806/3108 [1:07:44<07:13,  1.43s/it] 90%|█████████ | 2807/3108 [1:07:46<07:09,  1.43s/it] 90%|█████████ | 2808/3108 [1:07:47<07:05,  1.42s/it] 90%|█████████ | 2809/3108 [1:07:48<07:02,  1.41s/it] 90%|█████████ | 2810/3108 [1:07:50<07:00,  1.41s/it] 90%|█████████ | 2811/3108 [1:07:51<06:56,  1.40s/it] 90%|█████████ | 2812/3108 [1:07:53<06:54,  1.40s/it] 91%|█████████ | 2813/3108 [1:07:54<06:54,  1.40s/it] 91%|█████████ | 2814/3108 [1:07:55<06:52,  1.40s/it] 91%|█████████ | 2815/3108 [1:07:57<06:50,  1.40s/it] 91%|█████████ | 2816/3108 [1:07:58<06:49,  1.40s/it] 91%|█████████ | 2817/3108 [1:08:00<06:47,  1.40s/it] 91%|█████████ | 2818/3108 [1:08:01<06:45,  1.40s/it] 91%|█████████ | 2819/3108 [1:08:02<06:44,  1.40s/it] 91%|█████████ | 2820/3108 [1:08:04<06:42,  1.40s/it] 91%|█████████ | 2821/3108 [1:08:05<06:41,  1.40s/it] 91%|█████████ | 2822/3108 [1:08:07<06:40,  1.40s/it] 91%|█████████ | 2823/3108 [1:08:08<06:38,  1.40s/it] 91%|█████████ | 2824/3108 [1:08:09<06:37,  1.40s/it] 91%|█████████ | 2825/3108 [1:08:11<06:42,  1.42s/it] 91%|█████████ | 2826/3108 [1:08:12<06:39,  1.42s/it] 91%|█████████ | 2827/3108 [1:08:14<06:36,  1.41s/it] 91%|█████████ | 2828/3108 [1:08:15<06:34,  1.41s/it] 91%|█████████ | 2829/3108 [1:08:16<06:32,  1.41s/it] 91%|█████████ | 2830/3108 [1:08:18<06:40,  1.44s/it] 91%|█████████ | 2831/3108 [1:08:19<06:36,  1.43s/it] 91%|█████████ | 2832/3108 [1:08:21<06:33,  1.42s/it] 91%|█████████ | 2833/3108 [1:08:22<06:29,  1.42s/it] 91%|█████████ | 2834/3108 [1:08:24<06:27,  1.41s/it] 91%|█████████ | 2835/3108 [1:08:25<06:24,  1.41s/it] 91%|█████████ | 2836/3108 [1:08:26<06:23,  1.41s/it] 91%|█████████▏| 2837/3108 [1:08:28<06:21,  1.41s/it] 91%|█████████▏| 2838/3108 [1:08:29<06:19,  1.40s/it] 91%|█████████▏| 2839/3108 [1:08:31<06:16,  1.40s/it] 91%|█████████▏| 2840/3108 [1:08:32<06:14,  1.40s/it] 91%|█████████▏| 2841/3108 [1:08:33<06:14,  1.40s/it] 91%|█████████▏| 2842/3108 [1:08:35<06:13,  1.41s/it] 91%|█████████▏| 2843/3108 [1:08:36<06:12,  1.41s/it] 92%|█████████▏| 2844/3108 [1:08:38<06:11,  1.41s/it] 92%|█████████▏| 2845/3108 [1:08:39<06:09,  1.40s/it] 92%|█████████▏| 2846/3108 [1:08:40<06:07,  1.40s/it] 92%|█████████▏| 2847/3108 [1:08:42<06:06,  1.40s/it] 92%|█████████▏| 2848/3108 [1:08:43<06:05,  1.41s/it] 92%|█████████▏| 2849/3108 [1:08:45<06:04,  1.41s/it] 92%|█████████▏| 2850/3108 [1:08:46<06:09,  1.43s/it] 92%|█████████▏| 2851/3108 [1:08:48<06:05,  1.42s/it] 92%|█████████▏| 2852/3108 [1:08:49<06:02,  1.42s/it] 92%|█████████▏| 2853/3108 [1:08:50<05:59,  1.41s/it] 92%|█████████▏| 2854/3108 [1:08:52<05:57,  1.41s/it] 92%|█████████▏| 2855/3108 [1:08:53<05:55,  1.41s/it] 92%|█████████▏| 2856/3108 [1:08:55<06:02,  1.44s/it] 92%|█████████▏| 2857/3108 [1:08:56<05:59,  1.43s/it] 92%|█████████▏| 2858/3108 [1:08:57<05:56,  1.42s/it] 92%|█████████▏| 2859/3108 [1:08:59<05:52,  1.42s/it] 92%|█████████▏| 2860/3108 [1:09:00<05:58,  1.44s/it] 92%|█████████▏| 2861/3108 [1:09:02<05:54,  1.43s/it] 92%|█████████▏| 2862/3108 [1:09:03<05:49,  1.42s/it] 92%|█████████▏| 2863/3108 [1:09:05<05:46,  1.41s/it] 92%|█████████▏| 2864/3108 [1:09:06<05:43,  1.41s/it] 92%|█████████▏| 2865/3108 [1:09:07<05:41,  1.41s/it] 92%|█████████▏| 2866/3108 [1:09:09<05:40,  1.41s/it] 92%|█████████▏| 2867/3108 [1:09:10<05:39,  1.41s/it] 92%|█████████▏| 2868/3108 [1:09:12<05:37,  1.40s/it] 92%|█████████▏| 2869/3108 [1:09:13<05:35,  1.40s/it] 92%|█████████▏| 2870/3108 [1:09:14<05:34,  1.40s/it] 92%|█████████▏| 2871/3108 [1:09:16<05:33,  1.41s/it] 92%|█████████▏| 2872/3108 [1:09:17<05:31,  1.40s/it] 92%|█████████▏| 2873/3108 [1:09:19<05:30,  1.41s/it] 92%|█████████▏| 2874/3108 [1:09:20<05:28,  1.40s/it] 93%|█████████▎| 2875/3108 [1:09:22<05:32,  1.43s/it] 93%|█████████▎| 2876/3108 [1:09:23<05:29,  1.42s/it] 93%|█████████▎| 2877/3108 [1:09:24<05:27,  1.42s/it] 93%|█████████▎| 2878/3108 [1:09:26<05:25,  1.42s/it] 93%|█████████▎| 2879/3108 [1:09:27<05:22,  1.41s/it] 93%|█████████▎| 2880/3108 [1:09:29<05:20,  1.40s/it] 93%|█████████▎| 2881/3108 [1:09:30<05:18,  1.40s/it] 93%|█████████▎| 2882/3108 [1:09:31<05:24,  1.44s/it] 93%|█████████▎| 2883/3108 [1:09:33<05:22,  1.43s/it] 93%|█████████▎| 2884/3108 [1:09:34<05:18,  1.42s/it] 93%|█████████▎| 2885/3108 [1:09:36<05:16,  1.42s/it] 93%|█████████▎| 2886/3108 [1:09:37<05:13,  1.41s/it] 93%|█████████▎| 2887/3108 [1:09:38<05:11,  1.41s/it] 93%|█████████▎| 2888/3108 [1:09:40<05:09,  1.41s/it] 93%|█████████▎| 2889/3108 [1:09:41<05:07,  1.41s/it] 93%|█████████▎| 2890/3108 [1:09:43<05:12,  1.44s/it] 93%|█████████▎| 2891/3108 [1:09:44<05:09,  1.43s/it] 93%|█████████▎| 2892/3108 [1:09:46<05:06,  1.42s/it] 93%|█████████▎| 2893/3108 [1:09:47<05:03,  1.41s/it] 93%|█████████▎| 2894/3108 [1:09:48<05:01,  1.41s/it] 93%|█████████▎| 2895/3108 [1:09:50<04:59,  1.41s/it] 93%|█████████▎| 2896/3108 [1:09:51<04:58,  1.41s/it] 93%|█████████▎| 2897/3108 [1:09:53<04:56,  1.41s/it] 93%|█████████▎| 2898/3108 [1:09:54<04:55,  1.41s/it] 93%|█████████▎| 2899/3108 [1:09:55<04:53,  1.40s/it] 93%|█████████▎| 2900/3108 [1:09:57<04:52,  1.41s/it] 93%|█████████▎| 2901/3108 [1:09:58<04:54,  1.42s/it] 93%|█████████▎| 2902/3108 [1:10:00<04:51,  1.41s/it] 93%|█████████▎| 2903/3108 [1:10:01<04:49,  1.41s/it] 93%|█████████▎| 2904/3108 [1:10:02<04:47,  1.41s/it] 93%|█████████▎| 2905/3108 [1:10:04<04:45,  1.40s/it] 94%|█████████▎| 2906/3108 [1:10:05<04:43,  1.40s/it] 94%|█████████▎| 2907/3108 [1:10:07<04:49,  1.44s/it] 94%|█████████▎| 2908/3108 [1:10:08<04:45,  1.43s/it] 94%|█████████▎| 2909/3108 [1:10:10<04:43,  1.42s/it] 94%|█████████▎| 2910/3108 [1:10:11<04:39,  1.41s/it] 94%|█████████▎| 2911/3108 [1:10:12<04:37,  1.41s/it] 94%|█████████▎| 2912/3108 [1:10:14<04:35,  1.41s/it] 94%|█████████▎| 2913/3108 [1:10:15<04:33,  1.40s/it] 94%|█████████▍| 2914/3108 [1:10:17<04:32,  1.40s/it] 94%|█████████▍| 2915/3108 [1:10:18<04:31,  1.41s/it] 94%|█████████▍| 2916/3108 [1:10:19<04:29,  1.40s/it] 94%|█████████▍| 2917/3108 [1:10:21<04:27,  1.40s/it] 94%|█████████▍| 2918/3108 [1:10:22<04:26,  1.40s/it] 94%|█████████▍| 2919/3108 [1:10:24<04:24,  1.40s/it] 94%|█████████▍| 2920/3108 [1:10:25<04:28,  1.43s/it] 94%|█████████▍| 2921/3108 [1:10:27<04:26,  1.42s/it] 94%|█████████▍| 2922/3108 [1:10:28<04:24,  1.42s/it] 94%|█████████▍| 2923/3108 [1:10:29<04:21,  1.41s/it] 94%|█████████▍| 2924/3108 [1:10:31<04:19,  1.41s/it] 94%|█████████▍| 2925/3108 [1:10:32<04:18,  1.41s/it] 94%|█████████▍| 2926/3108 [1:10:34<04:15,  1.40s/it] 94%|█████████▍| 2927/3108 [1:10:35<04:17,  1.42s/it] 94%|█████████▍| 2928/3108 [1:10:36<04:15,  1.42s/it] 94%|█████████▍| 2929/3108 [1:10:38<04:13,  1.42s/it] 94%|█████████▍| 2930/3108 [1:10:39<04:11,  1.41s/it] 94%|█████████▍| 2931/3108 [1:10:41<04:10,  1.41s/it] 94%|█████████▍| 2932/3108 [1:10:42<04:13,  1.44s/it] 94%|█████████▍| 2933/3108 [1:10:44<04:11,  1.44s/it] 94%|█████████▍| 2934/3108 [1:10:45<04:07,  1.42s/it] 94%|█████████▍| 2935/3108 [1:10:46<04:05,  1.42s/it] 94%|█████████▍| 2936/3108 [1:10:48<04:02,  1.41s/it] 94%|█████████▍| 2937/3108 [1:10:49<04:00,  1.41s/it] 95%|█████████▍| 2938/3108 [1:10:51<03:58,  1.40s/it] 95%|█████████▍| 2939/3108 [1:10:52<03:57,  1.41s/it] 95%|█████████▍| 2940/3108 [1:10:53<03:55,  1.40s/it] 95%|█████████▍| 2941/3108 [1:10:55<03:53,  1.40s/it] 95%|█████████▍| 2942/3108 [1:10:56<03:52,  1.40s/it] 95%|█████████▍| 2943/3108 [1:10:58<03:51,  1.41s/it] 95%|█████████▍| 2944/3108 [1:10:59<03:49,  1.40s/it] 95%|█████████▍| 2945/3108 [1:11:00<03:48,  1.40s/it] 95%|█████████▍| 2946/3108 [1:11:02<03:47,  1.40s/it] 95%|█████████▍| 2947/3108 [1:11:03<03:45,  1.40s/it] 95%|█████████▍| 2948/3108 [1:11:05<03:43,  1.40s/it] 95%|█████████▍| 2949/3108 [1:11:06<03:42,  1.40s/it] 95%|█████████▍| 2950/3108 [1:11:07<03:46,  1.43s/it] 95%|█████████▍| 2951/3108 [1:11:09<03:42,  1.42s/it] 95%|█████████▍| 2952/3108 [1:11:10<03:43,  1.43s/it] 95%|█████████▌| 2953/3108 [1:11:12<03:40,  1.43s/it] 95%|█████████▌| 2954/3108 [1:11:13<03:37,  1.42s/it] 95%|█████████▌| 2955/3108 [1:11:15<03:35,  1.41s/it] 95%|█████████▌| 2956/3108 [1:11:16<03:33,  1.41s/it] 95%|█████████▌| 2957/3108 [1:11:17<03:31,  1.40s/it] 95%|█████████▌| 2958/3108 [1:11:19<03:35,  1.44s/it] 95%|█████████▌| 2959/3108 [1:11:20<03:33,  1.43s/it] 95%|█████████▌| 2960/3108 [1:11:22<03:30,  1.42s/it] 95%|█████████▌| 2961/3108 [1:11:23<03:28,  1.42s/it] 95%|█████████▌| 2962/3108 [1:11:24<03:26,  1.42s/it] 95%|█████████▌| 2963/3108 [1:11:26<03:24,  1.41s/it] 95%|█████████▌| 2964/3108 [1:11:27<03:22,  1.41s/it] 95%|█████████▌| 2965/3108 [1:11:29<03:21,  1.41s/it] 95%|█████████▌| 2966/3108 [1:11:30<03:19,  1.40s/it] 95%|█████████▌| 2967/3108 [1:11:31<03:17,  1.40s/it] 95%|█████████▌| 2968/3108 [1:11:33<03:16,  1.40s/it] 96%|█████████▌| 2969/3108 [1:11:34<03:14,  1.40s/it] 96%|█████████▌| 2970/3108 [1:11:36<03:12,  1.40s/it] 96%|█████████▌| 2971/3108 [1:11:37<03:11,  1.40s/it] 96%|█████████▌| 2972/3108 [1:11:38<03:10,  1.40s/it] 96%|█████████▌| 2973/3108 [1:11:40<03:09,  1.41s/it] 96%|█████████▌| 2974/3108 [1:11:41<03:08,  1.41s/it] 96%|█████████▌| 2975/3108 [1:11:43<03:06,  1.40s/it] 96%|█████████▌| 2976/3108 [1:11:44<03:05,  1.40s/it] 96%|█████████▌| 2977/3108 [1:11:46<03:07,  1.43s/it] 96%|█████████▌| 2978/3108 [1:11:47<03:04,  1.42s/it] 96%|█████████▌| 2979/3108 [1:11:48<03:02,  1.42s/it] 96%|█████████▌| 2980/3108 [1:11:50<03:04,  1.44s/it] 96%|█████████▌| 2981/3108 [1:11:51<03:01,  1.43s/it] 96%|█████████▌| 2982/3108 [1:11:53<02:58,  1.42s/it] 96%|█████████▌| 2983/3108 [1:11:54<02:56,  1.41s/it] 96%|█████████▌| 2984/3108 [1:11:56<02:58,  1.44s/it] 96%|█████████▌| 2985/3108 [1:11:57<02:56,  1.43s/it] 96%|█████████▌| 2986/3108 [1:11:58<02:53,  1.43s/it] 96%|█████████▌| 2987/3108 [1:12:00<02:52,  1.42s/it] 96%|█████████▌| 2988/3108 [1:12:01<02:50,  1.42s/it] 96%|█████████▌| 2989/3108 [1:12:03<02:48,  1.41s/it] 96%|█████████▌| 2990/3108 [1:12:04<02:46,  1.41s/it] 96%|█████████▌| 2991/3108 [1:12:05<02:44,  1.41s/it] 96%|█████████▋| 2992/3108 [1:12:07<02:43,  1.41s/it] 96%|█████████▋| 2993/3108 [1:12:08<02:41,  1.41s/it] 96%|█████████▋| 2994/3108 [1:12:10<02:39,  1.40s/it] 96%|█████████▋| 2995/3108 [1:12:11<02:38,  1.40s/it] 96%|█████████▋| 2996/3108 [1:12:12<02:36,  1.40s/it] 96%|█████████▋| 2997/3108 [1:12:14<02:35,  1.40s/it] 96%|█████████▋| 2998/3108 [1:12:15<02:34,  1.40s/it] 96%|█████████▋| 2999/3108 [1:12:17<02:33,  1.41s/it] 97%|█████████▋| 3000/3108 [1:12:18<02:31,  1.40s/it]                                                     loss: 0.55148566, learning_rate: 7.315e-07, global_step: 3000, interval_runtime: 706.4623, interval_samples_per_second: 12.739533751611914, interval_steps_per_second: 0.7077518750895507, epoch: 3.861
 97%|█████████▋| 3000/3108 [1:12:18<02:31,  1.40s/it][32m[2023-11-10 10:19:41,691] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, token_to_orig_map, token_is_max_context, question_id, questions, tokens, id, start_labels. If end_labels, token_to_orig_map, token_is_max_context, question_id, questions, tokens, id, start_labels are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:19:42,167] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:19:42,167] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:19:42,167] [    INFO][0m -   Total prediction steps = 48[0m
[32m[2023-11-10 10:19:42,167] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:19:42,168] [    INFO][0m -   Total Batch size = 18[0m

  0%|          | 0/48 [00:00<?, ?it/s][A
  4%|▍         | 2/48 [00:00<00:15,  2.93it/s][A
  6%|▋         | 3/48 [00:01<00:21,  2.07it/s][A
  8%|▊         | 4/48 [00:02<00:24,  1.79it/s][A
 10%|█         | 5/48 [00:02<00:25,  1.66it/s][A
 12%|█▎        | 6/48 [00:03<00:26,  1.59it/s][A
 15%|█▍        | 7/48 [00:04<00:27,  1.48it/s][A
 17%|█▋        | 8/48 [00:04<00:27,  1.48it/s][A
 19%|█▉        | 9/48 [00:05<00:26,  1.47it/s][A
 21%|██        | 10/48 [00:06<00:25,  1.47it/s][A
 23%|██▎       | 11/48 [00:07<00:26,  1.40it/s][A
 25%|██▌       | 12/48 [00:07<00:25,  1.41it/s][A
 27%|██▋       | 13/48 [00:08<00:25,  1.36it/s][A
 29%|██▉       | 14/48 [00:09<00:24,  1.39it/s][A
 31%|███▏      | 15/48 [00:09<00:23,  1.41it/s][A
 33%|███▎      | 16/48 [00:10<00:22,  1.42it/s][A
 35%|███▌      | 17/48 [00:11<00:21,  1.43it/s][A
 38%|███▊      | 18/48 [00:11<00:20,  1.44it/s][A
 40%|███▉      | 19/48 [00:12<00:20,  1.45it/s][A
 42%|████▏     | 20/48 [00:13<00:19,  1.45it/s][A
 44%|████▍     | 21/48 [00:14<00:18,  1.45it/s][A
 46%|████▌     | 22/48 [00:14<00:17,  1.45it/s][A
 48%|████▊     | 23/48 [00:15<00:17,  1.46it/s][A
 50%|█████     | 24/48 [00:16<00:16,  1.46it/s][A
 52%|█████▏    | 25/48 [00:16<00:15,  1.46it/s][A
 54%|█████▍    | 26/48 [00:17<00:15,  1.46it/s][A
 56%|█████▋    | 27/48 [00:18<00:14,  1.46it/s][A
 58%|█████▊    | 28/48 [00:18<00:13,  1.46it/s][A
 60%|██████    | 29/48 [00:19<00:13,  1.46it/s][A
 62%|██████▎   | 30/48 [00:20<00:12,  1.46it/s][A
 65%|██████▍   | 31/48 [00:20<00:11,  1.46it/s][A
 67%|██████▋   | 32/48 [00:21<00:10,  1.46it/s][A
 69%|██████▉   | 33/48 [00:22<00:10,  1.46it/s][A
 71%|███████   | 34/48 [00:22<00:09,  1.46it/s][A
 73%|███████▎  | 35/48 [00:23<00:08,  1.46it/s][A
 75%|███████▌  | 36/48 [00:24<00:08,  1.39it/s][A
 77%|███████▋  | 37/48 [00:25<00:07,  1.41it/s][A
 79%|███████▉  | 38/48 [00:25<00:07,  1.42it/s][A
 81%|████████▏ | 39/48 [00:26<00:06,  1.44it/s][A
 83%|████████▎ | 40/48 [00:27<00:05,  1.39it/s][A
 85%|████████▌ | 41/48 [00:27<00:04,  1.41it/s][A
 88%|████████▊ | 42/48 [00:28<00:04,  1.42it/s][A
 90%|████████▉ | 43/48 [00:29<00:03,  1.62it/s][A
 92%|█████████▏| 44/48 [00:29<00:02,  1.91it/s][A
 94%|█████████▍| 45/48 [00:29<00:01,  2.20it/s][A
 96%|█████████▌| 46/48 [00:29<00:00,  2.44it/s][A
 98%|█████████▊| 47/48 [00:30<00:00,  2.67it/s][A
100%|██████████| 48/48 [00:30<00:00,  3.41it/s][A                                                     
                                               [Aeval_anls: 59.84970733692079, epoch: 3.861
 97%|█████████▋| 3000/3108 [1:12:52<02:31,  1.40s/it]
100%|██████████| 48/48 [00:30<00:00,  3.41it/s][A
                                               [A[32m[2023-11-10 10:20:15,749] [    INFO][0m - Saving model checkpoint to ./models/fidelity/checkpoint-3000[0m
[32m[2023-11-10 10:20:15,757] [    INFO][0m - Configuration saved in ./models/fidelity/checkpoint-3000/config.json[0m
[32m[2023-11-10 10:20:18,334] [    INFO][0m - Model weights saved in ./models/fidelity/checkpoint-3000/model_state.pdparams[0m
[32m[2023-11-10 10:20:18,334] [    INFO][0m - tokenizer config file saved in ./models/fidelity/checkpoint-3000/tokenizer_config.json[0m
[32m[2023-11-10 10:20:18,334] [    INFO][0m - Special tokens file saved in ./models/fidelity/checkpoint-3000/special_tokens_map.json[0m
[32m[2023-11-10 10:20:23,441] [    INFO][0m - Deleting older checkpoint [models/fidelity/checkpoint-2000] due to args.save_total_limit[0m
 97%|█████████▋| 3001/3108 [1:13:02<25:05, 14.07s/it] 97%|█████████▋| 3002/3108 [1:13:03<18:09, 10.27s/it] 97%|█████████▋| 3003/3108 [1:13:05<13:19,  7.61s/it] 97%|█████████▋| 3004/3108 [1:13:06<09:57,  5.75s/it] 97%|█████████▋| 3005/3108 [1:13:07<07:37,  4.44s/it] 97%|█████████▋| 3006/3108 [1:13:09<06:00,  3.53s/it] 97%|█████████▋| 3007/3108 [1:13:10<04:52,  2.89s/it] 97%|█████████▋| 3008/3108 [1:13:12<04:04,  2.45s/it] 97%|█████████▋| 3009/3108 [1:13:13<03:30,  2.13s/it] 97%|█████████▋| 3010/3108 [1:13:14<03:07,  1.92s/it] 97%|█████████▋| 3011/3108 [1:13:16<02:50,  1.76s/it] 97%|█████████▋| 3012/3108 [1:13:17<02:38,  1.65s/it] 97%|█████████▋| 3013/3108 [1:13:19<02:30,  1.58s/it] 97%|█████████▋| 3014/3108 [1:13:20<02:23,  1.53s/it] 97%|█████████▋| 3015/3108 [1:13:21<02:18,  1.49s/it] 97%|█████████▋| 3016/3108 [1:13:23<02:14,  1.46s/it] 97%|█████████▋| 3017/3108 [1:13:24<02:11,  1.45s/it] 97%|█████████▋| 3018/3108 [1:13:26<02:11,  1.46s/it] 97%|█████████▋| 3019/3108 [1:13:27<02:08,  1.44s/it] 97%|█████████▋| 3020/3108 [1:13:28<02:05,  1.43s/it] 97%|█████████▋| 3021/3108 [1:13:30<02:03,  1.42s/it] 97%|█████████▋| 3022/3108 [1:13:31<02:01,  1.42s/it] 97%|█████████▋| 3023/3108 [1:13:33<02:00,  1.42s/it] 97%|█████████▋| 3024/3108 [1:13:34<01:58,  1.41s/it] 97%|█████████▋| 3025/3108 [1:13:36<01:59,  1.44s/it] 97%|█████████▋| 3026/3108 [1:13:37<01:57,  1.43s/it] 97%|█████████▋| 3027/3108 [1:13:38<01:56,  1.44s/it] 97%|█████████▋| 3028/3108 [1:13:40<01:54,  1.43s/it] 97%|█████████▋| 3029/3108 [1:13:41<01:52,  1.43s/it] 97%|█████████▋| 3030/3108 [1:13:43<01:50,  1.42s/it] 98%|█████████▊| 3031/3108 [1:13:44<01:48,  1.41s/it] 98%|█████████▊| 3032/3108 [1:13:46<01:47,  1.41s/it] 98%|█████████▊| 3033/3108 [1:13:47<01:45,  1.41s/it] 98%|█████████▊| 3034/3108 [1:13:48<01:43,  1.40s/it] 98%|█████████▊| 3035/3108 [1:13:50<01:42,  1.41s/it] 98%|█████████▊| 3036/3108 [1:13:51<01:41,  1.41s/it] 98%|█████████▊| 3037/3108 [1:13:53<01:39,  1.40s/it] 98%|█████████▊| 3038/3108 [1:13:54<01:38,  1.41s/it] 98%|█████████▊| 3039/3108 [1:13:55<01:37,  1.41s/it] 98%|█████████▊| 3040/3108 [1:13:57<01:35,  1.40s/it] 98%|█████████▊| 3041/3108 [1:13:58<01:33,  1.40s/it] 98%|█████████▊| 3042/3108 [1:14:00<01:32,  1.40s/it] 98%|█████████▊| 3043/3108 [1:14:01<01:31,  1.40s/it] 98%|█████████▊| 3044/3108 [1:14:02<01:31,  1.43s/it] 98%|█████████▊| 3045/3108 [1:14:04<01:29,  1.42s/it] 98%|█████████▊| 3046/3108 [1:14:05<01:27,  1.42s/it] 98%|█████████▊| 3047/3108 [1:14:07<01:26,  1.41s/it] 98%|█████████▊| 3048/3108 [1:14:08<01:24,  1.41s/it] 98%|█████████▊| 3049/3108 [1:14:09<01:23,  1.41s/it] 98%|█████████▊| 3050/3108 [1:14:11<01:23,  1.44s/it] 98%|█████████▊| 3051/3108 [1:14:12<01:21,  1.43s/it] 98%|█████████▊| 3052/3108 [1:14:14<01:19,  1.42s/it] 98%|█████████▊| 3053/3108 [1:14:15<01:18,  1.42s/it] 98%|█████████▊| 3054/3108 [1:14:17<01:16,  1.41s/it] 98%|█████████▊| 3055/3108 [1:14:18<01:14,  1.41s/it] 98%|█████████▊| 3056/3108 [1:14:19<01:13,  1.41s/it] 98%|█████████▊| 3057/3108 [1:14:21<01:12,  1.42s/it] 98%|█████████▊| 3058/3108 [1:14:22<01:10,  1.42s/it] 98%|█████████▊| 3059/3108 [1:14:24<01:09,  1.41s/it] 98%|█████████▊| 3060/3108 [1:14:25<01:07,  1.41s/it] 98%|█████████▊| 3061/3108 [1:14:26<01:06,  1.40s/it] 99%|█████████▊| 3062/3108 [1:14:28<01:04,  1.41s/it] 99%|█████████▊| 3063/3108 [1:14:29<01:03,  1.41s/it] 99%|█████████▊| 3064/3108 [1:14:31<01:01,  1.40s/it] 99%|█████████▊| 3065/3108 [1:14:32<01:00,  1.41s/it] 99%|█████████▊| 3066/3108 [1:14:33<00:58,  1.40s/it] 99%|█████████▊| 3067/3108 [1:14:35<00:57,  1.40s/it] 99%|█████████▊| 3068/3108 [1:14:36<00:56,  1.41s/it] 99%|█████████▊| 3069/3108 [1:14:38<00:55,  1.43s/it] 99%|█████████▉| 3070/3108 [1:14:39<00:54,  1.43s/it] 99%|█████████▉| 3071/3108 [1:14:41<00:52,  1.42s/it] 99%|█████████▉| 3072/3108 [1:14:42<00:50,  1.41s/it] 99%|█████████▉| 3073/3108 [1:14:43<00:49,  1.41s/it] 99%|█████████▉| 3074/3108 [1:14:45<00:47,  1.41s/it] 99%|█████████▉| 3075/3108 [1:14:46<00:47,  1.44s/it] 99%|█████████▉| 3076/3108 [1:14:48<00:45,  1.43s/it] 99%|█████████▉| 3077/3108 [1:14:49<00:44,  1.42s/it] 99%|█████████▉| 3078/3108 [1:14:51<00:42,  1.42s/it] 99%|█████████▉| 3079/3108 [1:14:52<00:41,  1.42s/it] 99%|█████████▉| 3080/3108 [1:14:53<00:39,  1.41s/it] 99%|█████████▉| 3081/3108 [1:14:55<00:38,  1.41s/it] 99%|█████████▉| 3082/3108 [1:14:56<00:36,  1.41s/it] 99%|█████████▉| 3083/3108 [1:14:58<00:35,  1.41s/it] 99%|█████████▉| 3084/3108 [1:14:59<00:33,  1.40s/it] 99%|█████████▉| 3085/3108 [1:15:00<00:32,  1.40s/it] 99%|█████████▉| 3086/3108 [1:15:02<00:30,  1.40s/it] 99%|█████████▉| 3087/3108 [1:15:03<00:29,  1.42s/it] 99%|█████████▉| 3088/3108 [1:15:05<00:28,  1.41s/it] 99%|█████████▉| 3089/3108 [1:15:06<00:26,  1.41s/it] 99%|█████████▉| 3090/3108 [1:15:07<00:25,  1.41s/it] 99%|█████████▉| 3091/3108 [1:15:09<00:23,  1.40s/it] 99%|█████████▉| 3092/3108 [1:15:10<00:22,  1.40s/it]100%|█████████▉| 3093/3108 [1:15:12<00:21,  1.41s/it]100%|█████████▉| 3094/3108 [1:15:13<00:20,  1.44s/it]100%|█████████▉| 3095/3108 [1:15:15<00:18,  1.43s/it]100%|█████████▉| 3096/3108 [1:15:16<00:17,  1.42s/it]100%|█████████▉| 3097/3108 [1:15:17<00:15,  1.41s/it]100%|█████████▉| 3098/3108 [1:15:19<00:14,  1.41s/it]100%|█████████▉| 3099/3108 [1:15:20<00:12,  1.41s/it]100%|█████████▉| 3100/3108 [1:15:22<00:11,  1.45s/it]100%|█████████▉| 3101/3108 [1:15:23<00:10,  1.44s/it]100%|█████████▉| 3102/3108 [1:15:25<00:08,  1.43s/it]100%|█████████▉| 3103/3108 [1:15:26<00:06,  1.38s/it]100%|█████████▉| 3104/3108 [1:15:27<00:05,  1.29s/it]100%|█████████▉| 3105/3108 [1:15:28<00:03,  1.22s/it]100%|█████████▉| 3106/3108 [1:15:29<00:02,  1.17s/it]100%|█████████▉| 3107/3108 [1:15:30<00:01,  1.14s/it]100%|██████████| 3108/3108 [1:15:31<00:00,  1.04s/it]                                                     loss: 0.57360487, learning_rate: 0.0, global_step: 3108, interval_runtime: 192.7878, interval_samples_per_second: 10.083627515249805, interval_steps_per_second: 0.5602015286249892, epoch: 4.0
100%|██████████| 3108/3108 [1:15:31<00:00,  1.04s/it][32m[2023-11-10 10:22:54,479] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, token_to_orig_map, token_is_max_context, question_id, questions, tokens, id, start_labels. If end_labels, token_to_orig_map, token_is_max_context, question_id, questions, tokens, id, start_labels are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:22:54,793] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:22:54,793] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:22:54,793] [    INFO][0m -   Total prediction steps = 48[0m
[32m[2023-11-10 10:22:54,793] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:22:54,793] [    INFO][0m -   Total Batch size = 18[0m

  0%|          | 0/48 [00:00<?, ?it/s][A
  4%|▍         | 2/48 [00:00<00:15,  2.93it/s][A
  6%|▋         | 3/48 [00:01<00:21,  2.06it/s][A
  8%|▊         | 4/48 [00:02<00:24,  1.79it/s][A
 10%|█         | 5/48 [00:02<00:25,  1.66it/s][A
 12%|█▎        | 6/48 [00:03<00:26,  1.59it/s][A
 15%|█▍        | 7/48 [00:04<00:26,  1.54it/s][A
 17%|█▋        | 8/48 [00:04<00:26,  1.52it/s][A
 19%|█▉        | 9/48 [00:05<00:26,  1.50it/s][A
 21%|██        | 10/48 [00:06<00:25,  1.49it/s][A
 23%|██▎       | 11/48 [00:06<00:25,  1.48it/s][A
 25%|██▌       | 12/48 [00:07<00:24,  1.47it/s][A
 27%|██▋       | 13/48 [00:08<00:23,  1.47it/s][A
 29%|██▉       | 14/48 [00:08<00:23,  1.47it/s][A
 31%|███▏      | 15/48 [00:09<00:23,  1.41it/s][A
 33%|███▎      | 16/48 [00:10<00:22,  1.42it/s][A
 35%|███▌      | 17/48 [00:11<00:21,  1.43it/s][A
 38%|███▊      | 18/48 [00:11<00:20,  1.44it/s][A
 40%|███▉      | 19/48 [00:12<00:20,  1.44it/s][A
 42%|████▏     | 20/48 [00:13<00:20,  1.38it/s][A
 44%|████▍     | 21/48 [00:13<00:19,  1.40it/s][A
 46%|████▌     | 22/48 [00:14<00:18,  1.42it/s][A
 48%|████▊     | 23/48 [00:15<00:17,  1.43it/s][A
 50%|█████     | 24/48 [00:15<00:16,  1.44it/s][A
 52%|█████▏    | 25/48 [00:16<00:16,  1.38it/s][A
 54%|█████▍    | 26/48 [00:17<00:15,  1.41it/s][A
 56%|█████▋    | 27/48 [00:18<00:14,  1.42it/s][A
 58%|█████▊    | 28/48 [00:18<00:13,  1.43it/s][A
 60%|██████    | 29/48 [00:19<00:13,  1.44it/s][A
 62%|██████▎   | 30/48 [00:20<00:12,  1.45it/s][A
 65%|██████▍   | 31/48 [00:20<00:11,  1.45it/s][A
 67%|██████▋   | 32/48 [00:21<00:11,  1.45it/s][A
 69%|██████▉   | 33/48 [00:22<00:10,  1.45it/s][A
 71%|███████   | 34/48 [00:22<00:09,  1.45it/s][A
 73%|███████▎  | 35/48 [00:23<00:08,  1.46it/s][A
 75%|███████▌  | 36/48 [00:24<00:08,  1.46it/s][A
 77%|███████▋  | 37/48 [00:24<00:07,  1.46it/s][A
 79%|███████▉  | 38/48 [00:25<00:06,  1.46it/s][A
 81%|████████▏ | 39/48 [00:26<00:06,  1.46it/s][A
 83%|████████▎ | 40/48 [00:27<00:05,  1.46it/s][A
 85%|████████▌ | 41/48 [00:27<00:04,  1.46it/s][A
 88%|████████▊ | 42/48 [00:28<00:04,  1.46it/s][A
 90%|████████▉ | 43/48 [00:28<00:03,  1.65it/s][A
 92%|█████████▏| 44/48 [00:29<00:02,  1.94it/s][A
 94%|█████████▍| 45/48 [00:29<00:01,  2.23it/s][A
 96%|█████████▌| 46/48 [00:29<00:00,  2.48it/s][A
 98%|█████████▊| 47/48 [00:30<00:00,  2.69it/s][A
100%|██████████| 48/48 [00:30<00:00,  3.43it/s][A                                                     
                                               [Aeval_anls: 59.94392136661869, epoch: 4.0
100%|██████████| 3108/3108 [1:16:05<00:00,  1.04s/it]
100%|██████████| 48/48 [00:30<00:00,  3.43it/s][A
                                               [A[32m[2023-11-10 10:23:28,172] [    INFO][0m - Saving model checkpoint to ./models/fidelity/checkpoint-3108[0m
[32m[2023-11-10 10:23:28,180] [    INFO][0m - Configuration saved in ./models/fidelity/checkpoint-3108/config.json[0m
[32m[2023-11-10 10:23:30,724] [    INFO][0m - Model weights saved in ./models/fidelity/checkpoint-3108/model_state.pdparams[0m
[32m[2023-11-10 10:23:30,725] [    INFO][0m - tokenizer config file saved in ./models/fidelity/checkpoint-3108/tokenizer_config.json[0m
[32m[2023-11-10 10:23:30,725] [    INFO][0m - Special tokens file saved in ./models/fidelity/checkpoint-3108/special_tokens_map.json[0m
[32m[2023-11-10 10:23:35,776] [    INFO][0m - Deleting older checkpoint [models/fidelity/checkpoint-3000] due to args.save_total_limit[0m
[32m[2023-11-10 10:23:36,146] [    INFO][0m - 
Training completed. 
[0m
[32m[2023-11-10 10:23:36,147] [    INFO][0m - Loading best model from ./models/fidelity/checkpoint-1000 (score: 61.12011982881637).[0m
[32m[2023-11-10 10:23:37,038] [    INFO][0m - set state-dict :([], [])[0m
                                                     train_runtime: 4573.9803, train_samples_per_second: 12.22392682397285, train_steps_per_second: 0.6794957177154746, train_loss: 0.9772201253333761, epoch: 4.0
100%|██████████| 3108/3108 [1:16:13<00:00,  1.04s/it]100%|██████████| 3108/3108 [1:16:13<00:00,  1.47s/it]
[32m[2023-11-10 10:23:37,087] [    INFO][0m - Saving model checkpoint to ./models/fidelity/[0m
[32m[2023-11-10 10:23:37,096] [    INFO][0m - Configuration saved in ./models/fidelity/config.json[0m
[32m[2023-11-10 10:23:39,662] [    INFO][0m - Model weights saved in ./models/fidelity/model_state.pdparams[0m
[32m[2023-11-10 10:23:39,662] [    INFO][0m - tokenizer config file saved in ./models/fidelity/tokenizer_config.json[0m
[32m[2023-11-10 10:23:39,662] [    INFO][0m - Special tokens file saved in ./models/fidelity/special_tokens_map.json[0m
[32m[2023-11-10 10:23:39,669] [    INFO][0m - ***** train metrics *****[0m
[32m[2023-11-10 10:23:39,669] [    INFO][0m -   epoch                    =        4.0[0m
[32m[2023-11-10 10:23:39,669] [    INFO][0m -   train_loss               =     0.9772[0m
[32m[2023-11-10 10:23:39,669] [    INFO][0m -   train_runtime            = 1:16:13.98[0m
[32m[2023-11-10 10:23:39,669] [    INFO][0m -   train_samples            =      13978[0m
[32m[2023-11-10 10:23:39,669] [    INFO][0m -   train_samples_per_second =    12.2239[0m
[32m[2023-11-10 10:23:39,669] [    INFO][0m -   train_steps_per_second   =     0.6795[0m
[32m[2023-11-10 10:23:39,670] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, token_to_orig_map, token_is_max_context, question_id, questions, tokens, id, start_labels. If end_labels, token_to_orig_map, token_is_max_context, question_id, questions, tokens, id, start_labels are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:23:40,094] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:23:40,094] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:23:40,095] [    INFO][0m -   Total prediction steps = 48[0m
[32m[2023-11-10 10:23:40,095] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:23:40,095] [    INFO][0m -   Total Batch size = 18[0m
  0%|          | 0/48 [00:00<?, ?it/s]  4%|▍         | 2/48 [00:00<00:15,  2.90it/s]  6%|▋         | 3/48 [00:01<00:21,  2.06it/s]  8%|▊         | 4/48 [00:02<00:26,  1.67it/s] 10%|█         | 5/48 [00:02<00:27,  1.59it/s] 12%|█▎        | 6/48 [00:03<00:27,  1.55it/s] 15%|█▍        | 7/48 [00:04<00:27,  1.52it/s] 17%|█▋        | 8/48 [00:04<00:26,  1.50it/s] 19%|█▉        | 9/48 [00:05<00:27,  1.41it/s] 21%|██        | 10/48 [00:06<00:27,  1.37it/s] 23%|██▎       | 11/48 [00:07<00:26,  1.40it/s] 25%|██▌       | 12/48 [00:07<00:25,  1.42it/s] 27%|██▋       | 13/48 [00:08<00:24,  1.43it/s] 29%|██▉       | 14/48 [00:09<00:23,  1.44it/s] 31%|███▏      | 15/48 [00:09<00:22,  1.44it/s] 33%|███▎      | 16/48 [00:10<00:22,  1.45it/s] 35%|███▌      | 17/48 [00:11<00:21,  1.44it/s] 38%|███▊      | 18/48 [00:11<00:20,  1.45it/s] 40%|███▉      | 19/48 [00:12<00:20,  1.45it/s] 42%|████▏     | 20/48 [00:13<00:19,  1.45it/s] 44%|████▍     | 21/48 [00:14<00:18,  1.45it/s] 46%|████▌     | 22/48 [00:14<00:17,  1.46it/s] 48%|████▊     | 23/48 [00:15<00:17,  1.46it/s] 50%|█████     | 24/48 [00:16<00:16,  1.46it/s] 52%|█████▏    | 25/48 [00:16<00:15,  1.46it/s] 54%|█████▍    | 26/48 [00:17<00:15,  1.46it/s] 56%|█████▋    | 27/48 [00:18<00:14,  1.46it/s] 58%|█████▊    | 28/48 [00:18<00:13,  1.46it/s] 60%|██████    | 29/48 [00:19<00:13,  1.39it/s] 62%|██████▎   | 30/48 [00:20<00:12,  1.41it/s] 65%|██████▍   | 31/48 [00:20<00:11,  1.42it/s] 67%|██████▋   | 32/48 [00:21<00:11,  1.43it/s] 69%|██████▉   | 33/48 [00:22<00:10,  1.44it/s] 71%|███████   | 34/48 [00:23<00:09,  1.44it/s] 73%|███████▎  | 35/48 [00:23<00:09,  1.43it/s] 75%|███████▌  | 36/48 [00:24<00:08,  1.44it/s] 77%|███████▋  | 37/48 [00:25<00:07,  1.44it/s] 79%|███████▉  | 38/48 [00:25<00:06,  1.44it/s] 81%|████████▏ | 39/48 [00:26<00:06,  1.43it/s] 83%|████████▎ | 40/48 [00:27<00:05,  1.35it/s] 85%|████████▌ | 41/48 [00:28<00:05,  1.34it/s] 88%|████████▊ | 42/48 [00:28<00:04,  1.37it/s] 90%|████████▉ | 43/48 [00:29<00:03,  1.56it/s] 92%|█████████▏| 44/48 [00:29<00:02,  1.86it/s] 94%|█████████▍| 45/48 [00:29<00:01,  2.15it/s] 96%|█████████▌| 46/48 [00:30<00:00,  2.40it/s] 98%|█████████▊| 47/48 [00:30<00:00,  2.62it/s]100%|██████████| 48/48 [00:30<00:00,  3.35it/s]100%|██████████| 48/48 [00:31<00:00,  1.55it/s]
[32m[2023-11-10 10:24:13,754] [    INFO][0m - ***** eval metrics *****[0m
[32m[2023-11-10 10:24:13,754] [    INFO][0m -   epoch     =     4.0[0m
[32m[2023-11-10 10:24:13,754] [    INFO][0m -   eval_anls = 61.3352[0m
I1110 10:24:14.782049 60869 tcp_store.cc:273] receive shutdown event and so quit from MasterDaemon run loop
[33m[2023-11-10 10:31:51,634] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 10:31:52,700] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')
=======================================================================
I1110 10:31:52.700776 172460 tcp_utils.cc:181] The server starts to listen on IP_ANY:42301
I1110 10:31:52.700903 172460 tcp_utils.cc:130] Successfully connected to 172.31.1.102:42301
W1110 10:31:57.350843 172460 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 10:31:57.356832 172460 gpu_resources.cc:149] device: 0, cuDNN Version: 8.5.
[32m[2023-11-10 10:31:58,004] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 10:31:58,004] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - model_name_or_path            : doc15k[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - [0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 10:31:58,005] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - return_entity_level_metrics   : False[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 10:31:58,006] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 10:31:58,007] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 10:31:58,007] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 10:31:58,007] [    INFO][0m - [0m
[32m[2023-11-10 10:31:58,045] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'> to load 'doc15k'.[0m
[32m[2023-11-10 10:31:58,601] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.modeling.ErnieLayoutForQuestionAnswering'> to load 'doc15k'.[0m
[32m[2023-11-10 10:31:58,602] [    INFO][0m - Loading configuration file doc15k/config.json[0m
[32m[2023-11-10 10:31:58,602] [    INFO][0m - Loading weights file doc15k/model_state.pdparams[0m
[32m[2023-11-10 10:31:59,903] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-11-10 10:32:01,357] [    INFO][0m - All model checkpoint weights were used when initializing ErnieLayoutForQuestionAnswering.
[0m
[32m[2023-11-10 10:32:01,357] [    INFO][0m - All the weights of ErnieLayoutForQuestionAnswering were initialized from the model checkpoint at doc15k.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieLayoutForQuestionAnswering for predictions without further training.[0m
[32m[2023-11-10 10:32:01,389] [    INFO][0m - spliting train dataset into 16 shard[0m
[32m[2023-11-10 10:32:38,099] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 10:32:38,099] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-11-10 10:32:38,099] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - bf16                          : False[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - current_device                : gpu:0[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-11-10 10:32:38,100] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - device                        : gpu[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - do_eval                       : True[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - do_export                     : False[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - do_predict                    : False[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - do_train                      : True[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - eval_batch_size               : 6[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - eval_steps                    : 100[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - fp16                          : False[0m
[32m[2023-11-10 10:32:38,101] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - fp16_opt_level                : O1[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - greater_is_better             : True[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - label_names                   : ['start_positions', 'end_positions'][0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - learning_rate                 : 2e-05[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - load_sharded_model            : False[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - local_process_index           : 0[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - local_rank                    : 0[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - log_level                     : -1[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-11-10 10:32:38,102] [    INFO][0m - logging_dir                   : ./models/fidelity_save_100/runs/Nov10_10-31-52_ip-172-31-1-102[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - logging_steps                 : 500[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - metric_for_best_model         : anls[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - num_train_epochs              : 4.0[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-11-10 10:32:38,103] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - output_dir                    : ./models/fidelity_save_100[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - overwrite_output_dir          : True[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - past_index                    : -1[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - per_device_eval_batch_size    : 6[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - per_device_train_batch_size   : 6[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - power                         : 1.0[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - process_index                 : 0[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - recompute                     : False[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-11-10 10:32:38,104] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - run_name                      : ./models/fidelity_save_100[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - save_sharded_model            : False[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - save_steps                    : 100[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - save_total_limit              : 1[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - seed                          : 1000[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - sharding                      : [][0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - should_log                    : True[0m
[32m[2023-11-10 10:32:38,105] [    INFO][0m - should_save                   : True[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - train_batch_size              : 6[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - world_size                    : 4[0m
[32m[2023-11-10 10:32:38,106] [    INFO][0m - [0m
[32m[2023-11-10 10:32:38,107] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens. If end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:32:44,382] [    INFO][0m - ***** Running training *****[0m
[32m[2023-11-10 10:32:44,382] [    INFO][0m -   Num examples = 13,978[0m
[32m[2023-11-10 10:32:44,383] [    INFO][0m -   Num Epochs = 4[0m
[32m[2023-11-10 10:32:44,383] [    INFO][0m -   Instantaneous batch size per device = 6[0m
[32m[2023-11-10 10:32:44,383] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 24[0m
[32m[2023-11-10 10:32:44,383] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-11-10 10:32:44,383] [    INFO][0m -   Total optimization steps = 2,332[0m
[32m[2023-11-10 10:32:44,383] [    INFO][0m -   Total num train samples = 55,912[0m
[32m[2023-11-10 10:32:44,385] [    INFO][0m -   Number of trainable parameters = 281,693,122 (per device)[0m
  0%|          | 0/2332 [00:00<?, ?it/s]  0%|          | 1/2332 [00:06<4:02:23,  6.24s/it]  0%|          | 2/2332 [00:07<2:12:11,  3.40s/it]  0%|          | 3/2332 [00:09<1:36:50,  2.49s/it]  0%|          | 4/2332 [00:10<1:20:18,  2.07s/it]  0%|          | 5/2332 [00:11<1:11:38,  1.85s/it]  0%|          | 6/2332 [00:13<1:05:59,  1.70s/it]  0%|          | 7/2332 [00:14<1:02:17,  1.61s/it]  0%|          | 8/2332 [00:16<59:47,  1.54s/it]    0%|          | 9/2332 [00:17<58:17,  1.51s/it]  0%|          | 10/2332 [00:19<57:10,  1.48s/it]  0%|          | 11/2332 [00:20<56:21,  1.46s/it]  1%|          | 12/2332 [00:21<55:48,  1.44s/it]  1%|          | 13/2332 [00:23<55:24,  1.43s/it]  1%|          | 14/2332 [00:24<55:08,  1.43s/it]  1%|          | 15/2332 [00:26<54:58,  1.42s/it]  1%|          | 16/2332 [00:27<54:45,  1.42s/it]  1%|          | 17/2332 [00:28<54:41,  1.42s/it]  1%|          | 18/2332 [00:30<54:35,  1.42s/it]  1%|          | 19/2332 [00:31<54:30,  1.41s/it]  1%|          | 20/2332 [00:33<54:22,  1.41s/it]  1%|          | 21/2332 [00:34<54:25,  1.41s/it]  1%|          | 22/2332 [00:35<54:20,  1.41s/it]  1%|          | 23/2332 [00:37<54:16,  1.41s/it]  1%|          | 24/2332 [00:38<54:16,  1.41s/it]  1%|          | 25/2332 [00:40<54:18,  1.41s/it]  1%|          | 26/2332 [00:41<54:22,  1.41s/it]  1%|          | 27/2332 [00:43<54:16,  1.41s/it]  1%|          | 28/2332 [00:44<54:13,  1.41s/it]  1%|          | 29/2332 [00:45<54:11,  1.41s/it]  1%|▏         | 30/2332 [00:47<54:10,  1.41s/it]  1%|▏         | 31/2332 [00:48<54:10,  1.41s/it]  1%|▏         | 32/2332 [00:50<54:02,  1.41s/it]  1%|▏         | 33/2332 [00:51<54:12,  1.41s/it]  1%|▏         | 34/2332 [00:52<54:20,  1.42s/it]  2%|▏         | 35/2332 [00:54<54:11,  1.42s/it]  2%|▏         | 36/2332 [00:55<54:10,  1.42s/it]  2%|▏         | 37/2332 [00:57<54:08,  1.42s/it]  2%|▏         | 38/2332 [00:58<54:09,  1.42s/it]  2%|▏         | 39/2332 [01:00<54:10,  1.42s/it]  2%|▏         | 40/2332 [01:01<54:05,  1.42s/it]  2%|▏         | 41/2332 [01:02<53:58,  1.41s/it]  2%|▏         | 42/2332 [01:04<53:54,  1.41s/it]  2%|▏         | 43/2332 [01:05<53:51,  1.41s/it]  2%|▏         | 44/2332 [01:07<53:54,  1.41s/it]  2%|▏         | 45/2332 [01:08<55:15,  1.45s/it]  2%|▏         | 46/2332 [01:09<54:41,  1.44s/it]  2%|▏         | 47/2332 [01:11<54:28,  1.43s/it]  2%|▏         | 48/2332 [01:12<54:16,  1.43s/it]  2%|▏         | 49/2332 [01:14<54:08,  1.42s/it]  2%|▏         | 50/2332 [01:15<53:53,  1.42s/it]  2%|▏         | 51/2332 [01:17<53:52,  1.42s/it]  2%|▏         | 52/2332 [01:18<53:51,  1.42s/it]  2%|▏         | 53/2332 [01:19<53:44,  1.41s/it]  2%|▏         | 54/2332 [01:21<54:51,  1.45s/it]  2%|▏         | 55/2332 [01:22<54:29,  1.44s/it]  2%|▏         | 56/2332 [01:24<54:11,  1.43s/it]  2%|▏         | 57/2332 [01:25<53:59,  1.42s/it]  2%|▏         | 58/2332 [01:27<53:55,  1.42s/it]  3%|▎         | 59/2332 [01:28<53:41,  1.42s/it]  3%|▎         | 60/2332 [01:29<53:39,  1.42s/it]  3%|▎         | 61/2332 [01:31<53:35,  1.42s/it]  3%|▎         | 62/2332 [01:32<53:27,  1.41s/it]  3%|▎         | 63/2332 [01:34<53:27,  1.41s/it]  3%|▎         | 64/2332 [01:35<53:24,  1.41s/it]  3%|▎         | 65/2332 [01:36<53:21,  1.41s/it]  3%|▎         | 66/2332 [01:38<53:22,  1.41s/it]  3%|▎         | 67/2332 [01:39<53:20,  1.41s/it]  3%|▎         | 68/2332 [01:41<53:11,  1.41s/it]  3%|▎         | 69/2332 [01:42<53:19,  1.41s/it]  3%|▎         | 70/2332 [01:44<53:15,  1.41s/it]  3%|▎         | 71/2332 [01:45<54:21,  1.44s/it]  3%|▎         | 72/2332 [01:46<53:56,  1.43s/it]  3%|▎         | 73/2332 [01:48<53:40,  1.43s/it]  3%|▎         | 74/2332 [01:49<53:30,  1.42s/it]  3%|▎         | 75/2332 [01:51<53:22,  1.42s/it]  3%|▎         | 76/2332 [01:52<53:18,  1.42s/it]  3%|▎         | 77/2332 [01:53<53:10,  1.41s/it]  3%|▎         | 78/2332 [01:55<53:08,  1.41s/it]  3%|▎         | 79/2332 [01:56<53:04,  1.41s/it]  3%|▎         | 80/2332 [01:58<53:02,  1.41s/it]  3%|▎         | 81/2332 [01:59<52:59,  1.41s/it]  4%|▎         | 82/2332 [02:01<52:58,  1.41s/it]  4%|▎         | 83/2332 [02:02<52:54,  1.41s/it]  4%|▎         | 84/2332 [02:03<53:55,  1.44s/it]  4%|▎         | 85/2332 [02:05<53:36,  1.43s/it]  4%|▎         | 86/2332 [02:06<53:29,  1.43s/it]  4%|▎         | 87/2332 [02:08<53:22,  1.43s/it]  4%|▍         | 88/2332 [02:09<53:18,  1.43s/it]  4%|▍         | 89/2332 [02:11<53:08,  1.42s/it]  4%|▍         | 90/2332 [02:12<52:59,  1.42s/it]  4%|▍         | 91/2332 [02:13<52:53,  1.42s/it]  4%|▍         | 92/2332 [02:15<52:58,  1.42s/it]  4%|▍         | 93/2332 [02:16<52:55,  1.42s/it]  4%|▍         | 94/2332 [02:18<52:41,  1.41s/it]  4%|▍         | 95/2332 [02:19<52:47,  1.42s/it]  4%|▍         | 96/2332 [02:21<53:44,  1.44s/it]  4%|▍         | 97/2332 [02:22<53:24,  1.43s/it]  4%|▍         | 98/2332 [02:23<53:15,  1.43s/it]  4%|▍         | 99/2332 [02:25<53:01,  1.42s/it]  4%|▍         | 100/2332 [02:26<52:44,  1.42s/it][32m[2023-11-10 10:35:11,084] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens. If end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:35:11,532] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:35:11,532] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:35:11,532] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:35:11,533] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:35:11,533] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.93it/s][A
  8%|▊         | 3/36 [00:01<00:16,  2.06it/s][A
 11%|█         | 4/36 [00:02<00:17,  1.79it/s][A
 14%|█▍        | 5/36 [00:02<00:18,  1.65it/s][A
 17%|█▋        | 6/36 [00:03<00:18,  1.59it/s][A
 19%|█▉        | 7/36 [00:04<00:18,  1.54it/s][A
 22%|██▏       | 8/36 [00:04<00:18,  1.51it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.49it/s][A
 28%|██▊       | 10/36 [00:06<00:17,  1.48it/s][A
 31%|███       | 11/36 [00:06<00:17,  1.46it/s][A
 33%|███▎      | 12/36 [00:07<00:17,  1.40it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.42it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.43it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.44it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.43it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.44it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.45it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.45it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.45it/s][A
 58%|█████▊    | 21/36 [00:13<00:10,  1.45it/s][A
 61%|██████    | 22/36 [00:14<00:09,  1.45it/s][A
 64%|██████▍   | 23/36 [00:15<00:08,  1.45it/s][A
 67%|██████▋   | 24/36 [00:15<00:08,  1.45it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.45it/s][A
 72%|███████▏  | 26/36 [00:17<00:06,  1.45it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.45it/s][A
 78%|███████▊  | 28/36 [00:18<00:05,  1.38it/s][A
 81%|████████  | 29/36 [00:19<00:04,  1.40it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.42it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.56it/s][A
 89%|████████▉ | 32/36 [00:20<00:02,  1.86it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.15it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.41it/s][A
 97%|█████████▋| 35/36 [00:21<00:00,  2.63it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.20it/s][A                                                  
                                               [Aeval_anls: 50.90847628806079, epoch: 0.1715
  4%|▍         | 100/2332 [02:55<52:44,  1.42s/it]
100%|██████████| 36/36 [00:25<00:00,  3.20it/s][A
                                               [A[32m[2023-11-10 10:35:39,653] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-100[0m
[32m[2023-11-10 10:35:39,662] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-100/config.json[0m
[32m[2023-11-10 10:35:42,194] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-100/model_state.pdparams[0m
[32m[2023-11-10 10:35:42,194] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-100/tokenizer_config.json[0m
[32m[2023-11-10 10:35:42,194] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-100/special_tokens_map.json[0m
  4%|▍         | 101/2332 [03:04<7:36:13, 12.27s/it]  4%|▍         | 102/2332 [03:05<5:35:02,  9.01s/it]  4%|▍         | 103/2332 [03:07<4:11:21,  6.77s/it]  4%|▍         | 104/2332 [03:08<3:11:40,  5.16s/it]  5%|▍         | 105/2332 [03:10<2:29:51,  4.04s/it]  5%|▍         | 106/2332 [03:11<2:00:35,  3.25s/it]  5%|▍         | 107/2332 [03:12<1:40:01,  2.70s/it]  5%|▍         | 108/2332 [03:14<1:25:46,  2.31s/it]  5%|▍         | 109/2332 [03:15<1:15:42,  2.04s/it]  5%|▍         | 110/2332 [03:17<1:08:40,  1.85s/it]  5%|▍         | 111/2332 [03:18<1:03:43,  1.72s/it]  5%|▍         | 112/2332 [03:19<1:00:13,  1.63s/it]  5%|▍         | 113/2332 [03:21<57:49,  1.56s/it]    5%|▍         | 114/2332 [03:22<56:01,  1.52s/it]  5%|▍         | 115/2332 [03:24<54:55,  1.49s/it]  5%|▍         | 116/2332 [03:25<54:02,  1.46s/it]  5%|▌         | 117/2332 [03:27<53:30,  1.45s/it]  5%|▌         | 118/2332 [03:28<53:02,  1.44s/it]  5%|▌         | 119/2332 [03:29<52:45,  1.43s/it]  5%|▌         | 120/2332 [03:31<52:32,  1.43s/it]  5%|▌         | 121/2332 [03:32<53:42,  1.46s/it]  5%|▌         | 122/2332 [03:34<53:05,  1.44s/it]  5%|▌         | 123/2332 [03:35<52:52,  1.44s/it]  5%|▌         | 124/2332 [03:37<52:31,  1.43s/it]  5%|▌         | 125/2332 [03:38<52:22,  1.42s/it]  5%|▌         | 126/2332 [03:39<52:14,  1.42s/it]  5%|▌         | 127/2332 [03:41<52:11,  1.42s/it]  5%|▌         | 128/2332 [03:42<53:19,  1.45s/it]  6%|▌         | 129/2332 [03:44<52:55,  1.44s/it]  6%|▌         | 130/2332 [03:45<52:36,  1.43s/it]  6%|▌         | 131/2332 [03:47<52:29,  1.43s/it]  6%|▌         | 132/2332 [03:48<52:15,  1.43s/it]  6%|▌         | 133/2332 [03:49<52:03,  1.42s/it]  6%|▌         | 134/2332 [03:51<52:00,  1.42s/it]  6%|▌         | 135/2332 [03:52<51:56,  1.42s/it]  6%|▌         | 136/2332 [03:54<51:47,  1.42s/it]  6%|▌         | 137/2332 [03:55<51:46,  1.42s/it]  6%|▌         | 138/2332 [03:56<51:39,  1.41s/it]  6%|▌         | 139/2332 [03:58<51:40,  1.41s/it]  6%|▌         | 140/2332 [03:59<51:39,  1.41s/it]  6%|▌         | 141/2332 [04:01<51:41,  1.42s/it]  6%|▌         | 142/2332 [04:02<51:43,  1.42s/it]  6%|▌         | 143/2332 [04:04<51:38,  1.42s/it]  6%|▌         | 144/2332 [04:05<51:38,  1.42s/it]  6%|▌         | 145/2332 [04:06<51:33,  1.41s/it]  6%|▋         | 146/2332 [04:08<52:47,  1.45s/it]  6%|▋         | 147/2332 [04:09<52:28,  1.44s/it]  6%|▋         | 148/2332 [04:11<52:09,  1.43s/it]  6%|▋         | 149/2332 [04:12<51:59,  1.43s/it]  6%|▋         | 150/2332 [04:14<51:43,  1.42s/it]  6%|▋         | 151/2332 [04:15<51:43,  1.42s/it]  7%|▋         | 152/2332 [04:16<51:39,  1.42s/it]  7%|▋         | 153/2332 [04:18<52:42,  1.45s/it]  7%|▋         | 154/2332 [04:19<52:20,  1.44s/it]  7%|▋         | 155/2332 [04:21<52:00,  1.43s/it]  7%|▋         | 156/2332 [04:22<51:47,  1.43s/it]  7%|▋         | 157/2332 [04:24<51:38,  1.42s/it]  7%|▋         | 158/2332 [04:25<51:27,  1.42s/it]  7%|▋         | 159/2332 [04:26<51:19,  1.42s/it]  7%|▋         | 160/2332 [04:28<51:12,  1.41s/it]  7%|▋         | 161/2332 [04:29<51:11,  1.41s/it]  7%|▋         | 162/2332 [04:31<51:09,  1.41s/it]  7%|▋         | 163/2332 [04:32<51:04,  1.41s/it]  7%|▋         | 164/2332 [04:33<51:04,  1.41s/it]  7%|▋         | 165/2332 [04:35<51:00,  1.41s/it]  7%|▋         | 166/2332 [04:36<51:01,  1.41s/it]  7%|▋         | 167/2332 [04:38<50:54,  1.41s/it]  7%|▋         | 168/2332 [04:39<51:00,  1.41s/it]  7%|▋         | 169/2332 [04:41<50:56,  1.41s/it]  7%|▋         | 170/2332 [04:42<50:57,  1.41s/it]  7%|▋         | 171/2332 [04:43<50:56,  1.41s/it]  7%|▋         | 172/2332 [04:45<52:10,  1.45s/it]  7%|▋         | 173/2332 [04:46<51:42,  1.44s/it]  7%|▋         | 174/2332 [04:48<51:29,  1.43s/it]  8%|▊         | 175/2332 [04:49<51:12,  1.42s/it]  8%|▊         | 176/2332 [04:51<51:07,  1.42s/it]  8%|▊         | 177/2332 [04:52<51:00,  1.42s/it]  8%|▊         | 178/2332 [04:53<52:04,  1.45s/it]  8%|▊         | 179/2332 [04:55<51:43,  1.44s/it]  8%|▊         | 180/2332 [04:56<51:21,  1.43s/it]  8%|▊         | 181/2332 [04:58<51:10,  1.43s/it]  8%|▊         | 182/2332 [04:59<50:57,  1.42s/it]  8%|▊         | 183/2332 [05:01<50:48,  1.42s/it]  8%|▊         | 184/2332 [05:02<50:44,  1.42s/it]  8%|▊         | 185/2332 [05:03<50:40,  1.42s/it]  8%|▊         | 186/2332 [05:05<50:35,  1.41s/it]  8%|▊         | 187/2332 [05:06<50:38,  1.42s/it]  8%|▊         | 188/2332 [05:08<50:36,  1.42s/it]  8%|▊         | 189/2332 [05:09<50:34,  1.42s/it]  8%|▊         | 190/2332 [05:10<50:30,  1.41s/it]  8%|▊         | 191/2332 [05:12<50:28,  1.41s/it]  8%|▊         | 192/2332 [05:13<50:24,  1.41s/it]  8%|▊         | 193/2332 [05:15<50:22,  1.41s/it]  8%|▊         | 194/2332 [05:16<50:19,  1.41s/it]  8%|▊         | 195/2332 [05:17<50:17,  1.41s/it]  8%|▊         | 196/2332 [05:19<50:10,  1.41s/it]  8%|▊         | 197/2332 [05:20<50:15,  1.41s/it]  8%|▊         | 198/2332 [05:22<51:32,  1.45s/it]  9%|▊         | 199/2332 [05:23<51:06,  1.44s/it]  9%|▊         | 200/2332 [05:25<50:42,  1.43s/it][32m[2023-11-10 10:38:09,544] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens. If end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:38:09,875] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:38:09,875] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:38:09,875] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:38:09,875] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:38:09,875] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.91it/s][A
  8%|▊         | 3/36 [00:01<00:17,  1.87it/s][A
 11%|█         | 4/36 [00:02<00:18,  1.69it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.60it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.55it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.52it/s][A
 22%|██▏       | 8/36 [00:04<00:18,  1.50it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.49it/s][A
 28%|██▊       | 10/36 [00:06<00:17,  1.48it/s][A
 31%|███       | 11/36 [00:06<00:16,  1.47it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.47it/s][A
 36%|███▌      | 13/36 [00:08<00:15,  1.47it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.46it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.46it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.46it/s][A
 47%|████▋     | 17/36 [00:11<00:12,  1.46it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.46it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.46it/s][A
 56%|█████▌    | 20/36 [00:13<00:10,  1.46it/s][A
 58%|█████▊    | 21/36 [00:13<00:10,  1.46it/s][A
 61%|██████    | 22/36 [00:14<00:09,  1.46it/s][A
 64%|██████▍   | 23/36 [00:15<00:08,  1.46it/s][A
 67%|██████▋   | 24/36 [00:15<00:08,  1.46it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.46it/s][A
 72%|███████▏  | 26/36 [00:17<00:06,  1.46it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.39it/s][A
 78%|███████▊  | 28/36 [00:18<00:05,  1.41it/s][A
 81%|████████  | 29/36 [00:19<00:04,  1.42it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.43it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.58it/s][A
 89%|████████▉ | 32/36 [00:20<00:02,  1.88it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.16it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.41it/s][A
 97%|█████████▋| 35/36 [00:21<00:00,  2.63it/s][A
100%|██████████| 36/36 [00:21<00:00,  3.20it/s][A                                                  
                                               [Aeval_anls: 58.94764221281235, epoch: 0.3431
  9%|▊         | 200/2332 [05:50<50:42,  1.43s/it]
100%|██████████| 36/36 [00:22<00:00,  3.20it/s][A
                                               [A[32m[2023-11-10 10:38:35,083] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-200[0m
[32m[2023-11-10 10:38:35,092] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-200/config.json[0m
[32m[2023-11-10 10:38:37,631] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-200/model_state.pdparams[0m
[32m[2023-11-10 10:38:37,632] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-200/tokenizer_config.json[0m
[32m[2023-11-10 10:38:37,632] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-200/special_tokens_map.json[0m
  9%|▊         | 201/2332 [05:59<6:44:04, 11.38s/it]  9%|▊         | 202/2332 [06:01<4:57:51,  8.39s/it]  9%|▊         | 203/2332 [06:02<3:44:35,  6.33s/it]  9%|▊         | 204/2332 [06:04<2:52:11,  4.85s/it]  9%|▉         | 205/2332 [06:05<2:15:36,  3.83s/it]  9%|▉         | 206/2332 [06:06<1:49:49,  3.10s/it]  9%|▉         | 207/2332 [06:08<1:31:53,  2.59s/it]  9%|▉         | 208/2332 [06:09<1:19:13,  2.24s/it]  9%|▉         | 209/2332 [06:11<1:10:27,  1.99s/it]  9%|▉         | 210/2332 [06:12<1:04:19,  1.82s/it]  9%|▉         | 211/2332 [06:14<1:00:03,  1.70s/it]  9%|▉         | 212/2332 [06:15<57:04,  1.62s/it]    9%|▉         | 213/2332 [06:16<54:53,  1.55s/it]  9%|▉         | 214/2332 [06:18<53:22,  1.51s/it]  9%|▉         | 215/2332 [06:19<52:11,  1.48s/it]  9%|▉         | 216/2332 [06:21<51:34,  1.46s/it]  9%|▉         | 217/2332 [06:22<51:08,  1.45s/it]  9%|▉         | 218/2332 [06:23<50:41,  1.44s/it]  9%|▉         | 219/2332 [06:25<50:26,  1.43s/it]  9%|▉         | 220/2332 [06:26<50:12,  1.43s/it]  9%|▉         | 221/2332 [06:28<49:57,  1.42s/it] 10%|▉         | 222/2332 [06:29<50:01,  1.42s/it] 10%|▉         | 223/2332 [06:30<49:57,  1.42s/it] 10%|▉         | 224/2332 [06:32<51:07,  1.46s/it] 10%|▉         | 225/2332 [06:33<50:37,  1.44s/it] 10%|▉         | 226/2332 [06:35<50:16,  1.43s/it] 10%|▉         | 227/2332 [06:36<50:03,  1.43s/it] 10%|▉         | 228/2332 [06:38<51:06,  1.46s/it] 10%|▉         | 229/2332 [06:39<50:43,  1.45s/it] 10%|▉         | 230/2332 [06:41<50:17,  1.44s/it] 10%|▉         | 231/2332 [06:42<50:01,  1.43s/it] 10%|▉         | 232/2332 [06:43<49:52,  1.42s/it] 10%|▉         | 233/2332 [06:45<49:43,  1.42s/it] 10%|█         | 234/2332 [06:46<49:30,  1.42s/it] 10%|█         | 235/2332 [06:48<49:32,  1.42s/it] 10%|█         | 236/2332 [06:49<49:28,  1.42s/it] 10%|█         | 237/2332 [06:51<49:27,  1.42s/it] 10%|█         | 238/2332 [06:52<49:21,  1.41s/it] 10%|█         | 239/2332 [06:53<49:15,  1.41s/it] 10%|█         | 240/2332 [06:55<49:15,  1.41s/it] 10%|█         | 241/2332 [06:56<49:18,  1.41s/it] 10%|█         | 242/2332 [06:58<49:16,  1.41s/it] 10%|█         | 243/2332 [06:59<49:08,  1.41s/it] 10%|█         | 244/2332 [07:00<49:12,  1.41s/it] 11%|█         | 245/2332 [07:02<49:07,  1.41s/it] 11%|█         | 246/2332 [07:03<49:13,  1.42s/it] 11%|█         | 247/2332 [07:05<49:21,  1.42s/it] 11%|█         | 248/2332 [07:06<49:14,  1.42s/it] 11%|█         | 249/2332 [07:07<49:10,  1.42s/it] 11%|█         | 250/2332 [07:09<50:19,  1.45s/it] 11%|█         | 251/2332 [07:10<49:54,  1.44s/it] 11%|█         | 252/2332 [07:12<49:36,  1.43s/it] 11%|█         | 253/2332 [07:13<50:31,  1.46s/it] 11%|█         | 254/2332 [07:15<50:06,  1.45s/it] 11%|█         | 255/2332 [07:16<49:41,  1.44s/it] 11%|█         | 256/2332 [07:18<49:27,  1.43s/it] 11%|█         | 257/2332 [07:19<49:18,  1.43s/it] 11%|█         | 258/2332 [07:20<49:15,  1.43s/it] 11%|█         | 259/2332 [07:22<49:08,  1.42s/it] 11%|█         | 260/2332 [07:23<48:57,  1.42s/it] 11%|█         | 261/2332 [07:25<48:56,  1.42s/it] 11%|█         | 262/2332 [07:26<48:51,  1.42s/it] 11%|█▏        | 263/2332 [07:28<48:52,  1.42s/it] 11%|█▏        | 264/2332 [07:29<48:51,  1.42s/it] 11%|█▏        | 265/2332 [07:30<48:42,  1.41s/it] 11%|█▏        | 266/2332 [07:32<48:45,  1.42s/it] 11%|█▏        | 267/2332 [07:33<48:42,  1.42s/it] 11%|█▏        | 268/2332 [07:35<48:35,  1.41s/it] 12%|█▏        | 269/2332 [07:36<48:31,  1.41s/it] 12%|█▏        | 270/2332 [07:37<48:34,  1.41s/it] 12%|█▏        | 271/2332 [07:39<48:27,  1.41s/it] 12%|█▏        | 272/2332 [07:40<48:30,  1.41s/it] 12%|█▏        | 273/2332 [07:42<48:27,  1.41s/it] 12%|█▏        | 274/2332 [07:43<48:25,  1.41s/it] 12%|█▏        | 275/2332 [07:45<49:44,  1.45s/it] 12%|█▏        | 276/2332 [07:46<49:16,  1.44s/it] 12%|█▏        | 277/2332 [07:47<48:58,  1.43s/it] 12%|█▏        | 278/2332 [07:49<49:59,  1.46s/it] 12%|█▏        | 279/2332 [07:50<49:27,  1.45s/it] 12%|█▏        | 280/2332 [07:52<49:05,  1.44s/it] 12%|█▏        | 281/2332 [07:53<48:51,  1.43s/it] 12%|█▏        | 282/2332 [07:55<48:52,  1.43s/it] 12%|█▏        | 283/2332 [07:56<48:36,  1.42s/it] 12%|█▏        | 284/2332 [07:57<48:26,  1.42s/it] 12%|█▏        | 285/2332 [07:59<48:21,  1.42s/it] 12%|█▏        | 286/2332 [08:00<48:19,  1.42s/it] 12%|█▏        | 287/2332 [08:02<48:17,  1.42s/it] 12%|█▏        | 288/2332 [08:03<48:24,  1.42s/it] 12%|█▏        | 289/2332 [08:05<48:21,  1.42s/it] 12%|█▏        | 290/2332 [08:06<48:13,  1.42s/it] 12%|█▏        | 291/2332 [08:07<48:10,  1.42s/it] 13%|█▎        | 292/2332 [08:09<48:14,  1.42s/it] 13%|█▎        | 293/2332 [08:10<48:12,  1.42s/it] 13%|█▎        | 294/2332 [08:12<48:13,  1.42s/it] 13%|█▎        | 295/2332 [08:13<48:17,  1.42s/it] 13%|█▎        | 296/2332 [08:14<48:03,  1.42s/it] 13%|█▎        | 297/2332 [08:16<48:04,  1.42s/it] 13%|█▎        | 298/2332 [08:17<48:12,  1.42s/it] 13%|█▎        | 299/2332 [08:19<48:05,  1.42s/it] 13%|█▎        | 300/2332 [08:20<49:22,  1.46s/it][32m[2023-11-10 10:41:05,164] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens. If end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:41:05,490] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:41:05,491] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:41:05,491] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:41:05,491] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:41:05,491] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.92it/s][A
  8%|▊         | 3/36 [00:01<00:17,  1.88it/s][A
 11%|█         | 4/36 [00:02<00:18,  1.69it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.60it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.55it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.52it/s][A
 22%|██▏       | 8/36 [00:04<00:18,  1.50it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.48it/s][A
 28%|██▊       | 10/36 [00:06<00:17,  1.47it/s][A
 31%|███       | 11/36 [00:06<00:17,  1.47it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.47it/s][A
 36%|███▌      | 13/36 [00:08<00:15,  1.46it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.46it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.46it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.46it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.46it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.46it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.46it/s][A
 56%|█████▌    | 20/36 [00:13<00:10,  1.46it/s][A
 58%|█████▊    | 21/36 [00:13<00:10,  1.45it/s][A
 61%|██████    | 22/36 [00:14<00:09,  1.45it/s][A
 64%|██████▍   | 23/36 [00:15<00:08,  1.45it/s][A
 67%|██████▋   | 24/36 [00:15<00:08,  1.46it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.46it/s][A
 72%|███████▏  | 26/36 [00:17<00:06,  1.46it/s][A
 75%|███████▌  | 27/36 [00:17<00:06,  1.46it/s][A
 78%|███████▊  | 28/36 [00:18<00:05,  1.46it/s][A
 81%|████████  | 29/36 [00:19<00:05,  1.39it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.41it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.55it/s][A
 89%|████████▉ | 32/36 [00:20<00:02,  1.85it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.15it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.41it/s][A
 97%|█████████▋| 35/36 [00:21<00:00,  2.63it/s][A
100%|██████████| 36/36 [00:21<00:00,  3.20it/s][A                                                  
                                               [Aeval_anls: 57.47108374610918, epoch: 0.5146
 13%|█▎        | 300/2332 [08:46<49:22,  1.46s/it]
100%|██████████| 36/36 [00:22<00:00,  3.20it/s][A
                                               [A[32m[2023-11-10 10:41:30,742] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-300[0m
[32m[2023-11-10 10:41:30,751] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-300/config.json[0m
[32m[2023-11-10 10:41:33,278] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-300/model_state.pdparams[0m
[32m[2023-11-10 10:41:33,279] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-300/tokenizer_config.json[0m
[32m[2023-11-10 10:41:33,279] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-300/special_tokens_map.json[0m
[32m[2023-11-10 10:41:38,298] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-100] due to args.save_total_limit[0m
 13%|█▎        | 301/2332 [08:55<6:29:32, 11.51s/it] 13%|█▎        | 302/2332 [08:57<4:46:56,  8.48s/it] 13%|█▎        | 303/2332 [08:58<3:36:10,  6.39s/it] 13%|█▎        | 304/2332 [09:00<2:45:35,  4.90s/it] 13%|█▎        | 305/2332 [09:01<2:10:15,  3.86s/it] 13%|█▎        | 306/2332 [09:02<1:45:28,  3.12s/it] 13%|█▎        | 307/2332 [09:04<1:28:08,  2.61s/it] 13%|█▎        | 308/2332 [09:05<1:15:53,  2.25s/it] 13%|█▎        | 309/2332 [09:07<1:07:23,  2.00s/it] 13%|█▎        | 310/2332 [09:08<1:01:26,  1.82s/it] 13%|█▎        | 311/2332 [09:09<57:17,  1.70s/it]   13%|█▎        | 312/2332 [09:11<54:17,  1.61s/it] 13%|█▎        | 313/2332 [09:12<52:21,  1.56s/it] 13%|█▎        | 314/2332 [09:14<50:53,  1.51s/it] 14%|█▎        | 315/2332 [09:15<49:50,  1.48s/it] 14%|█▎        | 316/2332 [09:17<49:05,  1.46s/it] 14%|█▎        | 317/2332 [09:18<48:32,  1.45s/it] 14%|█▎        | 318/2332 [09:19<48:14,  1.44s/it] 14%|█▎        | 319/2332 [09:21<47:55,  1.43s/it] 14%|█▎        | 320/2332 [09:22<47:46,  1.42s/it] 14%|█▍        | 321/2332 [09:24<47:33,  1.42s/it] 14%|█▍        | 322/2332 [09:25<47:29,  1.42s/it] 14%|█▍        | 323/2332 [09:26<47:18,  1.41s/it] 14%|█▍        | 324/2332 [09:28<47:20,  1.41s/it] 14%|█▍        | 325/2332 [09:29<47:18,  1.41s/it] 14%|█▍        | 326/2332 [09:31<47:13,  1.41s/it] 14%|█▍        | 327/2332 [09:32<47:13,  1.41s/it] 14%|█▍        | 328/2332 [09:34<48:22,  1.45s/it] 14%|█▍        | 329/2332 [09:35<48:01,  1.44s/it] 14%|█▍        | 330/2332 [09:36<47:43,  1.43s/it] 14%|█▍        | 331/2332 [09:38<47:31,  1.43s/it] 14%|█▍        | 332/2332 [09:39<47:29,  1.42s/it] 14%|█▍        | 333/2332 [09:41<47:20,  1.42s/it] 14%|█▍        | 334/2332 [09:42<47:15,  1.42s/it] 14%|█▍        | 335/2332 [09:44<47:14,  1.42s/it] 14%|█▍        | 336/2332 [09:45<47:06,  1.42s/it] 14%|█▍        | 337/2332 [09:46<47:05,  1.42s/it] 14%|█▍        | 338/2332 [09:48<47:01,  1.41s/it] 15%|█▍        | 339/2332 [09:49<46:56,  1.41s/it] 15%|█▍        | 340/2332 [09:51<46:53,  1.41s/it] 15%|█▍        | 341/2332 [09:52<46:47,  1.41s/it] 15%|█▍        | 342/2332 [09:53<46:49,  1.41s/it] 15%|█▍        | 343/2332 [09:55<46:48,  1.41s/it] 15%|█▍        | 344/2332 [09:56<46:48,  1.41s/it] 15%|█▍        | 345/2332 [09:58<46:41,  1.41s/it] 15%|█▍        | 346/2332 [09:59<46:44,  1.41s/it] 15%|█▍        | 347/2332 [10:00<46:43,  1.41s/it] 15%|█▍        | 348/2332 [10:02<46:41,  1.41s/it] 15%|█▍        | 349/2332 [10:03<46:39,  1.41s/it] 15%|█▌        | 350/2332 [10:05<46:38,  1.41s/it] 15%|█▌        | 351/2332 [10:06<46:28,  1.41s/it] 15%|█▌        | 352/2332 [10:08<46:36,  1.41s/it] 15%|█▌        | 353/2332 [10:09<47:40,  1.45s/it] 15%|█▌        | 354/2332 [10:10<47:24,  1.44s/it] 15%|█▌        | 355/2332 [10:12<47:06,  1.43s/it] 15%|█▌        | 356/2332 [10:13<47:00,  1.43s/it] 15%|█▌        | 357/2332 [10:15<46:49,  1.42s/it] 15%|█▌        | 358/2332 [10:16<46:40,  1.42s/it] 15%|█▌        | 359/2332 [10:18<47:51,  1.46s/it] 15%|█▌        | 360/2332 [10:19<47:32,  1.45s/it] 15%|█▌        | 361/2332 [10:20<47:12,  1.44s/it] 16%|█▌        | 362/2332 [10:22<46:54,  1.43s/it] 16%|█▌        | 363/2332 [10:23<46:50,  1.43s/it] 16%|█▌        | 364/2332 [10:25<46:44,  1.43s/it] 16%|█▌        | 365/2332 [10:26<46:41,  1.42s/it] 16%|█▌        | 366/2332 [10:28<46:44,  1.43s/it] 16%|█▌        | 367/2332 [10:29<46:37,  1.42s/it] 16%|█▌        | 368/2332 [10:30<46:37,  1.42s/it] 16%|█▌        | 369/2332 [10:32<46:27,  1.42s/it] 16%|█▌        | 370/2332 [10:33<46:23,  1.42s/it] 16%|█▌        | 371/2332 [10:35<46:14,  1.41s/it] 16%|█▌        | 372/2332 [10:36<46:16,  1.42s/it] 16%|█▌        | 373/2332 [10:38<46:09,  1.41s/it] 16%|█▌        | 374/2332 [10:39<46:12,  1.42s/it] 16%|█▌        | 375/2332 [10:40<46:11,  1.42s/it] 16%|█▌        | 376/2332 [10:42<46:14,  1.42s/it] 16%|█▌        | 377/2332 [10:43<46:19,  1.42s/it] 16%|█▌        | 378/2332 [10:45<47:29,  1.46s/it] 16%|█▋        | 379/2332 [10:46<47:02,  1.45s/it] 16%|█▋        | 380/2332 [10:48<46:43,  1.44s/it] 16%|█▋        | 381/2332 [10:49<46:26,  1.43s/it] 16%|█▋        | 382/2332 [10:50<46:28,  1.43s/it] 16%|█▋        | 383/2332 [10:52<46:17,  1.42s/it] 16%|█▋        | 384/2332 [10:53<46:14,  1.42s/it] 17%|█▋        | 385/2332 [10:55<46:07,  1.42s/it] 17%|█▋        | 386/2332 [10:56<46:02,  1.42s/it] 17%|█▋        | 387/2332 [10:57<45:57,  1.42s/it] 17%|█▋        | 388/2332 [10:59<46:00,  1.42s/it] 17%|█▋        | 389/2332 [11:00<45:53,  1.42s/it] 17%|█▋        | 390/2332 [11:02<45:54,  1.42s/it] 17%|█▋        | 391/2332 [11:03<46:59,  1.45s/it] 17%|█▋        | 392/2332 [11:05<46:35,  1.44s/it] 17%|█▋        | 393/2332 [11:06<46:11,  1.43s/it] 17%|█▋        | 394/2332 [11:08<46:04,  1.43s/it] 17%|█▋        | 395/2332 [11:09<46:03,  1.43s/it] 17%|█▋        | 396/2332 [11:10<45:57,  1.42s/it] 17%|█▋        | 397/2332 [11:12<45:44,  1.42s/it] 17%|█▋        | 398/2332 [11:13<45:41,  1.42s/it] 17%|█▋        | 399/2332 [11:15<45:38,  1.42s/it] 17%|█▋        | 400/2332 [11:16<45:35,  1.42s/it][32m[2023-11-10 10:44:00,900] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens. If end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:44:01,348] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:44:01,348] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:44:01,349] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:44:01,349] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:44:01,349] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.90it/s][A
  8%|▊         | 3/36 [00:01<00:17,  1.85it/s][A
 11%|█         | 4/36 [00:02<00:19,  1.67it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.59it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.54it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.51it/s][A
 22%|██▏       | 8/36 [00:04<00:18,  1.50it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.48it/s][A
 28%|██▊       | 10/36 [00:06<00:17,  1.48it/s][A
 31%|███       | 11/36 [00:06<00:16,  1.47it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.47it/s][A
 36%|███▌      | 13/36 [00:08<00:15,  1.46it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.46it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.46it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.46it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.46it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.46it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.46it/s][A
 56%|█████▌    | 20/36 [00:13<00:10,  1.46it/s][A
 58%|█████▊    | 21/36 [00:13<00:10,  1.46it/s][A
 61%|██████    | 22/36 [00:14<00:09,  1.46it/s][A
 64%|██████▍   | 23/36 [00:15<00:08,  1.46it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.39it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.41it/s][A
 72%|███████▏  | 26/36 [00:17<00:07,  1.42it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.43it/s][A
 78%|███████▊  | 28/36 [00:18<00:05,  1.43it/s][A
 81%|████████  | 29/36 [00:19<00:04,  1.44it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.44it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.59it/s][A
 89%|████████▉ | 32/36 [00:20<00:02,  1.89it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.17it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.42it/s][A
 97%|█████████▋| 35/36 [00:21<00:00,  2.64it/s][A
100%|██████████| 36/36 [00:21<00:00,  3.21it/s][A                                                  
                                               [Aeval_anls: 61.42647254822453, epoch: 0.6861
 17%|█▋        | 400/2332 [11:42<45:35,  1.42s/it]
100%|██████████| 36/36 [00:22<00:00,  3.21it/s][A
                                               [A[32m[2023-11-10 10:44:26,509] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-400[0m
[32m[2023-11-10 10:44:26,518] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-400/config.json[0m
[32m[2023-11-10 10:44:29,075] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-400/model_state.pdparams[0m
[32m[2023-11-10 10:44:29,075] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-400/tokenizer_config.json[0m
[32m[2023-11-10 10:44:29,076] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-400/special_tokens_map.json[0m
[32m[2023-11-10 10:44:34,158] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-200] due to args.save_total_limit[0m
 17%|█▋        | 401/2332 [11:51<6:10:52, 11.52s/it] 17%|█▋        | 402/2332 [11:53<4:33:11,  8.49s/it] 17%|█▋        | 403/2332 [11:54<3:25:49,  6.40s/it] 17%|█▋        | 404/2332 [11:55<2:37:38,  4.91s/it] 17%|█▋        | 405/2332 [11:57<2:03:51,  3.86s/it] 17%|█▋        | 406/2332 [11:58<1:40:18,  3.12s/it] 17%|█▋        | 407/2332 [12:00<1:23:50,  2.61s/it] 17%|█▋        | 408/2332 [12:01<1:12:17,  2.25s/it] 18%|█▊        | 409/2332 [12:03<1:04:09,  2.00s/it] 18%|█▊        | 410/2332 [12:04<58:27,  1.82s/it]   18%|█▊        | 411/2332 [12:05<54:23,  1.70s/it] 18%|█▊        | 412/2332 [12:07<51:41,  1.62s/it] 18%|█▊        | 413/2332 [12:08<49:42,  1.55s/it] 18%|█▊        | 414/2332 [12:10<48:29,  1.52s/it] 18%|█▊        | 415/2332 [12:11<47:25,  1.48s/it] 18%|█▊        | 416/2332 [12:12<46:46,  1.46s/it] 18%|█▊        | 417/2332 [12:14<46:07,  1.45s/it] 18%|█▊        | 418/2332 [12:15<45:55,  1.44s/it] 18%|█▊        | 419/2332 [12:17<45:39,  1.43s/it] 18%|█▊        | 420/2332 [12:18<46:41,  1.47s/it] 18%|█▊        | 421/2332 [12:20<46:07,  1.45s/it] 18%|█▊        | 422/2332 [12:21<45:45,  1.44s/it] 18%|█▊        | 423/2332 [12:22<45:34,  1.43s/it] 18%|█▊        | 424/2332 [12:24<45:17,  1.42s/it] 18%|█▊        | 425/2332 [12:25<45:09,  1.42s/it] 18%|█▊        | 426/2332 [12:27<45:07,  1.42s/it] 18%|█▊        | 427/2332 [12:28<44:58,  1.42s/it] 18%|█▊        | 428/2332 [12:30<46:01,  1.45s/it] 18%|█▊        | 429/2332 [12:31<45:39,  1.44s/it] 18%|█▊        | 430/2332 [12:32<45:27,  1.43s/it] 18%|█▊        | 431/2332 [12:34<45:12,  1.43s/it] 19%|█▊        | 432/2332 [12:35<45:07,  1.42s/it] 19%|█▊        | 433/2332 [12:37<44:57,  1.42s/it] 19%|█▊        | 434/2332 [12:38<44:49,  1.42s/it] 19%|█▊        | 435/2332 [12:40<44:50,  1.42s/it] 19%|█▊        | 436/2332 [12:41<44:43,  1.42s/it] 19%|█▊        | 437/2332 [12:42<44:40,  1.41s/it] 19%|█▉        | 438/2332 [12:44<44:32,  1.41s/it] 19%|█▉        | 439/2332 [12:45<44:39,  1.42s/it] 19%|█▉        | 440/2332 [12:47<44:32,  1.41s/it] 19%|█▉        | 441/2332 [12:48<44:29,  1.41s/it] 19%|█▉        | 442/2332 [12:49<44:31,  1.41s/it] 19%|█▉        | 443/2332 [12:51<44:29,  1.41s/it] 19%|█▉        | 444/2332 [12:52<44:27,  1.41s/it] 19%|█▉        | 445/2332 [12:54<44:20,  1.41s/it] 19%|█▉        | 446/2332 [12:55<44:22,  1.41s/it] 19%|█▉        | 447/2332 [12:57<44:21,  1.41s/it] 19%|█▉        | 448/2332 [12:58<44:18,  1.41s/it] 19%|█▉        | 449/2332 [12:59<44:16,  1.41s/it] 19%|█▉        | 450/2332 [13:01<44:18,  1.41s/it] 19%|█▉        | 451/2332 [13:02<44:16,  1.41s/it] 19%|█▉        | 452/2332 [13:04<45:18,  1.45s/it] 19%|█▉        | 453/2332 [13:05<46:07,  1.47s/it] 19%|█▉        | 454/2332 [13:07<45:35,  1.46s/it] 20%|█▉        | 455/2332 [13:08<45:06,  1.44s/it] 20%|█▉        | 456/2332 [13:09<44:45,  1.43s/it] 20%|█▉        | 457/2332 [13:11<44:37,  1.43s/it] 20%|█▉        | 458/2332 [13:12<44:25,  1.42s/it] 20%|█▉        | 459/2332 [13:14<44:17,  1.42s/it] 20%|█▉        | 460/2332 [13:15<44:12,  1.42s/it] 20%|█▉        | 461/2332 [13:17<44:06,  1.41s/it] 20%|█▉        | 462/2332 [13:18<44:04,  1.41s/it] 20%|█▉        | 463/2332 [13:19<44:05,  1.42s/it] 20%|█▉        | 464/2332 [13:21<44:01,  1.41s/it] 20%|█▉        | 465/2332 [13:22<43:58,  1.41s/it] 20%|█▉        | 466/2332 [13:24<43:53,  1.41s/it] 20%|██        | 467/2332 [13:25<43:56,  1.41s/it] 20%|██        | 468/2332 [13:26<43:50,  1.41s/it] 20%|██        | 469/2332 [13:28<43:53,  1.41s/it] 20%|██        | 470/2332 [13:29<43:51,  1.41s/it] 20%|██        | 471/2332 [13:31<43:50,  1.41s/it] 20%|██        | 472/2332 [13:32<43:47,  1.41s/it] 20%|██        | 473/2332 [13:33<43:47,  1.41s/it] 20%|██        | 474/2332 [13:35<43:44,  1.41s/it] 20%|██        | 475/2332 [13:36<43:42,  1.41s/it] 20%|██        | 476/2332 [13:38<43:40,  1.41s/it] 20%|██        | 477/2332 [13:39<43:41,  1.41s/it] 20%|██        | 478/2332 [13:41<44:42,  1.45s/it] 21%|██        | 479/2332 [13:42<44:26,  1.44s/it] 21%|██        | 480/2332 [13:44<45:15,  1.47s/it] 21%|██        | 481/2332 [13:45<44:47,  1.45s/it] 21%|██        | 482/2332 [13:46<44:24,  1.44s/it] 21%|██        | 483/2332 [13:48<44:06,  1.43s/it] 21%|██        | 484/2332 [13:49<43:52,  1.42s/it] 21%|██        | 485/2332 [13:51<43:47,  1.42s/it] 21%|██        | 486/2332 [13:52<43:47,  1.42s/it] 21%|██        | 487/2332 [13:53<43:41,  1.42s/it] 21%|██        | 488/2332 [13:55<43:31,  1.42s/it] 21%|██        | 489/2332 [13:56<43:31,  1.42s/it] 21%|██        | 490/2332 [13:58<43:27,  1.42s/it] 21%|██        | 491/2332 [13:59<43:24,  1.41s/it] 21%|██        | 492/2332 [14:01<43:21,  1.41s/it] 21%|██        | 493/2332 [14:02<43:17,  1.41s/it] 21%|██        | 494/2332 [14:03<43:17,  1.41s/it] 21%|██        | 495/2332 [14:05<43:16,  1.41s/it] 21%|██▏       | 496/2332 [14:06<43:17,  1.41s/it] 21%|██▏       | 497/2332 [14:08<43:19,  1.42s/it] 21%|██▏       | 498/2332 [14:09<43:13,  1.41s/it] 21%|██▏       | 499/2332 [14:10<43:14,  1.42s/it] 21%|██▏       | 500/2332 [14:12<43:11,  1.41s/it]                                                  loss: 1.81646838, learning_rate: 1.653e-05, global_step: 500, interval_runtime: 852.4324, interval_samples_per_second: 14.077363210230533, interval_steps_per_second: 0.5865568004262722, epoch: 0.8576
 21%|██▏       | 500/2332 [14:12<43:11,  1.41s/it][32m[2023-11-10 10:46:56,818] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens. If end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:46:57,265] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:46:57,265] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:46:57,265] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:46:57,265] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:46:57,265] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.90it/s][A
  8%|▊         | 3/36 [00:01<00:17,  1.86it/s][A
 11%|█         | 4/36 [00:02<00:19,  1.68it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.59it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.54it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.51it/s][A
 22%|██▏       | 8/36 [00:04<00:18,  1.50it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.49it/s][A
 28%|██▊       | 10/36 [00:06<00:17,  1.48it/s][A
 31%|███       | 11/36 [00:06<00:16,  1.47it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.47it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.39it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.41it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.43it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.44it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.44it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.45it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.45it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.45it/s][A
 58%|█████▊    | 21/36 [00:13<00:10,  1.45it/s][A
 61%|██████    | 22/36 [00:14<00:09,  1.45it/s][A
 64%|██████▍   | 23/36 [00:15<00:08,  1.45it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.45it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.45it/s][A
 72%|███████▏  | 26/36 [00:17<00:06,  1.46it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.46it/s][A
 78%|███████▊  | 28/36 [00:18<00:05,  1.46it/s][A
 81%|████████  | 29/36 [00:19<00:04,  1.46it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.46it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.59it/s][A
 89%|████████▉ | 32/36 [00:20<00:02,  1.89it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.18it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.44it/s][A
 97%|█████████▋| 35/36 [00:21<00:00,  2.66it/s][A
100%|██████████| 36/36 [00:21<00:00,  3.22it/s][A                                                  
                                               [Aeval_anls: 59.44533856113562, epoch: 0.8576
 21%|██▏       | 500/2332 [14:38<43:11,  1.41s/it]
100%|██████████| 36/36 [00:22<00:00,  3.22it/s][A
                                               [A[32m[2023-11-10 10:47:22,430] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-500[0m
[32m[2023-11-10 10:47:22,439] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-500/config.json[0m
[32m[2023-11-10 10:47:24,968] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-500/model_state.pdparams[0m
[32m[2023-11-10 10:47:24,969] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-500/tokenizer_config.json[0m
[32m[2023-11-10 10:47:24,969] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-500/special_tokens_map.json[0m
[32m[2023-11-10 10:47:29,995] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-300] due to args.save_total_limit[0m
 21%|██▏       | 501/2332 [14:47<5:51:08, 11.51s/it] 22%|██▏       | 502/2332 [14:48<4:18:34,  8.48s/it] 22%|██▏       | 503/2332 [14:50<3:14:54,  6.39s/it] 22%|██▏       | 504/2332 [14:51<2:29:18,  4.90s/it] 22%|██▏       | 505/2332 [14:53<1:57:14,  3.85s/it] 22%|██▏       | 506/2332 [14:54<1:34:57,  3.12s/it] 22%|██▏       | 507/2332 [14:56<1:19:18,  2.61s/it] 22%|██▏       | 508/2332 [14:57<1:08:26,  2.25s/it] 22%|██▏       | 509/2332 [14:58<1:01:52,  2.04s/it] 22%|██▏       | 510/2332 [15:00<56:21,  1.86s/it]   22%|██▏       | 511/2332 [15:01<52:15,  1.72s/it] 22%|██▏       | 512/2332 [15:03<49:25,  1.63s/it] 22%|██▏       | 513/2332 [15:04<47:20,  1.56s/it] 22%|██▏       | 514/2332 [15:06<46:02,  1.52s/it] 22%|██▏       | 515/2332 [15:07<45:05,  1.49s/it] 22%|██▏       | 516/2332 [15:08<44:26,  1.47s/it] 22%|██▏       | 517/2332 [15:10<43:54,  1.45s/it] 22%|██▏       | 518/2332 [15:11<43:30,  1.44s/it] 22%|██▏       | 519/2332 [15:13<43:10,  1.43s/it] 22%|██▏       | 520/2332 [15:14<43:07,  1.43s/it] 22%|██▏       | 521/2332 [15:15<42:57,  1.42s/it] 22%|██▏       | 522/2332 [15:17<42:53,  1.42s/it] 22%|██▏       | 523/2332 [15:18<42:48,  1.42s/it] 22%|██▏       | 524/2332 [15:20<42:45,  1.42s/it] 23%|██▎       | 525/2332 [15:21<42:42,  1.42s/it] 23%|██▎       | 526/2332 [15:23<42:47,  1.42s/it] 23%|██▎       | 527/2332 [15:24<42:45,  1.42s/it] 23%|██▎       | 528/2332 [15:26<43:41,  1.45s/it] 23%|██▎       | 529/2332 [15:27<43:23,  1.44s/it] 23%|██▎       | 530/2332 [15:28<43:03,  1.43s/it] 23%|██▎       | 531/2332 [15:30<42:51,  1.43s/it] 23%|██▎       | 532/2332 [15:31<42:38,  1.42s/it] 23%|██▎       | 533/2332 [15:33<42:35,  1.42s/it] 23%|██▎       | 534/2332 [15:34<42:26,  1.42s/it] 23%|██▎       | 535/2332 [15:35<42:21,  1.41s/it] 23%|██▎       | 536/2332 [15:37<42:19,  1.41s/it] 23%|██▎       | 537/2332 [15:38<42:18,  1.41s/it] 23%|██▎       | 538/2332 [15:40<42:14,  1.41s/it] 23%|██▎       | 539/2332 [15:41<43:19,  1.45s/it] 23%|██▎       | 540/2332 [15:43<42:57,  1.44s/it] 23%|██▎       | 541/2332 [15:44<42:40,  1.43s/it] 23%|██▎       | 542/2332 [15:45<42:30,  1.42s/it] 23%|██▎       | 543/2332 [15:47<42:24,  1.42s/it] 23%|██▎       | 544/2332 [15:48<42:19,  1.42s/it] 23%|██▎       | 545/2332 [15:50<42:11,  1.42s/it] 23%|██▎       | 546/2332 [15:51<42:10,  1.42s/it] 23%|██▎       | 547/2332 [15:52<42:05,  1.41s/it] 23%|██▎       | 548/2332 [15:54<42:04,  1.42s/it] 24%|██▎       | 549/2332 [15:55<42:02,  1.41s/it] 24%|██▎       | 550/2332 [15:57<42:02,  1.42s/it] 24%|██▎       | 551/2332 [15:58<42:03,  1.42s/it] 24%|██▎       | 552/2332 [16:00<42:04,  1.42s/it] 24%|██▎       | 553/2332 [16:01<42:59,  1.45s/it] 24%|██▍       | 554/2332 [16:02<42:39,  1.44s/it] 24%|██▍       | 555/2332 [16:04<42:26,  1.43s/it] 24%|██▍       | 556/2332 [16:05<42:11,  1.43s/it] 24%|██▍       | 557/2332 [16:07<42:02,  1.42s/it] 24%|██▍       | 558/2332 [16:08<41:59,  1.42s/it] 24%|██▍       | 559/2332 [16:10<42:04,  1.42s/it] 24%|██▍       | 560/2332 [16:11<42:01,  1.42s/it] 24%|██▍       | 561/2332 [16:12<41:57,  1.42s/it] 24%|██▍       | 562/2332 [16:14<41:55,  1.42s/it] 24%|██▍       | 563/2332 [16:15<41:54,  1.42s/it] 24%|██▍       | 564/2332 [16:17<41:48,  1.42s/it] 24%|██▍       | 565/2332 [16:18<41:52,  1.42s/it] 24%|██▍       | 566/2332 [16:20<41:50,  1.42s/it] 24%|██▍       | 567/2332 [16:21<41:44,  1.42s/it] 24%|██▍       | 568/2332 [16:22<41:45,  1.42s/it] 24%|██▍       | 569/2332 [16:24<42:34,  1.45s/it] 24%|██▍       | 570/2332 [16:25<42:10,  1.44s/it] 24%|██▍       | 571/2332 [16:27<41:57,  1.43s/it] 25%|██▍       | 572/2332 [16:28<41:49,  1.43s/it] 25%|██▍       | 573/2332 [16:30<41:40,  1.42s/it] 25%|██▍       | 574/2332 [16:31<41:36,  1.42s/it] 25%|██▍       | 575/2332 [16:32<41:37,  1.42s/it] 25%|██▍       | 576/2332 [16:34<41:31,  1.42s/it] 25%|██▍       | 577/2332 [16:35<41:31,  1.42s/it] 25%|██▍       | 578/2332 [16:36<39:47,  1.36s/it] 25%|██▍       | 579/2332 [16:38<37:15,  1.28s/it] 25%|██▍       | 580/2332 [16:39<35:23,  1.21s/it] 25%|██▍       | 581/2332 [16:40<34:09,  1.17s/it] 25%|██▍       | 582/2332 [16:41<33:16,  1.14s/it] 25%|██▌       | 583/2332 [16:41<29:39,  1.02s/it] 25%|██▌       | 584/2332 [16:45<51:05,  1.75s/it] 25%|██▌       | 585/2332 [16:46<48:04,  1.65s/it] 25%|██▌       | 586/2332 [16:48<45:56,  1.58s/it] 25%|██▌       | 587/2332 [16:49<44:35,  1.53s/it] 25%|██▌       | 588/2332 [16:51<43:34,  1.50s/it] 25%|██▌       | 589/2332 [16:52<42:46,  1.47s/it] 25%|██▌       | 590/2332 [16:53<42:12,  1.45s/it] 25%|██▌       | 591/2332 [16:55<41:50,  1.44s/it] 25%|██▌       | 592/2332 [16:56<41:33,  1.43s/it] 25%|██▌       | 593/2332 [16:58<41:20,  1.43s/it] 25%|██▌       | 594/2332 [16:59<41:13,  1.42s/it] 26%|██▌       | 595/2332 [17:00<41:06,  1.42s/it] 26%|██▌       | 596/2332 [17:02<40:59,  1.42s/it] 26%|██▌       | 597/2332 [17:03<40:59,  1.42s/it] 26%|██▌       | 598/2332 [17:05<41:58,  1.45s/it] 26%|██▌       | 599/2332 [17:06<41:34,  1.44s/it] 26%|██▌       | 600/2332 [17:08<41:28,  1.44s/it][32m[2023-11-10 10:49:52,557] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens. If end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:49:52,923] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:49:52,924] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:49:52,924] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:49:52,924] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:49:52,924] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.90it/s][A
  8%|▊         | 3/36 [00:01<00:16,  2.05it/s][A
 11%|█         | 4/36 [00:02<00:17,  1.78it/s][A
 14%|█▍        | 5/36 [00:02<00:18,  1.66it/s][A
 17%|█▋        | 6/36 [00:03<00:20,  1.50it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.48it/s][A
 22%|██▏       | 8/36 [00:04<00:19,  1.47it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.46it/s][A
 28%|██▊       | 10/36 [00:06<00:17,  1.46it/s][A
 31%|███       | 11/36 [00:06<00:17,  1.46it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.46it/s][A
 36%|███▌      | 13/36 [00:08<00:15,  1.46it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.46it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.45it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.45it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.45it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.46it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.46it/s][A
 56%|█████▌    | 20/36 [00:13<00:10,  1.46it/s][A
 58%|█████▊    | 21/36 [00:13<00:10,  1.46it/s][A
 61%|██████    | 22/36 [00:14<00:10,  1.39it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.41it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.42it/s][A
 69%|██████▉   | 25/36 [00:16<00:08,  1.35it/s][A
 72%|███████▏  | 26/36 [00:17<00:07,  1.31it/s][A
 75%|███████▌  | 27/36 [00:18<00:07,  1.28it/s][A
 78%|███████▊  | 28/36 [00:19<00:06,  1.26it/s][A
 81%|████████  | 29/36 [00:20<00:05,  1.24it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.23it/s][A
 86%|████████▌ | 31/36 [00:21<00:03,  1.36it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.66it/s][A
 92%|█████████▏| 33/36 [00:22<00:01,  1.95it/s][A
 94%|█████████▍| 34/36 [00:22<00:00,  2.23it/s][A
 97%|█████████▋| 35/36 [00:22<00:00,  2.49it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.06it/s][A                                                  
                                               [Aeval_anls: 59.82550958325837, epoch: 1.0292
 26%|██▌       | 600/2332 [17:34<41:28,  1.44s/it]
100%|██████████| 36/36 [00:23<00:00,  3.06it/s][A
                                               [A[32m[2023-11-10 10:50:19,103] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-600[0m
[32m[2023-11-10 10:50:19,111] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-600/config.json[0m
[32m[2023-11-10 10:50:21,691] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-600/model_state.pdparams[0m
[32m[2023-11-10 10:50:21,691] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-600/tokenizer_config.json[0m
[32m[2023-11-10 10:50:21,692] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-600/special_tokens_map.json[0m
[32m[2023-11-10 10:50:26,815] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-500] due to args.save_total_limit[0m
 26%|██▌       | 601/2332 [17:44<5:41:44, 11.85s/it] 26%|██▌       | 602/2332 [17:45<4:11:21,  8.72s/it] 26%|██▌       | 603/2332 [17:47<3:08:11,  6.53s/it] 26%|██▌       | 604/2332 [17:48<2:23:51,  5.00s/it] 26%|██▌       | 605/2332 [17:50<1:53:46,  3.95s/it] 26%|██▌       | 606/2332 [17:51<1:31:43,  3.19s/it] 26%|██▌       | 607/2332 [17:52<1:16:17,  2.65s/it] 26%|██▌       | 608/2332 [17:54<1:05:35,  2.28s/it] 26%|██▌       | 609/2332 [17:55<58:01,  2.02s/it]   26%|██▌       | 610/2332 [17:57<52:47,  1.84s/it] 26%|██▌       | 611/2332 [17:58<49:01,  1.71s/it] 26%|██▌       | 612/2332 [17:59<46:28,  1.62s/it] 26%|██▋       | 613/2332 [18:01<44:40,  1.56s/it] 26%|██▋       | 614/2332 [18:02<43:25,  1.52s/it] 26%|██▋       | 615/2332 [18:04<42:30,  1.49s/it] 26%|██▋       | 616/2332 [18:05<41:51,  1.46s/it] 26%|██▋       | 617/2332 [18:07<41:23,  1.45s/it] 27%|██▋       | 618/2332 [18:08<41:02,  1.44s/it] 27%|██▋       | 619/2332 [18:09<40:48,  1.43s/it] 27%|██▋       | 620/2332 [18:11<41:42,  1.46s/it] 27%|██▋       | 621/2332 [18:12<41:15,  1.45s/it] 27%|██▋       | 622/2332 [18:14<40:52,  1.43s/it] 27%|██▋       | 623/2332 [18:15<40:45,  1.43s/it] 27%|██▋       | 624/2332 [18:17<40:34,  1.43s/it] 27%|██▋       | 625/2332 [18:18<40:22,  1.42s/it] 27%|██▋       | 626/2332 [18:19<40:16,  1.42s/it] 27%|██▋       | 627/2332 [18:21<40:17,  1.42s/it] 27%|██▋       | 628/2332 [18:22<40:18,  1.42s/it] 27%|██▋       | 629/2332 [18:24<40:14,  1.42s/it] 27%|██▋       | 630/2332 [18:25<41:10,  1.45s/it] 27%|██▋       | 631/2332 [18:27<40:49,  1.44s/it] 27%|██▋       | 632/2332 [18:28<40:36,  1.43s/it] 27%|██▋       | 633/2332 [18:29<40:21,  1.43s/it] 27%|██▋       | 634/2332 [18:31<40:16,  1.42s/it] 27%|██▋       | 635/2332 [18:32<40:11,  1.42s/it] 27%|██▋       | 636/2332 [18:34<40:08,  1.42s/it] 27%|██▋       | 637/2332 [18:35<40:02,  1.42s/it] 27%|██▋       | 638/2332 [18:36<40:01,  1.42s/it] 27%|██▋       | 639/2332 [18:38<39:57,  1.42s/it] 27%|██▋       | 640/2332 [18:39<39:53,  1.41s/it] 27%|██▋       | 641/2332 [18:41<39:49,  1.41s/it] 28%|██▊       | 642/2332 [18:42<39:50,  1.41s/it] 28%|██▊       | 643/2332 [18:44<39:45,  1.41s/it] 28%|██▊       | 644/2332 [18:45<39:44,  1.41s/it] 28%|██▊       | 645/2332 [18:46<39:43,  1.41s/it] 28%|██▊       | 646/2332 [18:48<39:42,  1.41s/it] 28%|██▊       | 647/2332 [18:49<39:39,  1.41s/it] 28%|██▊       | 648/2332 [18:51<39:38,  1.41s/it] 28%|██▊       | 649/2332 [18:52<39:36,  1.41s/it] 28%|██▊       | 650/2332 [18:54<40:30,  1.45s/it] 28%|██▊       | 651/2332 [18:55<40:15,  1.44s/it] 28%|██▊       | 652/2332 [18:56<40:02,  1.43s/it] 28%|██▊       | 653/2332 [18:58<39:50,  1.42s/it] 28%|██▊       | 654/2332 [18:59<39:47,  1.42s/it] 28%|██▊       | 655/2332 [19:01<39:43,  1.42s/it] 28%|██▊       | 656/2332 [19:02<40:32,  1.45s/it] 28%|██▊       | 657/2332 [19:04<40:17,  1.44s/it] 28%|██▊       | 658/2332 [19:05<40:08,  1.44s/it] 28%|██▊       | 659/2332 [19:06<39:52,  1.43s/it] 28%|██▊       | 660/2332 [19:08<39:44,  1.43s/it] 28%|██▊       | 661/2332 [19:09<39:37,  1.42s/it] 28%|██▊       | 662/2332 [19:11<39:31,  1.42s/it] 28%|██▊       | 663/2332 [19:12<40:51,  1.47s/it] 28%|██▊       | 664/2332 [19:14<41:31,  1.49s/it] 29%|██▊       | 665/2332 [19:15<41:52,  1.51s/it] 29%|██▊       | 666/2332 [19:17<42:07,  1.52s/it] 29%|██▊       | 667/2332 [19:18<42:27,  1.53s/it] 29%|██▊       | 668/2332 [19:20<42:31,  1.53s/it] 29%|██▊       | 669/2332 [19:21<42:43,  1.54s/it] 29%|██▊       | 670/2332 [19:23<42:43,  1.54s/it] 29%|██▉       | 671/2332 [19:25<42:42,  1.54s/it] 29%|██▉       | 672/2332 [19:26<42:43,  1.54s/it] 29%|██▉       | 673/2332 [19:28<42:44,  1.55s/it] 29%|██▉       | 674/2332 [19:29<42:48,  1.55s/it] 29%|██▉       | 675/2332 [19:31<42:49,  1.55s/it] 29%|██▉       | 676/2332 [19:32<42:47,  1.55s/it] 29%|██▉       | 677/2332 [19:34<42:46,  1.55s/it] 29%|██▉       | 678/2332 [19:35<42:45,  1.55s/it] 29%|██▉       | 679/2332 [19:37<42:44,  1.55s/it] 29%|██▉       | 680/2332 [19:39<43:46,  1.59s/it] 29%|██▉       | 681/2332 [19:40<43:25,  1.58s/it] 29%|██▉       | 682/2332 [19:42<43:08,  1.57s/it] 29%|██▉       | 683/2332 [19:43<42:59,  1.56s/it] 29%|██▉       | 684/2332 [19:45<42:53,  1.56s/it] 29%|██▉       | 685/2332 [19:46<42:44,  1.56s/it] 29%|██▉       | 686/2332 [19:48<42:42,  1.56s/it] 29%|██▉       | 687/2332 [19:50<42:34,  1.55s/it] 30%|██▉       | 688/2332 [19:51<42:33,  1.55s/it] 30%|██▉       | 689/2332 [19:53<42:21,  1.55s/it] 30%|██▉       | 690/2332 [19:54<42:24,  1.55s/it] 30%|██▉       | 691/2332 [19:56<42:22,  1.55s/it] 30%|██▉       | 692/2332 [19:57<42:27,  1.55s/it] 30%|██▉       | 693/2332 [19:59<42:21,  1.55s/it] 30%|██▉       | 694/2332 [20:00<42:22,  1.55s/it] 30%|██▉       | 695/2332 [20:02<42:21,  1.55s/it] 30%|██▉       | 696/2332 [20:03<42:17,  1.55s/it] 30%|██▉       | 697/2332 [20:05<42:18,  1.55s/it] 30%|██▉       | 698/2332 [20:07<42:13,  1.55s/it] 30%|██▉       | 699/2332 [20:08<42:14,  1.55s/it] 30%|███       | 700/2332 [20:10<42:13,  1.55s/it][32m[2023-11-10 10:52:54,587] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens. If end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:52:55,042] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:52:55,043] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:52:55,043] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:52:55,043] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:52:55,043] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.89it/s][A
  8%|▊         | 3/36 [00:01<00:16,  2.05it/s][A
 11%|█         | 4/36 [00:02<00:18,  1.76it/s][A
 14%|█▍        | 5/36 [00:02<00:18,  1.64it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.58it/s][A
 19%|█▉        | 7/36 [00:04<00:18,  1.54it/s][A
 22%|██▏       | 8/36 [00:04<00:19,  1.43it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.44it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.37it/s][A
 31%|███       | 11/36 [00:07<00:17,  1.40it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.41it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.43it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.44it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.44it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.44it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.44it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.44it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.44it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.44it/s][A
 58%|█████▊    | 21/36 [00:14<00:10,  1.45it/s][A
 61%|██████    | 22/36 [00:14<00:09,  1.45it/s][A
 64%|██████▍   | 23/36 [00:15<00:08,  1.45it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.45it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.45it/s][A
 72%|███████▏  | 26/36 [00:17<00:06,  1.44it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.44it/s][A
 78%|███████▊  | 28/36 [00:18<00:05,  1.45it/s][A
 81%|████████  | 29/36 [00:19<00:04,  1.44it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.45it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.59it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.89it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.17it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.43it/s][A
 97%|█████████▋| 35/36 [00:21<00:00,  2.64it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.21it/s][A                                                  
                                               [Aeval_anls: 61.025839962940545, epoch: 1.2007
 30%|███       | 700/2332 [20:35<42:13,  1.55s/it]
100%|██████████| 36/36 [00:22<00:00,  3.21it/s][A
                                               [A[32m[2023-11-10 10:53:20,303] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-700[0m
[32m[2023-11-10 10:53:20,311] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-700/config.json[0m
[32m[2023-11-10 10:53:22,894] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-700/model_state.pdparams[0m
[32m[2023-11-10 10:53:22,895] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-700/tokenizer_config.json[0m
[32m[2023-11-10 10:53:22,895] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-700/special_tokens_map.json[0m
[32m[2023-11-10 10:53:28,016] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-600] due to args.save_total_limit[0m
 30%|███       | 701/2332 [20:45<5:17:11, 11.67s/it] 30%|███       | 702/2332 [20:46<3:53:24,  8.59s/it] 30%|███       | 703/2332 [20:48<2:54:49,  6.44s/it] 30%|███       | 704/2332 [20:49<2:14:47,  4.97s/it] 30%|███       | 705/2332 [20:51<1:45:44,  3.90s/it] 30%|███       | 706/2332 [20:52<1:25:23,  3.15s/it] 30%|███       | 707/2332 [20:54<1:11:15,  2.63s/it] 30%|███       | 708/2332 [20:55<1:02:11,  2.30s/it] 30%|███       | 709/2332 [20:56<54:58,  2.03s/it]   30%|███       | 710/2332 [20:58<49:51,  1.84s/it] 30%|███       | 711/2332 [20:59<46:22,  1.72s/it] 31%|███       | 712/2332 [21:01<43:57,  1.63s/it] 31%|███       | 713/2332 [21:02<42:15,  1.57s/it] 31%|███       | 714/2332 [21:04<40:57,  1.52s/it] 31%|███       | 715/2332 [21:05<40:02,  1.49s/it] 31%|███       | 716/2332 [21:06<39:25,  1.46s/it] 31%|███       | 717/2332 [21:08<39:02,  1.45s/it] 31%|███       | 718/2332 [21:09<38:47,  1.44s/it] 31%|███       | 719/2332 [21:11<38:34,  1.43s/it] 31%|███       | 720/2332 [21:12<38:23,  1.43s/it] 31%|███       | 721/2332 [21:13<38:10,  1.42s/it] 31%|███       | 722/2332 [21:15<38:06,  1.42s/it] 31%|███       | 723/2332 [21:16<38:04,  1.42s/it] 31%|███       | 724/2332 [21:18<38:03,  1.42s/it] 31%|███       | 725/2332 [21:19<37:59,  1.42s/it] 31%|███       | 726/2332 [21:21<37:54,  1.42s/it] 31%|███       | 727/2332 [21:22<37:53,  1.42s/it] 31%|███       | 728/2332 [21:23<37:53,  1.42s/it] 31%|███▏      | 729/2332 [21:25<37:54,  1.42s/it] 31%|███▏      | 730/2332 [21:26<37:59,  1.42s/it] 31%|███▏      | 731/2332 [21:28<37:58,  1.42s/it] 31%|███▏      | 732/2332 [21:29<37:56,  1.42s/it] 31%|███▏      | 733/2332 [21:31<38:46,  1.46s/it] 31%|███▏      | 734/2332 [21:32<39:24,  1.48s/it] 32%|███▏      | 735/2332 [21:34<38:51,  1.46s/it] 32%|███▏      | 736/2332 [21:35<38:26,  1.45s/it] 32%|███▏      | 737/2332 [21:36<38:09,  1.44s/it] 32%|███▏      | 738/2332 [21:38<37:58,  1.43s/it] 32%|███▏      | 739/2332 [21:39<37:47,  1.42s/it] 32%|███▏      | 740/2332 [21:41<37:41,  1.42s/it] 32%|███▏      | 741/2332 [21:42<37:36,  1.42s/it] 32%|███▏      | 742/2332 [21:43<37:38,  1.42s/it] 32%|███▏      | 743/2332 [21:45<37:34,  1.42s/it] 32%|███▏      | 744/2332 [21:46<37:30,  1.42s/it] 32%|███▏      | 745/2332 [21:48<37:26,  1.42s/it] 32%|███▏      | 746/2332 [21:49<37:20,  1.41s/it] 32%|███▏      | 747/2332 [21:51<37:21,  1.41s/it] 32%|███▏      | 748/2332 [21:52<37:17,  1.41s/it] 32%|███▏      | 749/2332 [21:53<37:17,  1.41s/it] 32%|███▏      | 750/2332 [21:55<37:12,  1.41s/it] 32%|███▏      | 751/2332 [21:56<37:10,  1.41s/it] 32%|███▏      | 752/2332 [21:58<37:15,  1.42s/it] 32%|███▏      | 753/2332 [21:59<37:17,  1.42s/it] 32%|███▏      | 754/2332 [22:00<37:12,  1.42s/it] 32%|███▏      | 755/2332 [22:02<37:13,  1.42s/it] 32%|███▏      | 756/2332 [22:03<37:09,  1.41s/it] 32%|███▏      | 757/2332 [22:05<37:05,  1.41s/it] 33%|███▎      | 758/2332 [22:06<38:01,  1.45s/it] 33%|███▎      | 759/2332 [22:08<37:50,  1.44s/it] 33%|███▎      | 760/2332 [22:09<37:33,  1.43s/it] 33%|███▎      | 761/2332 [22:10<37:24,  1.43s/it] 33%|███▎      | 762/2332 [22:12<37:14,  1.42s/it] 33%|███▎      | 763/2332 [22:13<37:11,  1.42s/it] 33%|███▎      | 764/2332 [22:15<37:57,  1.45s/it] 33%|███▎      | 765/2332 [22:16<37:41,  1.44s/it] 33%|███▎      | 766/2332 [22:18<37:29,  1.44s/it] 33%|███▎      | 767/2332 [22:19<37:20,  1.43s/it] 33%|███▎      | 768/2332 [22:21<37:12,  1.43s/it] 33%|███▎      | 769/2332 [22:22<37:02,  1.42s/it] 33%|███▎      | 770/2332 [22:23<36:59,  1.42s/it] 33%|███▎      | 771/2332 [22:25<36:54,  1.42s/it] 33%|███▎      | 772/2332 [22:26<36:49,  1.42s/it] 33%|███▎      | 773/2332 [22:28<36:45,  1.41s/it] 33%|███▎      | 774/2332 [22:29<36:42,  1.41s/it] 33%|███▎      | 775/2332 [22:30<36:41,  1.41s/it] 33%|███▎      | 776/2332 [22:32<36:42,  1.42s/it] 33%|███▎      | 777/2332 [22:33<36:47,  1.42s/it] 33%|███▎      | 778/2332 [22:35<36:43,  1.42s/it] 33%|███▎      | 779/2332 [22:36<36:38,  1.42s/it] 33%|███▎      | 780/2332 [22:37<36:34,  1.41s/it] 33%|███▎      | 781/2332 [22:39<36:28,  1.41s/it] 34%|███▎      | 782/2332 [22:40<36:31,  1.41s/it] 34%|███▎      | 783/2332 [22:42<37:27,  1.45s/it] 34%|███▎      | 784/2332 [22:43<37:11,  1.44s/it] 34%|███▎      | 785/2332 [22:45<37:02,  1.44s/it] 34%|███▎      | 786/2332 [22:46<36:52,  1.43s/it] 34%|███▎      | 787/2332 [22:48<36:42,  1.43s/it] 34%|███▍      | 788/2332 [22:49<36:38,  1.42s/it] 34%|███▍      | 789/2332 [22:50<36:33,  1.42s/it] 34%|███▍      | 790/2332 [22:52<36:29,  1.42s/it] 34%|███▍      | 791/2332 [22:53<36:31,  1.42s/it] 34%|███▍      | 792/2332 [22:55<36:25,  1.42s/it] 34%|███▍      | 793/2332 [22:56<36:20,  1.42s/it] 34%|███▍      | 794/2332 [22:58<37:10,  1.45s/it] 34%|███▍      | 795/2332 [22:59<36:51,  1.44s/it] 34%|███▍      | 796/2332 [23:00<36:35,  1.43s/it] 34%|███▍      | 797/2332 [23:02<36:28,  1.43s/it] 34%|███▍      | 798/2332 [23:03<36:21,  1.42s/it] 34%|███▍      | 799/2332 [23:05<36:15,  1.42s/it] 34%|███▍      | 800/2332 [23:06<36:11,  1.42s/it][32m[2023-11-10 10:55:50,908] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens. If end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:55:51,400] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:55:51,400] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:55:51,400] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:55:51,400] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:55:51,400] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.92it/s][A
  8%|▊         | 3/36 [00:01<00:16,  2.03it/s][A
 11%|█         | 4/36 [00:02<00:18,  1.77it/s][A
 14%|█▍        | 5/36 [00:02<00:18,  1.65it/s][A
 17%|█▋        | 6/36 [00:03<00:18,  1.58it/s][A
 19%|█▉        | 7/36 [00:04<00:18,  1.53it/s][A
 22%|██▏       | 8/36 [00:04<00:18,  1.50it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.49it/s][A
 28%|██▊       | 10/36 [00:06<00:17,  1.48it/s][A
 31%|███       | 11/36 [00:06<00:16,  1.47it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.47it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.40it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.41it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.42it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.43it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.44it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.45it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.44it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.45it/s][A
 58%|█████▊    | 21/36 [00:13<00:10,  1.44it/s][A
 61%|██████    | 22/36 [00:14<00:10,  1.37it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.40it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.42it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.43it/s][A
 72%|███████▏  | 26/36 [00:17<00:06,  1.44it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.44it/s][A
 78%|███████▊  | 28/36 [00:18<00:05,  1.44it/s][A
 81%|████████  | 29/36 [00:19<00:04,  1.45it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.45it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.59it/s][A
 89%|████████▉ | 32/36 [00:20<00:02,  1.89it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.18it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.43it/s][A
 97%|█████████▋| 35/36 [00:21<00:00,  2.65it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.22it/s][A                                                  
                                               [Aeval_anls: 60.602485126934226, epoch: 1.3722
 34%|███▍      | 800/2332 [23:32<36:11,  1.42s/it]
100%|██████████| 36/36 [00:22<00:00,  3.22it/s][A
                                               [A[32m[2023-11-10 10:56:16,599] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-800[0m
[32m[2023-11-10 10:56:16,608] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-800/config.json[0m
[32m[2023-11-10 10:56:19,192] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-800/model_state.pdparams[0m
[32m[2023-11-10 10:56:19,192] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-800/tokenizer_config.json[0m
[32m[2023-11-10 10:56:19,192] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-800/special_tokens_map.json[0m
[32m[2023-11-10 10:56:24,313] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-700] due to args.save_total_limit[0m
 34%|███▍      | 801/2332 [23:41<4:55:13, 11.57s/it] 34%|███▍      | 802/2332 [23:43<3:37:18,  8.52s/it] 34%|███▍      | 803/2332 [23:44<2:42:51,  6.39s/it] 34%|███▍      | 804/2332 [23:46<2:04:38,  4.89s/it] 35%|███▍      | 805/2332 [23:47<1:37:57,  3.85s/it] 35%|███▍      | 806/2332 [23:48<1:19:21,  3.12s/it] 35%|███▍      | 807/2332 [23:50<1:06:18,  2.61s/it] 35%|███▍      | 808/2332 [23:51<57:10,  2.25s/it]   35%|███▍      | 809/2332 [23:53<50:47,  2.00s/it] 35%|███▍      | 810/2332 [23:54<46:18,  1.83s/it] 35%|███▍      | 811/2332 [23:55<43:12,  1.70s/it] 35%|███▍      | 812/2332 [23:57<40:57,  1.62s/it] 35%|███▍      | 813/2332 [23:58<40:19,  1.59s/it] 35%|███▍      | 814/2332 [24:00<38:57,  1.54s/it] 35%|███▍      | 815/2332 [24:01<37:59,  1.50s/it] 35%|███▍      | 816/2332 [24:03<37:15,  1.47s/it] 35%|███▌      | 817/2332 [24:04<36:48,  1.46s/it] 35%|███▌      | 818/2332 [24:05<36:23,  1.44s/it] 35%|███▌      | 819/2332 [24:07<36:08,  1.43s/it] 35%|███▌      | 820/2332 [24:08<36:49,  1.46s/it] 35%|███▌      | 821/2332 [24:10<36:30,  1.45s/it] 35%|███▌      | 822/2332 [24:11<36:08,  1.44s/it] 35%|███▌      | 823/2332 [24:13<35:59,  1.43s/it] 35%|███▌      | 824/2332 [24:14<35:48,  1.42s/it] 35%|███▌      | 825/2332 [24:15<35:43,  1.42s/it] 35%|███▌      | 826/2332 [24:17<35:34,  1.42s/it] 35%|███▌      | 827/2332 [24:18<35:30,  1.42s/it] 36%|███▌      | 828/2332 [24:20<35:28,  1.42s/it] 36%|███▌      | 829/2332 [24:21<35:24,  1.41s/it] 36%|███▌      | 830/2332 [24:22<35:18,  1.41s/it] 36%|███▌      | 831/2332 [24:24<35:19,  1.41s/it] 36%|███▌      | 832/2332 [24:25<35:18,  1.41s/it] 36%|███▌      | 833/2332 [24:27<35:16,  1.41s/it] 36%|███▌      | 834/2332 [24:28<35:17,  1.41s/it] 36%|███▌      | 835/2332 [24:30<35:13,  1.41s/it] 36%|███▌      | 836/2332 [24:31<35:13,  1.41s/it] 36%|███▌      | 837/2332 [24:32<35:15,  1.41s/it] 36%|███▌      | 838/2332 [24:34<35:16,  1.42s/it] 36%|███▌      | 839/2332 [24:35<35:10,  1.41s/it] 36%|███▌      | 840/2332 [24:37<35:09,  1.41s/it] 36%|███▌      | 841/2332 [24:38<35:05,  1.41s/it] 36%|███▌      | 842/2332 [24:39<35:06,  1.41s/it] 36%|███▌      | 843/2332 [24:41<35:06,  1.41s/it] 36%|███▌      | 844/2332 [24:42<35:13,  1.42s/it] 36%|███▌      | 845/2332 [24:44<35:57,  1.45s/it] 36%|███▋      | 846/2332 [24:45<35:43,  1.44s/it] 36%|███▋      | 847/2332 [24:47<35:33,  1.44s/it] 36%|███▋      | 848/2332 [24:48<35:24,  1.43s/it] 36%|███▋      | 849/2332 [24:50<35:10,  1.42s/it] 36%|███▋      | 850/2332 [24:51<35:50,  1.45s/it] 36%|███▋      | 851/2332 [24:52<35:33,  1.44s/it] 37%|███▋      | 852/2332 [24:54<35:21,  1.43s/it] 37%|███▋      | 853/2332 [24:55<35:10,  1.43s/it] 37%|███▋      | 854/2332 [24:57<35:02,  1.42s/it] 37%|███▋      | 855/2332 [24:58<34:54,  1.42s/it] 37%|███▋      | 856/2332 [24:59<34:48,  1.41s/it] 37%|███▋      | 857/2332 [25:01<34:48,  1.42s/it] 37%|███▋      | 858/2332 [25:02<34:49,  1.42s/it] 37%|███▋      | 859/2332 [25:04<34:50,  1.42s/it] 37%|███▋      | 860/2332 [25:05<34:47,  1.42s/it] 37%|███▋      | 861/2332 [25:07<34:41,  1.41s/it] 37%|███▋      | 862/2332 [25:08<34:42,  1.42s/it] 37%|███▋      | 863/2332 [25:09<34:45,  1.42s/it] 37%|███▋      | 864/2332 [25:11<34:48,  1.42s/it] 37%|███▋      | 865/2332 [25:12<35:07,  1.44s/it] 37%|███▋      | 866/2332 [25:14<34:57,  1.43s/it] 37%|███▋      | 867/2332 [25:15<34:45,  1.42s/it] 37%|███▋      | 868/2332 [25:17<34:40,  1.42s/it] 37%|███▋      | 869/2332 [25:18<34:35,  1.42s/it] 37%|███▋      | 870/2332 [25:19<34:31,  1.42s/it] 37%|███▋      | 871/2332 [25:21<34:28,  1.42s/it] 37%|███▋      | 872/2332 [25:22<34:24,  1.41s/it] 37%|███▋      | 873/2332 [25:24<35:12,  1.45s/it] 37%|███▋      | 874/2332 [25:25<34:59,  1.44s/it] 38%|███▊      | 875/2332 [25:27<34:50,  1.43s/it] 38%|███▊      | 876/2332 [25:28<34:43,  1.43s/it] 38%|███▊      | 877/2332 [25:29<34:34,  1.43s/it] 38%|███▊      | 878/2332 [25:31<34:32,  1.43s/it] 38%|███▊      | 879/2332 [25:32<34:27,  1.42s/it] 38%|███▊      | 880/2332 [25:34<35:12,  1.45s/it] 38%|███▊      | 881/2332 [25:35<35:00,  1.45s/it] 38%|███▊      | 882/2332 [25:37<34:45,  1.44s/it] 38%|███▊      | 883/2332 [25:38<34:33,  1.43s/it] 38%|███▊      | 884/2332 [25:39<34:26,  1.43s/it] 38%|███▊      | 885/2332 [25:41<34:22,  1.43s/it] 38%|███▊      | 886/2332 [25:42<34:14,  1.42s/it] 38%|███▊      | 887/2332 [25:44<34:15,  1.42s/it] 38%|███▊      | 888/2332 [25:45<34:14,  1.42s/it] 38%|███▊      | 889/2332 [25:47<34:07,  1.42s/it] 38%|███▊      | 890/2332 [25:48<34:01,  1.42s/it] 38%|███▊      | 891/2332 [25:49<33:57,  1.41s/it] 38%|███▊      | 892/2332 [25:51<33:57,  1.41s/it] 38%|███▊      | 893/2332 [25:52<34:00,  1.42s/it] 38%|███▊      | 894/2332 [25:54<33:57,  1.42s/it] 38%|███▊      | 895/2332 [25:55<33:52,  1.41s/it] 38%|███▊      | 896/2332 [25:56<33:54,  1.42s/it] 38%|███▊      | 897/2332 [25:58<33:49,  1.41s/it] 39%|███▊      | 898/2332 [25:59<33:49,  1.42s/it] 39%|███▊      | 899/2332 [26:01<33:52,  1.42s/it] 39%|███▊      | 900/2332 [26:02<33:51,  1.42s/it][32m[2023-11-10 10:58:47,030] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens. If end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 10:58:47,488] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 10:58:47,488] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 10:58:47,488] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 10:58:47,488] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 10:58:47,488] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.89it/s][A
  8%|▊         | 3/36 [00:01<00:16,  2.05it/s][A
 11%|█         | 4/36 [00:02<00:17,  1.78it/s][A
 14%|█▍        | 5/36 [00:02<00:18,  1.64it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.57it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.45it/s][A
 22%|██▏       | 8/36 [00:04<00:19,  1.45it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.45it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.38it/s][A
 31%|███       | 11/36 [00:07<00:17,  1.40it/s][A
 33%|███▎      | 12/36 [00:07<00:17,  1.41it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.43it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.44it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.44it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.44it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.44it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.44it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.44it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.45it/s][A
 58%|█████▊    | 21/36 [00:14<00:10,  1.45it/s][A
 61%|██████    | 22/36 [00:14<00:09,  1.45it/s][A
 64%|██████▍   | 23/36 [00:15<00:08,  1.46it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.45it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.46it/s][A
 72%|███████▏  | 26/36 [00:17<00:06,  1.46it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.46it/s][A
 78%|███████▊  | 28/36 [00:18<00:05,  1.45it/s][A
 81%|████████  | 29/36 [00:19<00:04,  1.45it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.44it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.58it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.89it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.17it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.42it/s][A
 97%|█████████▋| 35/36 [00:21<00:00,  2.64it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.21it/s][A                                                  
                                               [Aeval_anls: 61.242689550487576, epoch: 1.5437
 39%|███▊      | 900/2332 [26:28<33:51,  1.42s/it]
100%|██████████| 36/36 [00:22<00:00,  3.21it/s][A
                                               [A[32m[2023-11-10 10:59:12,753] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-900[0m
[32m[2023-11-10 10:59:12,768] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-900/config.json[0m
[32m[2023-11-10 10:59:15,327] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-900/model_state.pdparams[0m
[32m[2023-11-10 10:59:15,327] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-900/tokenizer_config.json[0m
[32m[2023-11-10 10:59:15,327] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-900/special_tokens_map.json[0m
[32m[2023-11-10 10:59:20,377] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-800] due to args.save_total_limit[0m
 39%|███▊      | 901/2332 [26:37<4:35:29, 11.55s/it] 39%|███▊      | 902/2332 [26:39<3:22:50,  8.51s/it] 39%|███▊      | 903/2332 [26:40<2:32:01,  6.38s/it] 39%|███▉      | 904/2332 [26:42<1:57:14,  4.93s/it] 39%|███▉      | 905/2332 [26:43<1:32:08,  3.87s/it] 39%|███▉      | 906/2332 [26:45<1:14:36,  3.14s/it] 39%|███▉      | 907/2332 [26:46<1:02:15,  2.62s/it] 39%|███▉      | 908/2332 [26:47<53:37,  2.26s/it]   39%|███▉      | 909/2332 [26:49<47:36,  2.01s/it] 39%|███▉      | 910/2332 [26:50<43:25,  1.83s/it] 39%|███▉      | 911/2332 [26:52<40:23,  1.71s/it] 39%|███▉      | 912/2332 [26:53<38:18,  1.62s/it] 39%|███▉      | 913/2332 [26:54<36:49,  1.56s/it] 39%|███▉      | 914/2332 [26:56<35:46,  1.51s/it] 39%|███▉      | 915/2332 [26:57<35:01,  1.48s/it] 39%|███▉      | 916/2332 [26:59<34:30,  1.46s/it] 39%|███▉      | 917/2332 [27:00<34:10,  1.45s/it] 39%|███▉      | 918/2332 [27:02<33:51,  1.44s/it] 39%|███▉      | 919/2332 [27:03<33:43,  1.43s/it] 39%|███▉      | 920/2332 [27:04<33:31,  1.42s/it] 39%|███▉      | 921/2332 [27:06<33:26,  1.42s/it] 40%|███▉      | 922/2332 [27:07<33:22,  1.42s/it] 40%|███▉      | 923/2332 [27:09<33:18,  1.42s/it] 40%|███▉      | 924/2332 [27:10<33:22,  1.42s/it] 40%|███▉      | 925/2332 [27:11<33:15,  1.42s/it] 40%|███▉      | 926/2332 [27:13<33:13,  1.42s/it] 40%|███▉      | 927/2332 [27:14<33:10,  1.42s/it] 40%|███▉      | 928/2332 [27:16<33:13,  1.42s/it] 40%|███▉      | 929/2332 [27:17<33:07,  1.42s/it] 40%|███▉      | 930/2332 [27:19<33:09,  1.42s/it] 40%|███▉      | 931/2332 [27:20<33:10,  1.42s/it] 40%|███▉      | 932/2332 [27:21<33:02,  1.42s/it] 40%|████      | 933/2332 [27:23<32:59,  1.42s/it] 40%|████      | 934/2332 [27:24<33:47,  1.45s/it] 40%|████      | 935/2332 [27:26<33:35,  1.44s/it] 40%|████      | 936/2332 [27:27<33:25,  1.44s/it] 40%|████      | 937/2332 [27:29<33:14,  1.43s/it] 40%|████      | 938/2332 [27:30<33:07,  1.43s/it] 40%|████      | 939/2332 [27:31<32:59,  1.42s/it] 40%|████      | 940/2332 [27:33<32:53,  1.42s/it] 40%|████      | 941/2332 [27:34<32:50,  1.42s/it] 40%|████      | 942/2332 [27:36<32:48,  1.42s/it] 40%|████      | 943/2332 [27:37<32:48,  1.42s/it] 40%|████      | 944/2332 [27:38<32:45,  1.42s/it] 41%|████      | 945/2332 [27:40<32:44,  1.42s/it] 41%|████      | 946/2332 [27:41<32:43,  1.42s/it] 41%|████      | 947/2332 [27:43<32:41,  1.42s/it] 41%|████      | 948/2332 [27:44<32:44,  1.42s/it] 41%|████      | 949/2332 [27:46<32:41,  1.42s/it] 41%|████      | 950/2332 [27:47<32:40,  1.42s/it] 41%|████      | 951/2332 [27:48<32:39,  1.42s/it] 41%|████      | 952/2332 [27:50<32:36,  1.42s/it] 41%|████      | 953/2332 [27:51<32:31,  1.41s/it] 41%|████      | 954/2332 [27:53<32:29,  1.41s/it] 41%|████      | 955/2332 [27:54<32:27,  1.41s/it] 41%|████      | 956/2332 [27:55<32:26,  1.41s/it] 41%|████      | 957/2332 [27:57<32:23,  1.41s/it] 41%|████      | 958/2332 [27:58<32:30,  1.42s/it] 41%|████      | 959/2332 [28:00<32:29,  1.42s/it] 41%|████      | 960/2332 [28:01<32:29,  1.42s/it] 41%|████      | 961/2332 [28:03<32:31,  1.42s/it] 41%|████▏     | 962/2332 [28:04<32:24,  1.42s/it] 41%|████▏     | 963/2332 [28:05<32:20,  1.42s/it] 41%|████▏     | 964/2332 [28:07<33:10,  1.45s/it] 41%|████▏     | 965/2332 [28:08<32:56,  1.45s/it] 41%|████▏     | 966/2332 [28:10<32:43,  1.44s/it] 41%|████▏     | 967/2332 [28:11<32:38,  1.43s/it] 42%|████▏     | 968/2332 [28:13<32:26,  1.43s/it] 42%|████▏     | 969/2332 [28:14<32:20,  1.42s/it] 42%|████▏     | 970/2332 [28:15<32:14,  1.42s/it] 42%|████▏     | 971/2332 [28:17<32:09,  1.42s/it] 42%|████▏     | 972/2332 [28:18<32:07,  1.42s/it] 42%|████▏     | 973/2332 [28:20<32:10,  1.42s/it] 42%|████▏     | 974/2332 [28:21<32:07,  1.42s/it] 42%|████▏     | 975/2332 [28:23<32:02,  1.42s/it] 42%|████▏     | 976/2332 [28:24<31:59,  1.42s/it] 42%|████▏     | 977/2332 [28:25<31:55,  1.41s/it] 42%|████▏     | 978/2332 [28:27<31:59,  1.42s/it] 42%|████▏     | 979/2332 [28:28<31:59,  1.42s/it] 42%|████▏     | 980/2332 [28:30<31:56,  1.42s/it] 42%|████▏     | 981/2332 [28:31<31:53,  1.42s/it] 42%|████▏     | 982/2332 [28:32<31:51,  1.42s/it] 42%|████▏     | 983/2332 [28:34<31:53,  1.42s/it] 42%|████▏     | 984/2332 [28:35<31:55,  1.42s/it] 42%|████▏     | 985/2332 [28:37<31:49,  1.42s/it] 42%|████▏     | 986/2332 [28:38<31:47,  1.42s/it] 42%|████▏     | 987/2332 [28:40<31:44,  1.42s/it] 42%|████▏     | 988/2332 [28:41<31:49,  1.42s/it] 42%|████▏     | 989/2332 [28:42<31:47,  1.42s/it] 42%|████▏     | 990/2332 [28:44<31:45,  1.42s/it] 42%|████▏     | 991/2332 [28:45<31:40,  1.42s/it] 43%|████▎     | 992/2332 [28:47<31:38,  1.42s/it] 43%|████▎     | 993/2332 [28:48<31:35,  1.42s/it] 43%|████▎     | 994/2332 [28:50<32:29,  1.46s/it] 43%|████▎     | 995/2332 [28:51<32:16,  1.45s/it] 43%|████▎     | 996/2332 [28:52<31:59,  1.44s/it] 43%|████▎     | 997/2332 [28:54<31:50,  1.43s/it] 43%|████▎     | 998/2332 [28:55<31:41,  1.43s/it] 43%|████▎     | 999/2332 [28:57<31:33,  1.42s/it] 43%|████▎     | 1000/2332 [28:58<31:30,  1.42s/it]                                                   loss: 1.02848926, learning_rate: 1.202e-05, global_step: 1000, interval_runtime: 886.2113, interval_samples_per_second: 13.540788065408785, interval_steps_per_second: 0.564199502725366, epoch: 1.7153
 43%|████▎     | 1000/2332 [28:58<31:30,  1.42s/it][32m[2023-11-10 11:01:43,030] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens. If end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:01:43,491] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:01:43,492] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:01:43,492] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:01:43,492] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:01:43,492] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.90it/s][A
  8%|▊         | 3/36 [00:01<00:16,  2.04it/s][A
 11%|█         | 4/36 [00:02<00:18,  1.78it/s][A
 14%|█▍        | 5/36 [00:02<00:18,  1.65it/s][A
 17%|█▋        | 6/36 [00:03<00:18,  1.58it/s][A
 19%|█▉        | 7/36 [00:04<00:18,  1.54it/s][A
 22%|██▏       | 8/36 [00:04<00:18,  1.51it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.50it/s][A
 28%|██▊       | 10/36 [00:06<00:17,  1.49it/s][A
 31%|███       | 11/36 [00:06<00:16,  1.48it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.47it/s][A
 36%|███▌      | 13/36 [00:08<00:15,  1.47it/s][A
 39%|███▉      | 14/36 [00:08<00:15,  1.46it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.45it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.45it/s][A
 47%|████▋     | 17/36 [00:10<00:13,  1.46it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.46it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.45it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.45it/s][A
 58%|█████▊    | 21/36 [00:13<00:10,  1.45it/s][A
 61%|██████    | 22/36 [00:14<00:10,  1.37it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.39it/s][A
 67%|██████▋   | 24/36 [00:15<00:08,  1.41it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.43it/s][A
 72%|███████▏  | 26/36 [00:17<00:06,  1.44it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.44it/s][A
 78%|███████▊  | 28/36 [00:18<00:05,  1.45it/s][A
 81%|████████  | 29/36 [00:19<00:04,  1.45it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.45it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.59it/s][A
 89%|████████▉ | 32/36 [00:20<00:02,  1.90it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.17it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.43it/s][A
 97%|█████████▋| 35/36 [00:21<00:00,  2.64it/s][A
100%|██████████| 36/36 [00:21<00:00,  3.21it/s][A                                                   
                                               [Aeval_anls: 61.17761311558268, epoch: 1.7153
 43%|████▎     | 1000/2332 [29:24<31:30,  1.42s/it]
100%|██████████| 36/36 [00:22<00:00,  3.21it/s][A
                                               [A[32m[2023-11-10 11:02:08,554] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-1000[0m
[32m[2023-11-10 11:02:08,563] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-1000/config.json[0m
[32m[2023-11-10 11:02:11,104] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-1000/model_state.pdparams[0m
[32m[2023-11-10 11:02:11,105] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-1000/tokenizer_config.json[0m
[32m[2023-11-10 11:02:11,105] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-1000/special_tokens_map.json[0m
[32m[2023-11-10 11:02:16,143] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-900] due to args.save_total_limit[0m
 43%|████▎     | 1001/2332 [29:33<4:14:56, 11.49s/it] 43%|████▎     | 1002/2332 [29:34<3:07:41,  8.47s/it] 43%|████▎     | 1003/2332 [29:36<2:20:40,  6.35s/it] 43%|████▎     | 1004/2332 [29:37<1:47:48,  4.87s/it] 43%|████▎     | 1005/2332 [29:39<1:24:43,  3.83s/it] 43%|████▎     | 1006/2332 [29:40<1:08:36,  3.10s/it] 43%|████▎     | 1007/2332 [29:42<57:20,  2.60s/it]   43%|████▎     | 1008/2332 [29:43<49:32,  2.24s/it] 43%|████▎     | 1009/2332 [29:44<43:56,  1.99s/it] 43%|████▎     | 1010/2332 [29:46<40:03,  1.82s/it] 43%|████▎     | 1011/2332 [29:47<37:22,  1.70s/it] 43%|████▎     | 1012/2332 [29:49<35:26,  1.61s/it] 43%|████▎     | 1013/2332 [29:50<34:07,  1.55s/it] 43%|████▎     | 1014/2332 [29:51<33:12,  1.51s/it] 44%|████▎     | 1015/2332 [29:53<32:29,  1.48s/it] 44%|████▎     | 1016/2332 [29:54<32:00,  1.46s/it] 44%|████▎     | 1017/2332 [29:56<32:29,  1.48s/it] 44%|████▎     | 1018/2332 [29:57<31:58,  1.46s/it] 44%|████▎     | 1019/2332 [29:59<31:36,  1.44s/it] 44%|████▎     | 1020/2332 [30:00<32:04,  1.47s/it] 44%|████▍     | 1021/2332 [30:02<31:40,  1.45s/it] 44%|████▍     | 1022/2332 [30:03<31:22,  1.44s/it] 44%|████▍     | 1023/2332 [30:04<31:12,  1.43s/it] 44%|████▍     | 1024/2332 [30:06<31:01,  1.42s/it] 44%|████▍     | 1025/2332 [30:07<30:59,  1.42s/it] 44%|████▍     | 1026/2332 [30:09<30:51,  1.42s/it] 44%|████▍     | 1027/2332 [30:10<30:54,  1.42s/it] 44%|████▍     | 1028/2332 [30:11<30:52,  1.42s/it] 44%|████▍     | 1029/2332 [30:13<30:50,  1.42s/it] 44%|████▍     | 1030/2332 [30:14<30:47,  1.42s/it] 44%|████▍     | 1031/2332 [30:16<30:43,  1.42s/it] 44%|████▍     | 1032/2332 [30:17<30:44,  1.42s/it] 44%|████▍     | 1033/2332 [30:19<30:47,  1.42s/it] 44%|████▍     | 1034/2332 [30:20<30:41,  1.42s/it] 44%|████▍     | 1035/2332 [30:21<30:38,  1.42s/it] 44%|████▍     | 1036/2332 [30:23<30:33,  1.41s/it] 44%|████▍     | 1037/2332 [30:24<30:31,  1.41s/it] 45%|████▍     | 1038/2332 [30:26<30:29,  1.41s/it] 45%|████▍     | 1039/2332 [30:27<30:30,  1.42s/it] 45%|████▍     | 1040/2332 [30:28<30:26,  1.41s/it] 45%|████▍     | 1041/2332 [30:30<30:26,  1.41s/it] 45%|████▍     | 1042/2332 [30:31<31:10,  1.45s/it] 45%|████▍     | 1043/2332 [30:33<30:53,  1.44s/it] 45%|████▍     | 1044/2332 [30:34<30:43,  1.43s/it] 45%|████▍     | 1045/2332 [30:36<30:41,  1.43s/it] 45%|████▍     | 1046/2332 [30:37<30:38,  1.43s/it] 45%|████▍     | 1047/2332 [30:38<30:32,  1.43s/it] 45%|████▍     | 1048/2332 [30:40<30:26,  1.42s/it] 45%|████▍     | 1049/2332 [30:41<30:24,  1.42s/it] 45%|████▌     | 1050/2332 [30:43<31:01,  1.45s/it] 45%|████▌     | 1051/2332 [30:44<30:41,  1.44s/it] 45%|████▌     | 1052/2332 [30:46<30:29,  1.43s/it] 45%|████▌     | 1053/2332 [30:47<30:21,  1.42s/it] 45%|████▌     | 1054/2332 [30:48<30:17,  1.42s/it] 45%|████▌     | 1055/2332 [30:50<30:14,  1.42s/it] 45%|████▌     | 1056/2332 [30:51<30:08,  1.42s/it] 45%|████▌     | 1057/2332 [30:53<30:05,  1.42s/it] 45%|████▌     | 1058/2332 [30:54<29:58,  1.41s/it] 45%|████▌     | 1059/2332 [30:56<30:00,  1.41s/it] 45%|████▌     | 1060/2332 [30:57<30:00,  1.42s/it] 45%|████▌     | 1061/2332 [30:58<29:55,  1.41s/it] 46%|████▌     | 1062/2332 [31:00<29:54,  1.41s/it] 46%|████▌     | 1063/2332 [31:01<29:51,  1.41s/it] 46%|████▌     | 1064/2332 [31:03<29:51,  1.41s/it] 46%|████▌     | 1065/2332 [31:04<29:49,  1.41s/it] 46%|████▌     | 1066/2332 [31:05<29:48,  1.41s/it] 46%|████▌     | 1067/2332 [31:07<30:35,  1.45s/it] 46%|████▌     | 1068/2332 [31:08<30:22,  1.44s/it] 46%|████▌     | 1069/2332 [31:10<30:07,  1.43s/it] 46%|████▌     | 1070/2332 [31:11<29:58,  1.43s/it] 46%|████▌     | 1071/2332 [31:13<29:54,  1.42s/it] 46%|████▌     | 1072/2332 [31:14<29:48,  1.42s/it] 46%|████▌     | 1073/2332 [31:15<29:43,  1.42s/it] 46%|████▌     | 1074/2332 [31:17<29:42,  1.42s/it] 46%|████▌     | 1075/2332 [31:18<29:38,  1.41s/it] 46%|████▌     | 1076/2332 [31:20<29:38,  1.42s/it] 46%|████▌     | 1077/2332 [31:21<29:39,  1.42s/it] 46%|████▌     | 1078/2332 [31:23<29:35,  1.42s/it] 46%|████▋     | 1079/2332 [31:24<29:33,  1.42s/it] 46%|████▋     | 1080/2332 [31:25<30:16,  1.45s/it] 46%|████▋     | 1081/2332 [31:27<30:08,  1.45s/it] 46%|████▋     | 1082/2332 [31:28<29:56,  1.44s/it] 46%|████▋     | 1083/2332 [31:30<29:43,  1.43s/it] 46%|████▋     | 1084/2332 [31:31<29:37,  1.42s/it] 47%|████▋     | 1085/2332 [31:33<29:32,  1.42s/it] 47%|████▋     | 1086/2332 [31:34<29:26,  1.42s/it] 47%|████▋     | 1087/2332 [31:35<29:24,  1.42s/it] 47%|████▋     | 1088/2332 [31:37<29:22,  1.42s/it] 47%|████▋     | 1089/2332 [31:38<29:19,  1.42s/it] 47%|████▋     | 1090/2332 [31:40<29:19,  1.42s/it] 47%|████▋     | 1091/2332 [31:41<29:21,  1.42s/it] 47%|████▋     | 1092/2332 [31:42<29:18,  1.42s/it] 47%|████▋     | 1093/2332 [31:44<29:59,  1.45s/it] 47%|████▋     | 1094/2332 [31:45<29:43,  1.44s/it] 47%|████▋     | 1095/2332 [31:47<29:31,  1.43s/it] 47%|████▋     | 1096/2332 [31:48<29:20,  1.42s/it] 47%|████▋     | 1097/2332 [31:50<29:12,  1.42s/it] 47%|████▋     | 1098/2332 [31:51<29:11,  1.42s/it] 47%|████▋     | 1099/2332 [31:52<29:05,  1.42s/it] 47%|████▋     | 1100/2332 [31:54<29:05,  1.42s/it][32m[2023-11-10 11:04:38,789] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens. If end_labels, question_id, id, start_labels, token_to_orig_map, questions, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:04:39,237] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:04:39,238] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:04:39,238] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:04:39,238] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:04:39,238] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.92it/s][A
  8%|▊         | 3/36 [00:01<00:16,  2.06it/s][A
 11%|█         | 4/36 [00:02<00:17,  1.79it/s][A
 14%|█▍        | 5/36 [00:02<00:18,  1.66it/s][A
 17%|█▋        | 6/36 [00:03<00:18,  1.58it/s][A
 19%|█▉        | 7/36 [00:04<00:18,  1.54it/s][A
 22%|██▏       | 8/36 [00:04<00:18,  1.52it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.50it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.41it/s][A
 31%|███       | 11/36 [00:06<00:17,  1.42it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.43it/s][A
 36%|███▌      | 13/36 [00:08<00:15,  1.44it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.45it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.45it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.45it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.46it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.46it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.46it/s][A
 56%|█████▌    | 20/36 [00:13<00:10,  1.46it/s][A
 58%|█████▊    | 21/36 [00:13<00:10,  1.46it/s][A
 61%|██████    | 22/36 [00:14<00:09,  1.46it/s][A
 64%|██████▍   | 23/36 [00:15<00:08,  1.46it/s][A
 67%|██████▋   | 24/36 [00:15<00:08,  1.46it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.46it/s][A
 72%|███████▏  | 26/36 [00:17<00:06,  1.46it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.39it/s][A
 78%|███████▊  | 28/36 [00:18<00:05,  1.41it/s][A
 81%|████████  | 29/36 [00:19<00:04,  1.42it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.42it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.57it/s][A
 89%|████████▉ | 32/36 [00:20<00:02,  1.87it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.15it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.40it/s][A
 97%|█████████▋| 35/36 [00:21<00:00,  2.63it/s][A
100%|██████████| 36/36 [00:21<00:00,  3.19it/s][A                                                   
                                               [Aeval_anls: 60.77056920701604, epoch: 1.8868
 47%|████▋     | 1100/2332 [32:19<29:05,  1.42s/it]
100%|██████████| 36/36 [00:22<00:00,  3.19it/s][A
                                               [A[32m[2023-11-10 11:05:04,339] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-1100[0m
[32m[2023-11-10 11:05:04,348] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-1100/config.json[0m
[32m[2023-11-10 11:05:06,909] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-1100/model_state.pdparams[0m
[32m[2023-11-10 11:05:06,910] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-1100/tokenizer_config.json[0m
[32m[2023-11-10 11:05:06,910] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-1100/special_tokens_map.json[0m
[32m[2023-11-10 11:05:11,988] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-1000] due to args.save_total_limit[0m
 47%|████▋     | 1101/2332 [32:29<3:56:01, 11.50s/it] 47%|████▋     | 1102/2332 [32:30<2:53:48,  8.48s/it] 47%|████▋     | 1103/2332 [32:32<2:10:18,  6.36s/it] 47%|████▋     | 1104/2332 [32:33<1:40:26,  4.91s/it] 47%|████▋     | 1105/2332 [32:35<1:18:58,  3.86s/it] 47%|████▋     | 1106/2332 [32:36<1:03:52,  3.13s/it] 47%|████▋     | 1107/2332 [32:38<53:18,  2.61s/it]   48%|████▊     | 1108/2332 [32:39<45:54,  2.25s/it] 48%|████▊     | 1109/2332 [32:40<40:47,  2.00s/it] 48%|████▊     | 1110/2332 [32:42<37:06,  1.82s/it] 48%|████▊     | 1111/2332 [32:43<34:37,  1.70s/it] 48%|████▊     | 1112/2332 [32:45<32:46,  1.61s/it] 48%|████▊     | 1113/2332 [32:46<31:36,  1.56s/it] 48%|████▊     | 1114/2332 [32:47<30:44,  1.51s/it] 48%|████▊     | 1115/2332 [32:49<30:02,  1.48s/it] 48%|████▊     | 1116/2332 [32:50<29:36,  1.46s/it] 48%|████▊     | 1117/2332 [32:52<29:15,  1.45s/it] 48%|████▊     | 1118/2332 [32:53<29:04,  1.44s/it] 48%|████▊     | 1119/2332 [32:54<28:55,  1.43s/it] 48%|████▊     | 1120/2332 [32:56<28:46,  1.42s/it] 48%|████▊     | 1121/2332 [32:57<28:39,  1.42s/it] 48%|████▊     | 1122/2332 [32:59<28:34,  1.42s/it] 48%|████▊     | 1123/2332 [33:00<28:32,  1.42s/it] 48%|████▊     | 1124/2332 [33:02<28:30,  1.42s/it] 48%|████▊     | 1125/2332 [33:03<28:26,  1.41s/it] 48%|████▊     | 1126/2332 [33:05<29:09,  1.45s/it] 48%|████▊     | 1127/2332 [33:06<28:54,  1.44s/it] 48%|████▊     | 1128/2332 [33:07<28:40,  1.43s/it] 48%|████▊     | 1129/2332 [33:09<28:32,  1.42s/it] 48%|████▊     | 1130/2332 [33:10<28:27,  1.42s/it] 48%|████▊     | 1131/2332 [33:12<28:23,  1.42s/it] 49%|████▊     | 1132/2332 [33:13<28:20,  1.42s/it] 49%|████▊     | 1133/2332 [33:14<28:13,  1.41s/it] 49%|████▊     | 1134/2332 [33:16<28:53,  1.45s/it] 49%|████▊     | 1135/2332 [33:17<28:43,  1.44s/it] 49%|████▊     | 1136/2332 [33:19<28:31,  1.43s/it] 49%|████▉     | 1137/2332 [33:20<28:20,  1.42s/it] 49%|████▉     | 1138/2332 [33:22<28:16,  1.42s/it] 49%|████▉     | 1139/2332 [33:23<28:14,  1.42s/it][33m[2023-11-10 11:06:33,787] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 11:06:34,860] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')
=======================================================================
I1110 11:06:34.860824 185709 tcp_utils.cc:181] The server starts to listen on IP_ANY:43154
I1110 11:06:34.860960 185709 tcp_utils.cc:130] Successfully connected to 172.31.1.102:43154
W1110 11:06:39.197669 185709 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 11:06:39.202502 185709 gpu_resources.cc:149] device: 0, cuDNN Version: 8.5.
[32m[2023-11-10 11:06:39,892] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 11:06:39,893] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:06:39,893] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - model_name_or_path            : doc15k[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - [0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:06:39,894] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 11:06:39,895] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 11:06:39,896] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 11:06:39,896] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 11:06:39,896] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 11:06:39,896] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 11:06:39,896] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 11:06:39,896] [    INFO][0m - return_entity_level_metrics   : False[0m
[32m[2023-11-10 11:06:39,896] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 11:06:39,896] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 11:06:39,896] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 11:06:39,897] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 11:06:39,897] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 11:06:39,897] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 11:06:39,897] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 11:06:39,897] [    INFO][0m - [0m
[32m[2023-11-10 11:06:39,986] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'> to load 'doc15k'.[0m
[32m[2023-11-10 11:06:40,558] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.modeling.ErnieLayoutForQuestionAnswering'> to load 'doc15k'.[0m
[32m[2023-11-10 11:06:40,559] [    INFO][0m - Loading configuration file doc15k/config.json[0m
[32m[2023-11-10 11:06:40,559] [    INFO][0m - Loading weights file doc15k/model_state.pdparams[0m
[32m[2023-11-10 11:06:41,856] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-11-10 11:06:43,288] [    INFO][0m - All model checkpoint weights were used when initializing ErnieLayoutForQuestionAnswering.
[0m
[32m[2023-11-10 11:06:43,288] [    INFO][0m - All the weights of ErnieLayoutForQuestionAnswering were initialized from the model checkpoint at doc15k.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieLayoutForQuestionAnswering for predictions without further training.[0m
[32m[2023-11-10 11:06:43,322] [    INFO][0m - spliting train dataset into 16 shard[0m
[32m[2023-11-10 11:07:20,425] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:07:20,425] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-11-10 11:07:20,425] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:07:20,426] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-11-10 11:07:20,426] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-11-10 11:07:20,426] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-11-10 11:07:20,426] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-11-10 11:07:20,426] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2023-11-10 11:07:20,426] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2023-11-10 11:07:20,426] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-11-10 11:07:20,426] [    INFO][0m - bf16                          : False[0m
[32m[2023-11-10 11:07:20,426] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-11-10 11:07:20,427] [    INFO][0m - current_device                : gpu:0[0m
[32m[2023-11-10 11:07:20,427] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-11-10 11:07:20,427] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-11-10 11:07:20,427] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-11-10 11:07:20,427] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-11-10 11:07:20,427] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2023-11-10 11:07:20,427] [    INFO][0m - device                        : gpu[0m
[32m[2023-11-10 11:07:20,427] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-11-10 11:07:20,427] [    INFO][0m - do_eval                       : True[0m
[32m[2023-11-10 11:07:20,427] [    INFO][0m - do_export                     : False[0m
[32m[2023-11-10 11:07:20,427] [    INFO][0m - do_predict                    : False[0m
[32m[2023-11-10 11:07:20,428] [    INFO][0m - do_train                      : True[0m
[32m[2023-11-10 11:07:20,428] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-11-10 11:07:20,428] [    INFO][0m - eval_batch_size               : 6[0m
[32m[2023-11-10 11:07:20,428] [    INFO][0m - eval_steps                    : 100[0m
[32m[2023-11-10 11:07:20,428] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:07:20,428] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-11-10 11:07:20,428] [    INFO][0m - fp16                          : False[0m
[32m[2023-11-10 11:07:20,428] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-11-10 11:07:20,428] [    INFO][0m - fp16_opt_level                : O1[0m
[32m[2023-11-10 11:07:20,428] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-11-10 11:07:20,428] [    INFO][0m - greater_is_better             : True[0m
[32m[2023-11-10 11:07:20,429] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2023-11-10 11:07:20,429] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-11-10 11:07:20,429] [    INFO][0m - label_names                   : ['start_positions', 'end_positions'][0m
[32m[2023-11-10 11:07:20,429] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-11-10 11:07:20,429] [    INFO][0m - learning_rate                 : 2e-05[0m
[32m[2023-11-10 11:07:20,429] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2023-11-10 11:07:20,429] [    INFO][0m - load_sharded_model            : False[0m
[32m[2023-11-10 11:07:20,429] [    INFO][0m - local_process_index           : 0[0m
[32m[2023-11-10 11:07:20,429] [    INFO][0m - local_rank                    : 0[0m
[32m[2023-11-10 11:07:20,429] [    INFO][0m - log_level                     : -1[0m
[32m[2023-11-10 11:07:20,429] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-11-10 11:07:20,430] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-11-10 11:07:20,430] [    INFO][0m - logging_dir                   : ./models/fidelity_save_100/runs/Nov10_11-06-34_ip-172-31-1-102[0m
[32m[2023-11-10 11:07:20,430] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-11-10 11:07:20,430] [    INFO][0m - logging_steps                 : 500[0m
[32m[2023-11-10 11:07:20,430] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:07:20,430] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2023-11-10 11:07:20,430] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-11-10 11:07:20,430] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2023-11-10 11:07:20,430] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-11-10 11:07:20,430] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-11-10 11:07:20,430] [    INFO][0m - metric_for_best_model         : eval_f1[0m
[32m[2023-11-10 11:07:20,430] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-11-10 11:07:20,431] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-11-10 11:07:20,431] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2023-11-10 11:07:20,431] [    INFO][0m - num_train_epochs              : 4.0[0m
[32m[2023-11-10 11:07:20,431] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-11-10 11:07:20,431] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2023-11-10 11:07:20,431] [    INFO][0m - output_dir                    : ./models/fidelity_save_100/[0m
[32m[2023-11-10 11:07:20,431] [    INFO][0m - overwrite_output_dir          : True[0m
[32m[2023-11-10 11:07:20,431] [    INFO][0m - past_index                    : -1[0m
[32m[2023-11-10 11:07:20,431] [    INFO][0m - per_device_eval_batch_size    : 6[0m
[32m[2023-11-10 11:07:20,431] [    INFO][0m - per_device_train_batch_size   : 6[0m
[32m[2023-11-10 11:07:20,431] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-11-10 11:07:20,432] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-11-10 11:07:20,432] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-11-10 11:07:20,432] [    INFO][0m - power                         : 1.0[0m
[32m[2023-11-10 11:07:20,432] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-11-10 11:07:20,432] [    INFO][0m - process_index                 : 0[0m
[32m[2023-11-10 11:07:20,432] [    INFO][0m - recompute                     : False[0m
[32m[2023-11-10 11:07:20,432] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-11-10 11:07:20,432] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-11-10 11:07:20,432] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-11-10 11:07:20,432] [    INFO][0m - run_name                      : ./models/fidelity_save_100/[0m
[32m[2023-11-10 11:07:20,432] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-11-10 11:07:20,433] [    INFO][0m - save_sharded_model            : False[0m
[32m[2023-11-10 11:07:20,433] [    INFO][0m - save_steps                    : 100[0m
[32m[2023-11-10 11:07:20,433] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:07:20,433] [    INFO][0m - save_total_limit              : 1[0m
[32m[2023-11-10 11:07:20,433] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-11-10 11:07:20,433] [    INFO][0m - seed                          : 1000[0m
[32m[2023-11-10 11:07:20,433] [    INFO][0m - sharding                      : [][0m
[32m[2023-11-10 11:07:20,433] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-11-10 11:07:20,433] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2023-11-10 11:07:20,433] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-11-10 11:07:20,433] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-11-10 11:07:20,434] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2023-11-10 11:07:20,434] [    INFO][0m - should_log                    : True[0m
[32m[2023-11-10 11:07:20,434] [    INFO][0m - should_save                   : True[0m
[32m[2023-11-10 11:07:20,434] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-11-10 11:07:20,434] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2023-11-10 11:07:20,434] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-11-10 11:07:20,434] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2023-11-10 11:07:20,434] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-11-10 11:07:20,434] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-11-10 11:07:20,434] [    INFO][0m - train_batch_size              : 6[0m
[32m[2023-11-10 11:07:20,434] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2023-11-10 11:07:20,434] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2023-11-10 11:07:20,435] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-11-10 11:07:20,435] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-11-10 11:07:20,435] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2023-11-10 11:07:20,435] [    INFO][0m - world_size                    : 4[0m
[32m[2023-11-10 11:07:20,435] [    INFO][0m - [0m
[32m[2023-11-10 11:07:20,435] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: token_to_orig_map, token_is_max_context, question_id, end_labels, start_labels, tokens, questions, id. If token_to_orig_map, token_is_max_context, question_id, end_labels, start_labels, tokens, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:07:26,684] [    INFO][0m - ***** Running training *****[0m
[32m[2023-11-10 11:07:26,684] [    INFO][0m -   Num examples = 13,978[0m
[32m[2023-11-10 11:07:26,684] [    INFO][0m -   Num Epochs = 4[0m
[32m[2023-11-10 11:07:26,684] [    INFO][0m -   Instantaneous batch size per device = 6[0m
[32m[2023-11-10 11:07:26,684] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 24[0m
[32m[2023-11-10 11:07:26,684] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-11-10 11:07:26,684] [    INFO][0m -   Total optimization steps = 2,332[0m
[32m[2023-11-10 11:07:26,684] [    INFO][0m -   Total num train samples = 55,912[0m
[32m[2023-11-10 11:07:26,687] [    INFO][0m -   Number of trainable parameters = 281,693,122 (per device)[0m
  0%|          | 0/2332 [00:00<?, ?it/s]  0%|          | 1/2332 [00:04<3:07:35,  4.83s/it]  0%|          | 2/2332 [00:06<1:49:39,  2.82s/it]  0%|          | 3/2332 [00:07<1:24:33,  2.18s/it]  0%|          | 4/2332 [00:09<1:12:50,  1.88s/it]  0%|          | 5/2332 [00:10<1:06:22,  1.71s/it]  0%|          | 6/2332 [00:11<1:02:25,  1.61s/it]  0%|          | 7/2332 [00:13<59:54,  1.55s/it]    0%|          | 8/2332 [00:14<58:10,  1.50s/it]  0%|          | 9/2332 [00:16<57:05,  1.47s/it]  0%|          | 10/2332 [00:17<56:23,  1.46s/it]  0%|          | 11/2332 [00:18<55:50,  1.44s/it]  1%|          | 12/2332 [00:20<55:35,  1.44s/it]  1%|          | 13/2332 [00:21<55:18,  1.43s/it]  1%|          | 14/2332 [00:23<55:09,  1.43s/it]  1%|          | 15/2332 [00:24<55:08,  1.43s/it]  1%|          | 16/2332 [00:26<54:55,  1.42s/it]  1%|          | 17/2332 [00:27<54:46,  1.42s/it]  1%|          | 18/2332 [00:28<54:40,  1.42s/it]  1%|          | 19/2332 [00:30<54:41,  1.42s/it]  1%|          | 20/2332 [00:31<54:38,  1.42s/it]  1%|          | 21/2332 [00:33<54:36,  1.42s/it]  1%|          | 22/2332 [00:34<54:39,  1.42s/it]  1%|          | 23/2332 [00:35<54:31,  1.42s/it]  1%|          | 24/2332 [00:37<54:26,  1.42s/it]  1%|          | 25/2332 [00:38<54:26,  1.42s/it]  1%|          | 26/2332 [00:40<54:25,  1.42s/it]  1%|          | 27/2332 [00:41<54:26,  1.42s/it]  1%|          | 28/2332 [00:43<54:28,  1.42s/it]  1%|          | 29/2332 [00:44<54:17,  1.41s/it]  1%|▏         | 30/2332 [00:45<54:15,  1.41s/it]  1%|▏         | 31/2332 [00:47<54:14,  1.41s/it]  1%|▏         | 32/2332 [00:48<54:07,  1.41s/it]  1%|▏         | 33/2332 [00:50<54:16,  1.42s/it]  1%|▏         | 34/2332 [00:51<54:23,  1.42s/it]  2%|▏         | 35/2332 [00:52<54:13,  1.42s/it]  2%|▏         | 36/2332 [00:54<54:08,  1.42s/it]  2%|▏         | 37/2332 [00:55<54:06,  1.41s/it]  2%|▏         | 38/2332 [00:57<54:01,  1.41s/it]  2%|▏         | 39/2332 [00:58<54:08,  1.42s/it]  2%|▏         | 40/2332 [01:00<54:07,  1.42s/it]  2%|▏         | 41/2332 [01:01<53:59,  1.41s/it]  2%|▏         | 42/2332 [01:02<53:53,  1.41s/it]  2%|▏         | 43/2332 [01:04<53:54,  1.41s/it]  2%|▏         | 44/2332 [01:05<53:56,  1.41s/it]  2%|▏         | 45/2332 [01:07<55:14,  1.45s/it]  2%|▏         | 46/2332 [01:08<54:43,  1.44s/it]  2%|▏         | 47/2332 [01:10<54:33,  1.43s/it]  2%|▏         | 48/2332 [01:11<54:27,  1.43s/it]  2%|▏         | 49/2332 [01:12<54:20,  1.43s/it]  2%|▏         | 50/2332 [01:14<54:12,  1.43s/it]  2%|▏         | 51/2332 [01:15<54:04,  1.42s/it]  2%|▏         | 52/2332 [01:17<53:59,  1.42s/it]  2%|▏         | 53/2332 [01:18<53:49,  1.42s/it]  2%|▏         | 54/2332 [01:20<55:03,  1.45s/it]  2%|▏         | 55/2332 [01:21<54:46,  1.44s/it]  2%|▏         | 56/2332 [01:22<54:24,  1.43s/it]  2%|▏         | 57/2332 [01:24<54:23,  1.43s/it]  2%|▏         | 58/2332 [01:25<54:17,  1.43s/it]  3%|▎         | 59/2332 [01:27<54:01,  1.43s/it]  3%|▎         | 60/2332 [01:28<53:54,  1.42s/it]  3%|▎         | 61/2332 [01:30<53:52,  1.42s/it]  3%|▎         | 62/2332 [01:31<53:48,  1.42s/it]  3%|▎         | 63/2332 [01:32<53:41,  1.42s/it]  3%|▎         | 64/2332 [01:34<53:47,  1.42s/it]  3%|▎         | 65/2332 [01:35<53:35,  1.42s/it]  3%|▎         | 66/2332 [01:37<53:31,  1.42s/it]  3%|▎         | 67/2332 [01:38<53:30,  1.42s/it]  3%|▎         | 68/2332 [01:39<53:30,  1.42s/it]  3%|▎         | 69/2332 [01:41<53:34,  1.42s/it]  3%|▎         | 70/2332 [01:42<53:38,  1.42s/it]  3%|▎         | 71/2332 [01:44<54:46,  1.45s/it]  3%|▎         | 72/2332 [01:45<54:18,  1.44s/it]  3%|▎         | 73/2332 [01:47<53:56,  1.43s/it]  3%|▎         | 74/2332 [01:48<53:52,  1.43s/it]  3%|▎         | 75/2332 [01:50<53:39,  1.43s/it]  3%|▎         | 76/2332 [01:51<53:30,  1.42s/it]  3%|▎         | 77/2332 [01:52<53:21,  1.42s/it]  3%|▎         | 78/2332 [01:54<53:14,  1.42s/it]  3%|▎         | 79/2332 [01:55<53:08,  1.42s/it]  3%|▎         | 80/2332 [01:57<53:05,  1.41s/it]  3%|▎         | 81/2332 [01:58<53:13,  1.42s/it]  4%|▎         | 82/2332 [01:59<53:11,  1.42s/it]  4%|▎         | 83/2332 [02:01<53:14,  1.42s/it]  4%|▎         | 84/2332 [02:02<54:14,  1.45s/it]  4%|▎         | 85/2332 [02:04<53:58,  1.44s/it]  4%|▎         | 86/2332 [02:05<53:43,  1.44s/it]  4%|▎         | 87/2332 [02:07<53:39,  1.43s/it]  4%|▍         | 88/2332 [02:08<53:27,  1.43s/it]  4%|▍         | 89/2332 [02:09<53:19,  1.43s/it]  4%|▍         | 90/2332 [02:11<53:13,  1.42s/it]  4%|▍         | 91/2332 [02:12<53:14,  1.43s/it]  4%|▍         | 92/2332 [02:14<53:00,  1.42s/it]  4%|▍         | 93/2332 [02:15<52:59,  1.42s/it]  4%|▍         | 94/2332 [02:17<52:53,  1.42s/it]  4%|▍         | 95/2332 [02:18<52:52,  1.42s/it]  4%|▍         | 96/2332 [02:20<53:57,  1.45s/it]  4%|▍         | 97/2332 [02:21<53:43,  1.44s/it]  4%|▍         | 98/2332 [02:22<53:31,  1.44s/it]  4%|▍         | 99/2332 [02:24<53:27,  1.44s/it]  4%|▍         | 100/2332 [02:25<53:12,  1.43s/it][32m[2023-11-10 11:09:52,405] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: token_to_orig_map, token_is_max_context, question_id, end_labels, start_labels, tokens, questions, id. If token_to_orig_map, token_is_max_context, question_id, end_labels, start_labels, tokens, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:09:52,851] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:09:52,852] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:09:52,852] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:09:52,852] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:09:52,852] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.90it/s][A
  8%|▊         | 3/36 [00:01<00:16,  2.02it/s][A
 11%|█         | 4/36 [00:02<00:18,  1.75it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.62it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.56it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.51it/s][A
 22%|██▏       | 8/36 [00:04<00:18,  1.48it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.46it/s][A
 28%|██▊       | 10/36 [00:06<00:17,  1.46it/s][A
 31%|███       | 11/36 [00:06<00:17,  1.45it/s][A
 33%|███▎      | 12/36 [00:07<00:17,  1.39it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.41it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.42it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.41it/s][A
 44%|████▍     | 16/36 [00:10<00:14,  1.43it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.42it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.43it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.43it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.44it/s][A
 58%|█████▊    | 21/36 [00:14<00:10,  1.43it/s][A
 61%|██████    | 22/36 [00:14<00:09,  1.43it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.44it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.44it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.44it/s][A
 72%|███████▏  | 26/36 [00:17<00:06,  1.44it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.44it/s][A
 78%|███████▊  | 28/36 [00:19<00:05,  1.37it/s][A
 81%|████████  | 29/36 [00:19<00:05,  1.39it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.41it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.55it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.85it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.13it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.39it/s][A
 97%|█████████▋| 35/36 [00:22<00:00,  2.61it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.18it/s][A                                                  
                                               [Aeval_anls: 50.90847628806079, epoch: 0.1715
  4%|▍         | 100/2332 [02:54<53:12,  1.43s/it]
100%|██████████| 36/36 [00:25<00:00,  3.18it/s][A
                                               [A[32m[2023-11-10 11:10:21,283] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-100[0m
[32m[2023-11-10 11:10:21,292] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-100/config.json[0m
[33m[2023-11-10 11:11:33,630] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 11:11:34,691] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')
=======================================================================
I1110 11:11:34.691927 187361 tcp_utils.cc:181] The server starts to listen on IP_ANY:63714
I1110 11:11:34.692060 187361 tcp_utils.cc:130] Successfully connected to 172.31.1.102:63714
W1110 11:11:38.931288 187361 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 11:11:38.937297 187361 gpu_resources.cc:149] device: 0, cuDNN Version: 8.5.
[32m[2023-11-10 11:11:39,619] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 11:11:39,620] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:11:39,620] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 11:11:39,620] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:11:39,620] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 11:11:39,620] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 11:11:39,620] [    INFO][0m - model_name_or_path            : doc15k[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - [0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 11:11:39,621] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - return_entity_level_metrics   : True[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 11:11:39,622] [    INFO][0m - [0m
[32m[2023-11-10 11:11:39,711] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'> to load 'doc15k'.[0m
[32m[2023-11-10 11:11:40,276] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.modeling.ErnieLayoutForQuestionAnswering'> to load 'doc15k'.[0m
[32m[2023-11-10 11:11:40,276] [    INFO][0m - Loading configuration file doc15k/config.json[0m
[32m[2023-11-10 11:11:40,277] [    INFO][0m - Loading weights file doc15k/model_state.pdparams[0m
[32m[2023-11-10 11:11:41,572] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-11-10 11:11:43,005] [    INFO][0m - All model checkpoint weights were used when initializing ErnieLayoutForQuestionAnswering.
[0m
[32m[2023-11-10 11:11:43,006] [    INFO][0m - All the weights of ErnieLayoutForQuestionAnswering were initialized from the model checkpoint at doc15k.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieLayoutForQuestionAnswering for predictions without further training.[0m
[32m[2023-11-10 11:11:43,040] [    INFO][0m - spliting train dataset into 16 shard[0m
[32m[2023-11-10 11:12:19,676] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:12:19,676] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-11-10 11:12:19,677] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:12:19,677] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-11-10 11:12:19,677] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-11-10 11:12:19,677] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-11-10 11:12:19,677] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-11-10 11:12:19,677] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2023-11-10 11:12:19,677] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2023-11-10 11:12:19,677] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-11-10 11:12:19,677] [    INFO][0m - bf16                          : False[0m
[32m[2023-11-10 11:12:19,677] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-11-10 11:12:19,677] [    INFO][0m - current_device                : gpu:0[0m
[32m[2023-11-10 11:12:19,677] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - device                        : gpu[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - do_eval                       : True[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - do_export                     : False[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - do_predict                    : False[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - do_train                      : True[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - eval_batch_size               : 6[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - eval_steps                    : 100[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:12:19,678] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - fp16                          : False[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - fp16_opt_level                : O1[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - greater_is_better             : True[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - label_names                   : ['start_positions', 'end_positions'][0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - learning_rate                 : 2e-05[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - load_sharded_model            : False[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - local_process_index           : 0[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - local_rank                    : 0[0m
[32m[2023-11-10 11:12:19,679] [    INFO][0m - log_level                     : -1[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - logging_dir                   : ./models/fidelity_save_100/runs/Nov10_11-11-34_ip-172-31-1-102[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - logging_steps                 : 500[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - metric_for_best_model         : anls[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-11-10 11:12:19,680] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - num_train_epochs              : 4.0[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - output_dir                    : ./models/fidelity_save_100/[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - overwrite_output_dir          : True[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - past_index                    : -1[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - per_device_eval_batch_size    : 6[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - per_device_train_batch_size   : 6[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - power                         : 1.0[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - process_index                 : 0[0m
[32m[2023-11-10 11:12:19,681] [    INFO][0m - recompute                     : False[0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - run_name                      : ./models/fidelity_save_100/[0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - save_sharded_model            : False[0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - save_steps                    : 100[0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - save_total_limit              : 1[0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - seed                          : 1000[0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - sharding                      : [][0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2023-11-10 11:12:19,682] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - should_log                    : True[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - should_save                   : True[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - train_batch_size              : 6[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-11-10 11:12:19,683] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-11-10 11:12:19,684] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2023-11-10 11:12:19,684] [    INFO][0m - world_size                    : 4[0m
[32m[2023-11-10 11:12:19,684] [    INFO][0m - [0m
[32m[2023-11-10 11:12:19,684] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: question_id, questions, start_labels, end_labels, token_to_orig_map, id, token_is_max_context, tokens. If question_id, questions, start_labels, end_labels, token_to_orig_map, id, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:12:25,893] [    INFO][0m - ***** Running training *****[0m
[32m[2023-11-10 11:12:25,893] [    INFO][0m -   Num examples = 13,978[0m
[32m[2023-11-10 11:12:25,893] [    INFO][0m -   Num Epochs = 4[0m
[32m[2023-11-10 11:12:25,893] [    INFO][0m -   Instantaneous batch size per device = 6[0m
[32m[2023-11-10 11:12:25,894] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 24[0m
[32m[2023-11-10 11:12:25,894] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-11-10 11:12:25,894] [    INFO][0m -   Total optimization steps = 2,332[0m
[32m[2023-11-10 11:12:25,894] [    INFO][0m -   Total num train samples = 55,912[0m
[32m[2023-11-10 11:12:25,896] [    INFO][0m -   Number of trainable parameters = 281,693,122 (per device)[0m
  0%|          | 0/2332 [00:00<?, ?it/s]  0%|          | 1/2332 [00:05<3:14:37,  5.01s/it]  0%|          | 2/2332 [00:06<1:52:29,  2.90s/it]  0%|          | 3/2332 [00:07<1:26:07,  2.22s/it]  0%|          | 4/2332 [00:09<1:13:45,  1.90s/it]  0%|          | 5/2332 [00:10<1:06:37,  1.72s/it]  0%|          | 6/2332 [00:12<1:02:18,  1.61s/it]  0%|          | 7/2332 [00:13<59:32,  1.54s/it]    0%|          | 8/2332 [00:14<57:44,  1.49s/it]  0%|          | 9/2332 [00:16<56:37,  1.46s/it]  0%|          | 10/2332 [00:17<55:59,  1.45s/it]  0%|          | 11/2332 [00:19<55:33,  1.44s/it]  1%|          | 12/2332 [00:20<55:05,  1.42s/it]  1%|          | 13/2332 [00:21<55:04,  1.43s/it]  1%|          | 14/2332 [00:23<54:48,  1.42s/it]  1%|          | 15/2332 [00:24<54:48,  1.42s/it]  1%|          | 16/2332 [00:26<54:41,  1.42s/it]  1%|          | 17/2332 [00:27<54:34,  1.41s/it]  1%|          | 18/2332 [00:28<54:21,  1.41s/it]  1%|          | 19/2332 [00:30<54:21,  1.41s/it]  1%|          | 20/2332 [00:31<54:26,  1.41s/it]  1%|          | 21/2332 [00:33<54:34,  1.42s/it]  1%|          | 22/2332 [00:34<54:31,  1.42s/it]  1%|          | 23/2332 [00:36<54:38,  1.42s/it]  1%|          | 24/2332 [00:37<54:31,  1.42s/it]  1%|          | 25/2332 [00:38<54:25,  1.42s/it]  1%|          | 26/2332 [00:40<54:16,  1.41s/it]  1%|          | 27/2332 [00:41<54:19,  1.41s/it]  1%|          | 28/2332 [00:43<54:14,  1.41s/it]  1%|          | 29/2332 [00:44<54:10,  1.41s/it]  1%|▏         | 30/2332 [00:45<54:02,  1.41s/it]  1%|▏         | 31/2332 [00:47<54:06,  1.41s/it]  1%|▏         | 32/2332 [00:48<53:59,  1.41s/it]  1%|▏         | 33/2332 [00:50<54:03,  1.41s/it]  1%|▏         | 34/2332 [00:51<54:01,  1.41s/it]  2%|▏         | 35/2332 [00:52<53:53,  1.41s/it]  2%|▏         | 36/2332 [00:54<54:01,  1.41s/it]  2%|▏         | 37/2332 [00:55<54:08,  1.42s/it]  2%|▏         | 38/2332 [00:57<54:02,  1.41s/it]  2%|▏         | 39/2332 [00:58<54:08,  1.42s/it]  2%|▏         | 40/2332 [01:00<54:06,  1.42s/it]  2%|▏         | 41/2332 [01:01<54:13,  1.42s/it]  2%|▏         | 42/2332 [01:02<54:06,  1.42s/it]  2%|▏         | 43/2332 [01:04<54:10,  1.42s/it]  2%|▏         | 44/2332 [01:05<54:06,  1.42s/it]  2%|▏         | 45/2332 [01:07<55:04,  1.44s/it]  2%|▏         | 46/2332 [01:08<54:41,  1.44s/it]  2%|▏         | 47/2332 [01:10<54:22,  1.43s/it]  2%|▏         | 48/2332 [01:11<54:13,  1.42s/it]  2%|▏         | 49/2332 [01:12<54:00,  1.42s/it]  2%|▏         | 50/2332 [01:14<53:48,  1.41s/it]  2%|▏         | 51/2332 [01:15<53:45,  1.41s/it]  2%|▏         | 52/2332 [01:17<53:41,  1.41s/it]  2%|▏         | 53/2332 [01:18<53:41,  1.41s/it]  2%|▏         | 54/2332 [01:20<55:01,  1.45s/it]  2%|▏         | 55/2332 [01:21<54:32,  1.44s/it]  2%|▏         | 56/2332 [01:22<54:17,  1.43s/it]  2%|▏         | 57/2332 [01:24<54:02,  1.43s/it]  2%|▏         | 58/2332 [01:25<53:59,  1.42s/it]  3%|▎         | 59/2332 [01:27<53:48,  1.42s/it]  3%|▎         | 60/2332 [01:28<53:35,  1.42s/it]  3%|▎         | 61/2332 [01:29<53:34,  1.42s/it]  3%|▎         | 62/2332 [01:31<53:29,  1.41s/it]  3%|▎         | 63/2332 [01:32<53:27,  1.41s/it]  3%|▎         | 64/2332 [01:34<53:30,  1.42s/it]  3%|▎         | 65/2332 [01:35<53:27,  1.41s/it]  3%|▎         | 66/2332 [01:36<53:23,  1.41s/it]  3%|▎         | 67/2332 [01:38<53:12,  1.41s/it]  3%|▎         | 68/2332 [01:39<53:15,  1.41s/it]  3%|▎         | 69/2332 [01:41<53:17,  1.41s/it]  3%|▎         | 70/2332 [01:42<53:14,  1.41s/it]  3%|▎         | 71/2332 [01:44<53:58,  1.43s/it]  3%|▎         | 72/2332 [01:45<53:49,  1.43s/it]  3%|▎         | 73/2332 [01:46<53:36,  1.42s/it]  3%|▎         | 74/2332 [01:48<53:29,  1.42s/it]  3%|▎         | 75/2332 [01:49<53:17,  1.42s/it]  3%|▎         | 76/2332 [01:51<53:11,  1.41s/it]  3%|▎         | 77/2332 [01:52<53:10,  1.41s/it]  3%|▎         | 78/2332 [01:54<53:10,  1.42s/it]  3%|▎         | 79/2332 [01:55<53:04,  1.41s/it]  3%|▎         | 80/2332 [01:56<53:04,  1.41s/it]  3%|▎         | 81/2332 [01:58<53:00,  1.41s/it]  4%|▎         | 82/2332 [01:59<52:59,  1.41s/it]  4%|▎         | 83/2332 [02:01<52:57,  1.41s/it]  4%|▎         | 84/2332 [02:02<54:09,  1.45s/it]  4%|▎         | 85/2332 [02:04<53:50,  1.44s/it]  4%|▎         | 86/2332 [02:05<53:32,  1.43s/it]  4%|▎         | 87/2332 [02:06<53:19,  1.43s/it]  4%|▍         | 88/2332 [02:08<53:09,  1.42s/it]  4%|▍         | 89/2332 [02:09<53:58,  1.44s/it]  4%|▍         | 90/2332 [02:11<53:32,  1.43s/it]  4%|▍         | 91/2332 [02:12<53:15,  1.43s/it]  4%|▍         | 92/2332 [02:13<53:08,  1.42s/it]  4%|▍         | 93/2332 [02:15<53:10,  1.42s/it]  4%|▍         | 94/2332 [02:16<53:02,  1.42s/it]  4%|▍         | 95/2332 [02:18<52:59,  1.42s/it]  4%|▍         | 96/2332 [02:19<53:35,  1.44s/it]  4%|▍         | 97/2332 [02:21<53:24,  1.43s/it]  4%|▍         | 98/2332 [02:22<53:08,  1.43s/it]  4%|▍         | 99/2332 [02:23<53:10,  1.43s/it]  4%|▍         | 100/2332 [02:25<52:57,  1.42s/it][32m[2023-11-10 11:14:51,296] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: question_id, questions, start_labels, end_labels, token_to_orig_map, id, token_is_max_context, tokens. If question_id, questions, start_labels, end_labels, token_to_orig_map, id, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:14:51,742] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:14:51,742] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:14:51,742] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:14:51,742] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:14:51,743] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.91it/s][A
  8%|▊         | 3/36 [00:01<00:16,  2.06it/s][A
 11%|█         | 4/36 [00:02<00:17,  1.78it/s][A
 14%|█▍        | 5/36 [00:02<00:18,  1.66it/s][A
 17%|█▋        | 6/36 [00:03<00:18,  1.58it/s][A
 19%|█▉        | 7/36 [00:04<00:18,  1.54it/s][A
 22%|██▏       | 8/36 [00:04<00:18,  1.51it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.50it/s][A
 28%|██▊       | 10/36 [00:06<00:17,  1.49it/s][A
 31%|███       | 11/36 [00:06<00:16,  1.48it/s][A
 33%|███▎      | 12/36 [00:07<00:17,  1.41it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.42it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.43it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.44it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.45it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.45it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.45it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.46it/s][A
 56%|█████▌    | 20/36 [00:13<00:10,  1.46it/s][A
 58%|█████▊    | 21/36 [00:13<00:10,  1.46it/s][A
 61%|██████    | 22/36 [00:14<00:09,  1.46it/s][A
 64%|██████▍   | 23/36 [00:15<00:08,  1.46it/s][A
 67%|██████▋   | 24/36 [00:15<00:08,  1.46it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.46it/s][A
 72%|███████▏  | 26/36 [00:17<00:06,  1.46it/s][A
 75%|███████▌  | 27/36 [00:17<00:06,  1.46it/s][A
 78%|███████▊  | 28/36 [00:18<00:05,  1.41it/s][A
 81%|████████  | 29/36 [00:19<00:04,  1.42it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.43it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.58it/s][A
 89%|████████▉ | 32/36 [00:20<00:02,  1.88it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.16it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.41it/s][A
 97%|█████████▋| 35/36 [00:21<00:00,  2.63it/s][A
100%|██████████| 36/36 [00:21<00:00,  3.20it/s][A                                                  
                                               [Aeval_anls: 50.90847628806079, epoch: 0.1715
  4%|▍         | 100/2332 [02:54<52:57,  1.42s/it]
100%|██████████| 36/36 [00:25<00:00,  3.20it/s][A
                                               [A[32m[2023-11-10 11:15:20,103] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-100[0m
[32m[2023-11-10 11:15:20,112] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-100/config.json[0m
[32m[2023-11-10 11:15:26,017] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-100/model_state.pdparams[0m
[32m[2023-11-10 11:15:26,018] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-100/tokenizer_config.json[0m
[32m[2023-11-10 11:15:26,018] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-100/special_tokens_map.json[0m
[32m[2023-11-10 11:15:31,147] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-400] due to args.save_total_limit[0m
  4%|▍         | 101/2332 [03:07<8:22:33, 13.52s/it]  4%|▍         | 102/2332 [03:08<6:07:20,  9.88s/it]  4%|▍         | 103/2332 [03:10<4:34:05,  7.38s/it]  4%|▍         | 104/2332 [03:11<3:27:33,  5.59s/it]  5%|▍         | 105/2332 [03:12<2:40:54,  4.34s/it]  5%|▍         | 106/2332 [03:14<2:08:22,  3.46s/it]  5%|▍         | 107/2332 [03:15<1:45:31,  2.85s/it]  5%|▍         | 108/2332 [03:17<1:29:29,  2.41s/it]  5%|▍         | 109/2332 [03:18<1:18:22,  2.12s/it]  5%|▍         | 110/2332 [03:19<1:10:27,  1.90s/it]  5%|▍         | 111/2332 [03:21<1:04:57,  1.75s/it]  5%|▍         | 112/2332 [03:22<1:01:05,  1.65s/it]  5%|▍         | 113/2332 [03:24<58:27,  1.58s/it]    5%|▍         | 114/2332 [03:25<56:34,  1.53s/it]  5%|▍         | 115/2332 [03:27<55:12,  1.49s/it]  5%|▍         | 116/2332 [03:28<54:09,  1.47s/it]  5%|▌         | 117/2332 [03:29<53:25,  1.45s/it]  5%|▌         | 118/2332 [03:31<52:56,  1.43s/it]  5%|▌         | 119/2332 [03:32<52:39,  1.43s/it]  5%|▌         | 120/2332 [03:34<52:31,  1.42s/it]  5%|▌         | 121/2332 [03:35<53:18,  1.45s/it]  5%|▌         | 122/2332 [03:36<52:52,  1.44s/it]  5%|▌         | 123/2332 [03:38<52:38,  1.43s/it]  5%|▌         | 124/2332 [03:39<52:21,  1.42s/it]  5%|▌         | 125/2332 [03:41<52:11,  1.42s/it]  5%|▌         | 126/2332 [03:42<52:08,  1.42s/it]  5%|▌         | 127/2332 [03:44<52:04,  1.42s/it]  5%|▌         | 128/2332 [03:45<53:19,  1.45s/it]  6%|▌         | 129/2332 [03:46<52:52,  1.44s/it]  6%|▌         | 130/2332 [03:48<52:27,  1.43s/it]  6%|▌         | 131/2332 [03:49<52:14,  1.42s/it]  6%|▌         | 132/2332 [03:51<52:05,  1.42s/it]  6%|▌         | 133/2332 [03:52<51:49,  1.41s/it]  6%|▌         | 134/2332 [03:54<51:45,  1.41s/it]  6%|▌         | 135/2332 [03:55<51:44,  1.41s/it]  6%|▌         | 136/2332 [03:56<51:39,  1.41s/it]  6%|▌         | 137/2332 [03:58<51:36,  1.41s/it]  6%|▌         | 138/2332 [03:59<51:35,  1.41s/it]  6%|▌         | 139/2332 [04:01<51:37,  1.41s/it]  6%|▌         | 140/2332 [04:02<51:33,  1.41s/it]  6%|▌         | 141/2332 [04:03<51:24,  1.41s/it]  6%|▌         | 142/2332 [04:05<51:21,  1.41s/it]  6%|▌         | 143/2332 [04:06<51:24,  1.41s/it]  6%|▌         | 144/2332 [04:08<51:23,  1.41s/it]  6%|▌         | 145/2332 [04:09<51:11,  1.40s/it]  6%|▋         | 146/2332 [04:11<52:20,  1.44s/it]  6%|▋         | 147/2332 [04:12<51:55,  1.43s/it]  6%|▋         | 148/2332 [04:13<51:38,  1.42s/it]  6%|▋         | 149/2332 [04:15<51:29,  1.42s/it]  6%|▋         | 150/2332 [04:16<51:28,  1.42s/it]  6%|▋         | 151/2332 [04:18<51:15,  1.41s/it]  7%|▋         | 152/2332 [04:19<51:12,  1.41s/it]  7%|▋         | 153/2332 [04:20<52:28,  1.44s/it]  7%|▋         | 154/2332 [04:22<52:00,  1.43s/it]  7%|▋         | 155/2332 [04:23<51:40,  1.42s/it]  7%|▋         | 156/2332 [04:25<51:30,  1.42s/it]  7%|▋         | 157/2332 [04:26<51:15,  1.41s/it]  7%|▋         | 158/2332 [04:27<51:10,  1.41s/it]  7%|▋         | 159/2332 [04:29<51:07,  1.41s/it]  7%|▋         | 160/2332 [04:30<51:04,  1.41s/it]  7%|▋         | 161/2332 [04:32<50:58,  1.41s/it]  7%|▋         | 162/2332 [04:33<50:59,  1.41s/it]  7%|▋         | 163/2332 [04:35<50:58,  1.41s/it]  7%|▋         | 164/2332 [04:36<50:52,  1.41s/it]  7%|▋         | 165/2332 [04:37<50:46,  1.41s/it]  7%|▋         | 166/2332 [04:39<50:50,  1.41s/it]  7%|▋         | 167/2332 [04:40<50:52,  1.41s/it]  7%|▋         | 168/2332 [04:42<50:49,  1.41s/it]  7%|▋         | 169/2332 [04:43<50:45,  1.41s/it]  7%|▋         | 170/2332 [04:44<50:40,  1.41s/it]  7%|▋         | 171/2332 [04:46<50:39,  1.41s/it]  7%|▋         | 172/2332 [04:47<51:44,  1.44s/it]  7%|▋         | 173/2332 [04:49<51:15,  1.42s/it]  7%|▋         | 174/2332 [04:50<50:58,  1.42s/it]  8%|▊         | 175/2332 [04:52<50:57,  1.42s/it]  8%|▊         | 176/2332 [04:53<50:48,  1.41s/it]  8%|▊         | 177/2332 [04:54<50:50,  1.42s/it]  8%|▊         | 178/2332 [04:56<52:02,  1.45s/it]  8%|▊         | 179/2332 [04:57<51:34,  1.44s/it]  8%|▊         | 180/2332 [04:59<51:15,  1.43s/it]  8%|▊         | 181/2332 [05:00<50:59,  1.42s/it]  8%|▊         | 182/2332 [05:01<50:46,  1.42s/it]  8%|▊         | 183/2332 [05:03<50:39,  1.41s/it]  8%|▊         | 184/2332 [05:04<50:32,  1.41s/it]  8%|▊         | 185/2332 [05:06<50:24,  1.41s/it]  8%|▊         | 186/2332 [05:07<50:25,  1.41s/it]  8%|▊         | 187/2332 [05:09<50:25,  1.41s/it]  8%|▊         | 188/2332 [05:10<50:20,  1.41s/it]  8%|▊         | 189/2332 [05:11<50:18,  1.41s/it]  8%|▊         | 190/2332 [05:13<50:08,  1.40s/it]  8%|▊         | 191/2332 [05:14<50:16,  1.41s/it]  8%|▊         | 192/2332 [05:16<50:12,  1.41s/it]  8%|▊         | 193/2332 [05:17<50:06,  1.41s/it]  8%|▊         | 194/2332 [05:18<50:10,  1.41s/it]  8%|▊         | 195/2332 [05:20<50:14,  1.41s/it]  8%|▊         | 196/2332 [05:21<50:10,  1.41s/it]  8%|▊         | 197/2332 [05:23<50:12,  1.41s/it]  8%|▊         | 198/2332 [05:24<51:13,  1.44s/it]  9%|▊         | 199/2332 [05:26<50:55,  1.43s/it]  9%|▊         | 200/2332 [05:27<50:38,  1.43s/it][32m[2023-11-10 11:17:53,352] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: question_id, questions, start_labels, end_labels, token_to_orig_map, id, token_is_max_context, tokens. If question_id, questions, start_labels, end_labels, token_to_orig_map, id, token_is_max_context, tokens are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:17:53,680] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:17:53,681] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:17:53,681] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:17:53,681] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:17:53,681] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.92it/s][A
  8%|▊         | 3/36 [00:01<00:17,  1.87it/s][A
 11%|█         | 4/36 [00:02<00:18,  1.69it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.61it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.55it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.52it/s][A
 22%|██▏       | 8/36 [00:04<00:18,  1.50it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.49it/s][A
 28%|██▊       | 10/36 [00:06<00:17,  1.48it/s][A
 31%|███       | 11/36 [00:06<00:16,  1.47it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.47it/s][A
 36%|███▌      | 13/36 [00:08<00:15,  1.47it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.46it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.46it/s][A
 44%|████▍     | 16/36 [00:10<00:13,  1.46it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.46it/s][A
 50%|█████     | 18/36 [00:11<00:12,  1.47it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.47it/s][A
 56%|█████▌    | 20/36 [00:13<00:10,  1.47it/s][A
 58%|█████▊    | 21/36 [00:13<00:10,  1.46it/s][A
 61%|██████    | 22/36 [00:14<00:09,  1.46it/s][A
 64%|██████▍   | 23/36 [00:15<00:08,  1.46it/s][A
 67%|██████▋   | 24/36 [00:15<00:08,  1.46it/s][A
 69%|██████▉   | 25/36 [00:16<00:07,  1.46it/s][A
 72%|███████▏  | 26/36 [00:17<00:07,  1.40it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.42it/s][A
 78%|███████▊  | 28/36 [00:18<00:05,  1.43it/s][A
 81%|████████  | 29/36 [00:19<00:04,  1.44it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.44it/s][A
 86%|████████▌ | 31/36 [00:20<00:03,  1.58it/s][A
 89%|████████▉ | 32/36 [00:20<00:02,  1.88it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.17it/s][A
 94%|█████████▍| 34/36 [00:21<00:00,  2.43it/s][A
 97%|█████████▋| 35/36 [00:21<00:00,  2.65it/s][A
100%|██████████| 36/36 [00:21<00:00,  3.22it/s][A                                                  
                                               [Aeval_anls: 58.57743644437427, epoch: 0.3431
  9%|▊         | 200/2332 [05:52<50:38,  1.43s/it]
100%|██████████| 36/36 [00:22<00:00,  3.22it/s][A
                                               [A[32m[2023-11-10 11:18:18,804] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-200[0m
[32m[2023-11-10 11:18:18,813] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-200/config.json[0m
[32m[2023-11-10 11:18:21,336] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-200/model_state.pdparams[0m
[32m[2023-11-10 11:18:21,336] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-200/tokenizer_config.json[0m
[32m[2023-11-10 11:18:21,337] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-200/special_tokens_map.json[0m
[33m[2023-11-10 11:18:58,831] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[33m[2023-11-10 11:18:59,890] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')
=======================================================================
I1110 11:18:59.891489 190087 tcp_utils.cc:181] The server starts to listen on IP_ANY:48747
I1110 11:18:59.891614 190087 tcp_utils.cc:130] Successfully connected to 172.31.1.102:48747
W1110 11:19:04.229796 190087 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 12.3, Runtime API Version: 11.7
W1110 11:19:04.235829 190087 gpu_resources.cc:149] device: 0, cuDNN Version: 8.5.
[32m[2023-11-10 11:19:04,916] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-11-10 11:19:04,917] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:19:04,917] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-11-10 11:19:04,917] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:19:04,917] [    INFO][0m - cache_dir                     : None[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - config_name                   : None[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - model_name_or_path            : ernie-layoutx-base-uncased[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - tokenizer_name                : None[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - [0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - dataset_config_name           : None[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - dataset_name                  : fidelity[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - doc_stride                    : 128[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - label_all_tokens              : False[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - lang                          : en[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - max_seq_length                : 512[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - max_test_samples              : None[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - max_train_samples             : None[0m
[32m[2023-11-10 11:19:04,918] [    INFO][0m - max_val_samples               : None[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - overwrite_cache               : False[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - pad_to_max_length             : True[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - pattern                       : mrc[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - preprocessing_num_workers     : 32[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - return_entity_level_metrics   : True[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - rst_converter                 : None[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - target_size                   : 1000[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - task_name                     : ner[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - task_type                     : ner[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - train_log_file                : None[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - train_nshard                  : 16[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - use_segment_box               : False[0m
[32m[2023-11-10 11:19:04,919] [    INFO][0m - [0m
[32m[2023-11-10 11:19:04,957] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie_layout.tokenizer.ErnieLayoutTokenizer'>, False) to load 'ernie-layoutx-base-uncased'.[0m
[32m[2023-11-10 11:19:04,958] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/vocab.txt[0m
[32m[2023-11-10 11:19:04,958] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/sentencepiece.bpe.model[0m
[32m[2023-11-10 11:19:05,517] [    INFO][0m - tokenizer config file saved in /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/tokenizer_config.json[0m
[32m[2023-11-10 11:19:05,517] [    INFO][0m - Special tokens file saved in /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/special_tokens_map.json[0m
[32m[2023-11-10 11:19:05,517] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie_layout.modeling.ErnieLayoutForQuestionAnswering'> to load 'ernie-layoutx-base-uncased'.[0m
[32m[2023-11-10 11:19:05,518] [    INFO][0m - Already cached /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/model_state.pdparams[0m
[32m[2023-11-10 11:19:05,518] [    INFO][0m - Loading weights file model_state.pdparams from cache at /home/ubuntu/.paddlenlp/models/ernie-layoutx-base-uncased/model_state.pdparams[0m
[32m[2023-11-10 11:19:06,804] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-11-10 11:19:08,249] [    INFO][0m - All model checkpoint weights were used when initializing ErnieLayoutForQuestionAnswering.
[0m
[33m[2023-11-10 11:19:08,249] [ WARNING][0m - Some weights of ErnieLayoutForQuestionAnswering were not initialized from the model checkpoint at ernie-layoutx-base-uncased and are newly initialized: ['visual.pixel_mean', 'qa_outputs.bias', 'qa_outputs.weight', 'embeddings.position_ids', 'visual.pixel_std']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-11-10 11:19:08,281] [    INFO][0m - spliting train dataset into 16 shard[0m
[32m[2023-11-10 11:19:45,315] [    INFO][0m - ============================================================[0m
[32m[2023-11-10 11:19:45,316] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-11-10 11:19:45,316] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2023-11-10 11:19:45,316] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-11-10 11:19:45,316] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-11-10 11:19:45,316] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-11-10 11:19:45,316] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-11-10 11:19:45,316] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2023-11-10 11:19:45,316] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2023-11-10 11:19:45,316] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-11-10 11:19:45,316] [    INFO][0m - bf16                          : False[0m
[32m[2023-11-10 11:19:45,316] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - current_device                : gpu:0[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - device                        : gpu[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - do_eval                       : True[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - do_export                     : False[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - do_predict                    : False[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - do_train                      : True[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - eval_batch_size               : 6[0m
[32m[2023-11-10 11:19:45,317] [    INFO][0m - eval_steps                    : 100[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - fp16                          : False[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - fp16_opt_level                : O1[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - greater_is_better             : True[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - label_names                   : ['start_positions', 'end_positions'][0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - learning_rate                 : 2e-05[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - load_sharded_model            : False[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - local_process_index           : 0[0m
[32m[2023-11-10 11:19:45,318] [    INFO][0m - local_rank                    : 0[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - log_level                     : -1[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - logging_dir                   : ./models/fidelity_save_100/runs/Nov10_11-18-59_ip-172-31-1-102[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - logging_steps                 : 500[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - metric_for_best_model         : anls[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-11-10 11:19:45,319] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - num_train_epochs              : 4.0[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - output_dir                    : ./models/fidelity_save_100/[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - overwrite_output_dir          : True[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - past_index                    : -1[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - per_device_eval_batch_size    : 6[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - per_device_train_batch_size   : 6[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - power                         : 1.0[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - process_index                 : 0[0m
[32m[2023-11-10 11:19:45,320] [    INFO][0m - recompute                     : False[0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - run_name                      : ./models/fidelity_save_100/[0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - save_sharded_model            : False[0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - save_steps                    : 100[0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - save_total_limit              : 1[0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - seed                          : 1000[0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - sharding                      : [][0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2023-11-10 11:19:45,321] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - should_log                    : True[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - should_save                   : True[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - train_batch_size              : 6[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - weight_decay                  : 0.05[0m
[32m[2023-11-10 11:19:45,322] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2023-11-10 11:19:45,323] [    INFO][0m - world_size                    : 4[0m
[32m[2023-11-10 11:19:45,323] [    INFO][0m - [0m
[32m[2023-11-10 11:19:45,323] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id. If tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:19:51,609] [    INFO][0m - ***** Running training *****[0m
[32m[2023-11-10 11:19:51,609] [    INFO][0m -   Num examples = 13,978[0m
[32m[2023-11-10 11:19:51,609] [    INFO][0m -   Num Epochs = 4[0m
[32m[2023-11-10 11:19:51,609] [    INFO][0m -   Instantaneous batch size per device = 6[0m
[32m[2023-11-10 11:19:51,609] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 24[0m
[32m[2023-11-10 11:19:51,609] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-11-10 11:19:51,609] [    INFO][0m -   Total optimization steps = 2,332[0m
[32m[2023-11-10 11:19:51,609] [    INFO][0m -   Total num train samples = 55,912[0m
[32m[2023-11-10 11:19:51,611] [    INFO][0m -   Number of trainable parameters = 281,693,122 (per device)[0m
  0%|          | 0/2332 [00:00<?, ?it/s]  0%|          | 1/2332 [00:04<3:09:54,  4.89s/it]  0%|          | 2/2332 [00:06<1:50:43,  2.85s/it]  0%|          | 3/2332 [00:07<1:25:41,  2.21s/it]  0%|          | 4/2332 [00:09<1:13:50,  1.90s/it]  0%|          | 5/2332 [00:10<1:07:09,  1.73s/it]  0%|          | 6/2332 [00:12<1:03:10,  1.63s/it]  0%|          | 7/2332 [00:13<1:00:41,  1.57s/it]  0%|          | 8/2332 [00:14<59:03,  1.52s/it]    0%|          | 9/2332 [00:16<58:01,  1.50s/it]  0%|          | 10/2332 [00:17<57:10,  1.48s/it]  0%|          | 11/2332 [00:19<56:39,  1.46s/it]  1%|          | 12/2332 [00:20<56:17,  1.46s/it]  1%|          | 13/2332 [00:22<56:02,  1.45s/it]  1%|          | 14/2332 [00:23<55:45,  1.44s/it]  1%|          | 15/2332 [00:24<55:36,  1.44s/it]  1%|          | 16/2332 [00:26<55:34,  1.44s/it]  1%|          | 17/2332 [00:27<55:27,  1.44s/it]  1%|          | 18/2332 [00:29<55:16,  1.43s/it]  1%|          | 19/2332 [00:30<55:16,  1.43s/it]  1%|          | 20/2332 [00:32<55:19,  1.44s/it]  1%|          | 21/2332 [00:33<55:20,  1.44s/it]  1%|          | 22/2332 [00:35<55:24,  1.44s/it]  1%|          | 23/2332 [00:36<55:18,  1.44s/it]  1%|          | 24/2332 [00:37<55:13,  1.44s/it]  1%|          | 25/2332 [00:39<55:11,  1.44s/it]  1%|          | 26/2332 [00:40<55:13,  1.44s/it]  1%|          | 27/2332 [00:42<55:16,  1.44s/it]  1%|          | 28/2332 [00:43<55:42,  1.45s/it]  1%|          | 29/2332 [00:45<55:25,  1.44s/it]  1%|▏         | 30/2332 [00:46<55:18,  1.44s/it]  1%|▏         | 31/2332 [00:47<55:22,  1.44s/it]  1%|▏         | 32/2332 [00:49<55:19,  1.44s/it]  1%|▏         | 33/2332 [00:50<55:17,  1.44s/it]  1%|▏         | 34/2332 [00:52<55:06,  1.44s/it]  2%|▏         | 35/2332 [00:53<55:04,  1.44s/it]  2%|▏         | 36/2332 [00:55<55:08,  1.44s/it]  2%|▏         | 37/2332 [00:56<55:05,  1.44s/it]  2%|▏         | 38/2332 [00:58<55:09,  1.44s/it]  2%|▏         | 39/2332 [00:59<55:13,  1.44s/it]  2%|▏         | 40/2332 [01:00<55:04,  1.44s/it]  2%|▏         | 41/2332 [01:02<55:03,  1.44s/it]  2%|▏         | 42/2332 [01:03<55:03,  1.44s/it]  2%|▏         | 43/2332 [01:05<54:59,  1.44s/it]  2%|▏         | 44/2332 [01:06<55:00,  1.44s/it]  2%|▏         | 45/2332 [01:08<56:01,  1.47s/it]  2%|▏         | 46/2332 [01:09<55:48,  1.47s/it]  2%|▏         | 47/2332 [01:11<55:34,  1.46s/it]  2%|▏         | 48/2332 [01:12<55:19,  1.45s/it]  2%|▏         | 49/2332 [01:14<55:09,  1.45s/it]  2%|▏         | 50/2332 [01:15<55:03,  1.45s/it]  2%|▏         | 51/2332 [01:16<55:01,  1.45s/it]  2%|▏         | 52/2332 [01:18<54:58,  1.45s/it]  2%|▏         | 53/2332 [01:19<54:47,  1.44s/it]  2%|▏         | 54/2332 [01:21<56:07,  1.48s/it]  2%|▏         | 55/2332 [01:22<55:41,  1.47s/it]  2%|▏         | 56/2332 [01:24<55:22,  1.46s/it]  2%|▏         | 57/2332 [01:25<55:11,  1.46s/it]  2%|▏         | 58/2332 [01:27<55:00,  1.45s/it]  3%|▎         | 59/2332 [01:28<54:56,  1.45s/it]  3%|▎         | 60/2332 [01:30<54:49,  1.45s/it]  3%|▎         | 61/2332 [01:31<54:49,  1.45s/it]  3%|▎         | 62/2332 [01:32<54:44,  1.45s/it]  3%|▎         | 63/2332 [01:34<54:40,  1.45s/it]  3%|▎         | 64/2332 [01:35<54:28,  1.44s/it]  3%|▎         | 65/2332 [01:37<54:24,  1.44s/it]  3%|▎         | 66/2332 [01:38<54:27,  1.44s/it]  3%|▎         | 67/2332 [01:40<54:24,  1.44s/it]  3%|▎         | 68/2332 [01:41<54:26,  1.44s/it]  3%|▎         | 69/2332 [01:43<54:23,  1.44s/it]  3%|▎         | 70/2332 [01:44<54:22,  1.44s/it]  3%|▎         | 71/2332 [01:45<55:06,  1.46s/it]  3%|▎         | 72/2332 [01:47<54:49,  1.46s/it]  3%|▎         | 73/2332 [01:48<54:40,  1.45s/it]  3%|▎         | 74/2332 [01:50<54:31,  1.45s/it]  3%|▎         | 75/2332 [01:51<54:31,  1.45s/it]  3%|▎         | 76/2332 [01:53<54:30,  1.45s/it]  3%|▎         | 77/2332 [01:54<54:18,  1.45s/it]  3%|▎         | 78/2332 [01:56<54:17,  1.45s/it]  3%|▎         | 79/2332 [01:57<54:12,  1.44s/it]  3%|▎         | 80/2332 [01:58<54:15,  1.45s/it]  3%|▎         | 81/2332 [02:00<54:20,  1.45s/it]  4%|▎         | 82/2332 [02:01<54:19,  1.45s/it]  4%|▎         | 83/2332 [02:03<54:16,  1.45s/it]  4%|▎         | 84/2332 [02:04<55:23,  1.48s/it]  4%|▎         | 85/2332 [02:06<55:06,  1.47s/it]  4%|▎         | 86/2332 [02:07<54:54,  1.47s/it]  4%|▎         | 87/2332 [02:09<54:43,  1.46s/it]  4%|▍         | 88/2332 [02:10<54:20,  1.45s/it]  4%|▍         | 89/2332 [02:12<54:17,  1.45s/it]  4%|▍         | 90/2332 [02:13<54:17,  1.45s/it]  4%|▍         | 91/2332 [02:15<54:15,  1.45s/it]  4%|▍         | 92/2332 [02:16<54:16,  1.45s/it]  4%|▍         | 93/2332 [02:17<54:14,  1.45s/it]  4%|▍         | 94/2332 [02:19<54:08,  1.45s/it]  4%|▍         | 95/2332 [02:20<53:57,  1.45s/it]  4%|▍         | 96/2332 [02:22<54:31,  1.46s/it]  4%|▍         | 97/2332 [02:23<54:22,  1.46s/it]  4%|▍         | 98/2332 [02:25<54:10,  1.46s/it]  4%|▍         | 99/2332 [02:26<54:04,  1.45s/it]  4%|▍         | 100/2332 [02:28<53:52,  1.45s/it][32m[2023-11-10 11:22:19,703] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id. If tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:22:20,153] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:22:20,153] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:22:20,153] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:22:20,153] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:22:20,153] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.85it/s][A
  8%|▊         | 3/36 [00:01<00:16,  2.00it/s][A
 11%|█         | 4/36 [00:02<00:18,  1.74it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.61it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.54it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.50it/s][A
 22%|██▏       | 8/36 [00:04<00:19,  1.47it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.46it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.44it/s][A
 31%|███       | 11/36 [00:07<00:17,  1.44it/s][A
 33%|███▎      | 12/36 [00:07<00:17,  1.36it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.38it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.39it/s][A
 42%|████▏     | 15/36 [00:09<00:15,  1.40it/s][A
 44%|████▍     | 16/36 [00:10<00:14,  1.40it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.40it/s][A
 50%|█████     | 18/36 [00:12<00:12,  1.40it/s][A
 53%|█████▎    | 19/36 [00:12<00:12,  1.41it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.41it/s][A
 58%|█████▊    | 21/36 [00:14<00:10,  1.41it/s][A
 61%|██████    | 22/36 [00:14<00:09,  1.41it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.41it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.42it/s][A
 69%|██████▉   | 25/36 [00:17<00:07,  1.42it/s][A
 72%|███████▏  | 26/36 [00:17<00:07,  1.42it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.42it/s][A
 78%|███████▊  | 28/36 [00:19<00:05,  1.37it/s][A
 81%|████████  | 29/36 [00:19<00:05,  1.37it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.39it/s][A
 86%|████████▌ | 31/36 [00:21<00:03,  1.52it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.82it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.11it/s][A
 94%|█████████▍| 34/36 [00:22<00:00,  2.37it/s][A
 97%|█████████▋| 35/36 [00:22<00:00,  2.61it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.18it/s][A                                                  
                                               [Aeval_anls: 8.735642729182835, epoch: 0.1715
  4%|▍         | 100/2332 [02:57<53:52,  1.45s/it]
100%|██████████| 36/36 [00:25<00:00,  3.18it/s][A
                                               [A[32m[2023-11-10 11:22:48,819] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-100[0m
[32m[2023-11-10 11:22:48,828] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-100/config.json[0m
[32m[2023-11-10 11:22:54,750] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-100/model_state.pdparams[0m
[32m[2023-11-10 11:22:54,750] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-100/tokenizer_config.json[0m
[32m[2023-11-10 11:22:54,750] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-100/special_tokens_map.json[0m
[32m[2023-11-10 11:23:07,590] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-1100] due to args.save_total_limit[0m
  4%|▍         | 101/2332 [03:17<9:52:29, 15.93s/it]  4%|▍         | 102/2332 [03:19<7:10:30, 11.58s/it]  4%|▍         | 103/2332 [03:20<5:18:40,  8.58s/it]  4%|▍         | 104/2332 [03:22<3:59:53,  6.46s/it]  5%|▍         | 105/2332 [03:23<3:03:54,  4.95s/it]  5%|▍         | 106/2332 [03:25<2:24:37,  3.90s/it]  5%|▍         | 107/2332 [03:26<1:57:07,  3.16s/it]  5%|▍         | 108/2332 [03:28<1:37:54,  2.64s/it]  5%|▍         | 109/2332 [03:29<1:24:28,  2.28s/it]  5%|▍         | 110/2332 [03:30<1:15:08,  2.03s/it]  5%|▍         | 111/2332 [03:32<1:08:31,  1.85s/it]  5%|▍         | 112/2332 [03:33<1:03:53,  1.73s/it]  5%|▍         | 113/2332 [03:35<1:00:46,  1.64s/it]  5%|▍         | 114/2332 [03:36<58:23,  1.58s/it]    5%|▍         | 115/2332 [03:38<56:46,  1.54s/it]  5%|▍         | 116/2332 [03:39<55:32,  1.50s/it]  5%|▌         | 117/2332 [03:41<54:56,  1.49s/it]  5%|▌         | 118/2332 [03:42<54:22,  1.47s/it]  5%|▌         | 119/2332 [03:43<54:07,  1.47s/it]  5%|▌         | 120/2332 [03:45<53:52,  1.46s/it]  5%|▌         | 121/2332 [03:46<54:25,  1.48s/it]  5%|▌         | 122/2332 [03:48<53:51,  1.46s/it]  5%|▌         | 123/2332 [03:49<53:35,  1.46s/it]  5%|▌         | 124/2332 [03:51<53:18,  1.45s/it]  5%|▌         | 125/2332 [03:52<53:05,  1.44s/it]  5%|▌         | 126/2332 [03:54<52:54,  1.44s/it]  5%|▌         | 127/2332 [03:55<52:50,  1.44s/it]  5%|▌         | 128/2332 [03:57<54:09,  1.47s/it]  6%|▌         | 129/2332 [03:58<54:38,  1.49s/it]  6%|▌         | 130/2332 [03:59<54:04,  1.47s/it]  6%|▌         | 131/2332 [04:01<53:41,  1.46s/it]  6%|▌         | 132/2332 [04:02<53:15,  1.45s/it]  6%|▌         | 133/2332 [04:04<53:04,  1.45s/it]  6%|▌         | 134/2332 [04:05<52:49,  1.44s/it]  6%|▌         | 135/2332 [04:07<52:45,  1.44s/it]  6%|▌         | 136/2332 [04:08<52:45,  1.44s/it]  6%|▌         | 137/2332 [04:10<52:51,  1.44s/it]  6%|▌         | 138/2332 [04:11<52:41,  1.44s/it]  6%|▌         | 139/2332 [04:12<52:40,  1.44s/it]  6%|▌         | 140/2332 [04:14<52:35,  1.44s/it]  6%|▌         | 141/2332 [04:15<52:34,  1.44s/it]  6%|▌         | 142/2332 [04:17<52:30,  1.44s/it]  6%|▌         | 143/2332 [04:18<52:34,  1.44s/it]  6%|▌         | 144/2332 [04:20<52:30,  1.44s/it]  6%|▌         | 145/2332 [04:21<52:37,  1.44s/it]  6%|▋         | 146/2332 [04:23<53:36,  1.47s/it]  6%|▋         | 147/2332 [04:24<53:17,  1.46s/it]  6%|▋         | 148/2332 [04:26<53:04,  1.46s/it]  6%|▋         | 149/2332 [04:27<52:54,  1.45s/it]  6%|▋         | 150/2332 [04:28<52:51,  1.45s/it]  6%|▋         | 151/2332 [04:30<52:47,  1.45s/it]  7%|▋         | 152/2332 [04:31<52:31,  1.45s/it]  7%|▋         | 153/2332 [04:33<53:44,  1.48s/it]  7%|▋         | 154/2332 [04:34<54:10,  1.49s/it]  7%|▋         | 155/2332 [04:36<53:33,  1.48s/it]  7%|▋         | 156/2332 [04:37<53:14,  1.47s/it]  7%|▋         | 157/2332 [04:39<52:47,  1.46s/it]  7%|▋         | 158/2332 [04:40<52:44,  1.46s/it]  7%|▋         | 159/2332 [04:42<52:30,  1.45s/it]  7%|▋         | 160/2332 [04:43<52:29,  1.45s/it]  7%|▋         | 161/2332 [04:44<52:19,  1.45s/it]  7%|▋         | 162/2332 [04:46<52:14,  1.44s/it]  7%|▋         | 163/2332 [04:47<52:10,  1.44s/it]  7%|▋         | 164/2332 [04:49<52:02,  1.44s/it]  7%|▋         | 165/2332 [04:50<51:56,  1.44s/it]  7%|▋         | 166/2332 [04:52<52:08,  1.44s/it]  7%|▋         | 167/2332 [04:53<52:04,  1.44s/it]  7%|▋         | 168/2332 [04:55<51:58,  1.44s/it]  7%|▋         | 169/2332 [04:56<51:53,  1.44s/it]  7%|▋         | 170/2332 [04:57<51:58,  1.44s/it]  7%|▋         | 171/2332 [04:59<51:57,  1.44s/it]  7%|▋         | 172/2332 [05:00<52:40,  1.46s/it]  7%|▋         | 173/2332 [05:02<52:18,  1.45s/it]  7%|▋         | 174/2332 [05:03<52:12,  1.45s/it]  8%|▊         | 175/2332 [05:05<52:04,  1.45s/it]  8%|▊         | 176/2332 [05:06<52:05,  1.45s/it]  8%|▊         | 177/2332 [05:08<51:55,  1.45s/it]  8%|▊         | 178/2332 [05:09<53:14,  1.48s/it]  8%|▊         | 179/2332 [05:11<53:40,  1.50s/it]  8%|▊         | 180/2332 [05:12<52:57,  1.48s/it]  8%|▊         | 181/2332 [05:14<52:38,  1.47s/it]  8%|▊         | 182/2332 [05:15<52:21,  1.46s/it]  8%|▊         | 183/2332 [05:16<52:13,  1.46s/it]  8%|▊         | 184/2332 [05:18<51:52,  1.45s/it]  8%|▊         | 185/2332 [05:19<51:52,  1.45s/it]  8%|▊         | 186/2332 [05:21<51:41,  1.45s/it]  8%|▊         | 187/2332 [05:22<51:37,  1.44s/it]  8%|▊         | 188/2332 [05:24<51:34,  1.44s/it]  8%|▊         | 189/2332 [05:25<51:31,  1.44s/it]  8%|▊         | 190/2332 [05:27<51:32,  1.44s/it]  8%|▊         | 191/2332 [05:28<51:37,  1.45s/it]  8%|▊         | 192/2332 [05:29<51:38,  1.45s/it]  8%|▊         | 193/2332 [05:31<51:39,  1.45s/it]  8%|▊         | 194/2332 [05:32<51:29,  1.44s/it]  8%|▊         | 195/2332 [05:34<51:34,  1.45s/it]  8%|▊         | 196/2332 [05:35<51:32,  1.45s/it]  8%|▊         | 197/2332 [05:37<51:33,  1.45s/it]  8%|▊         | 198/2332 [05:38<52:18,  1.47s/it]  9%|▊         | 199/2332 [05:40<51:58,  1.46s/it]  9%|▊         | 200/2332 [05:41<51:39,  1.45s/it][32m[2023-11-10 11:25:33,209] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id. If tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:25:33,540] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:25:33,540] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:25:33,540] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:25:33,540] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:25:33,540] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:12,  2.81it/s][A
  8%|▊         | 3/36 [00:01<00:18,  1.81it/s][A
 11%|█         | 4/36 [00:02<00:19,  1.64it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.56it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.51it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.47it/s][A
 22%|██▏       | 8/36 [00:05<00:20,  1.39it/s][A
 25%|██▌       | 9/36 [00:05<00:19,  1.39it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.40it/s][A
 31%|███       | 11/36 [00:07<00:17,  1.41it/s][A
 33%|███▎      | 12/36 [00:08<00:17,  1.41it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.41it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.41it/s][A
 42%|████▏     | 15/36 [00:10<00:14,  1.41it/s][A
 44%|████▍     | 16/36 [00:10<00:14,  1.41it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.41it/s][A
 50%|█████     | 18/36 [00:12<00:12,  1.40it/s][A
 53%|█████▎    | 19/36 [00:12<00:12,  1.40it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.40it/s][A
 58%|█████▊    | 21/36 [00:14<00:10,  1.40it/s][A
 61%|██████    | 22/36 [00:15<00:09,  1.41it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.41it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.40it/s][A
 69%|██████▉   | 25/36 [00:17<00:07,  1.41it/s][A
 72%|███████▏  | 26/36 [00:18<00:07,  1.36it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.37it/s][A
 78%|███████▊  | 28/36 [00:19<00:05,  1.38it/s][A
 81%|████████  | 29/36 [00:20<00:05,  1.39it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.40it/s][A
 86%|████████▌ | 31/36 [00:21<00:03,  1.54it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.84it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.13it/s][A
 94%|█████████▍| 34/36 [00:22<00:00,  2.39it/s][A
 97%|█████████▋| 35/36 [00:22<00:00,  2.61it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.19it/s][A                                                  
                                               [Aeval_anls: 45.67701654937478, epoch: 0.3431
  9%|▊         | 200/2332 [06:08<51:39,  1.45s/it]
100%|██████████| 36/36 [00:23<00:00,  3.19it/s][A
                                               [A[32m[2023-11-10 11:25:59,623] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-200[0m
[32m[2023-11-10 11:25:59,632] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-200/config.json[0m
[32m[2023-11-10 11:26:05,555] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-200/model_state.pdparams[0m
[32m[2023-11-10 11:26:05,555] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-200/tokenizer_config.json[0m
[32m[2023-11-10 11:26:05,556] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-200/special_tokens_map.json[0m
  9%|▊         | 201/2332 [06:28<8:52:12, 14.98s/it]  9%|▊         | 202/2332 [06:29<6:27:38, 10.92s/it]  9%|▊         | 203/2332 [06:31<4:47:48,  8.11s/it]  9%|▊         | 204/2332 [06:32<3:36:50,  6.11s/it]  9%|▉         | 205/2332 [06:34<2:47:06,  4.71s/it]  9%|▉         | 206/2332 [06:35<2:13:07,  3.76s/it]  9%|▉         | 207/2332 [06:37<1:48:28,  3.06s/it]  9%|▉         | 208/2332 [06:38<1:31:05,  2.57s/it]  9%|▉         | 209/2332 [06:39<1:19:09,  2.24s/it]  9%|▉         | 210/2332 [06:41<1:10:29,  1.99s/it]  9%|▉         | 211/2332 [06:42<1:04:36,  1.83s/it]  9%|▉         | 212/2332 [06:44<1:00:22,  1.71s/it]  9%|▉         | 213/2332 [06:45<57:36,  1.63s/it]    9%|▉         | 214/2332 [06:47<55:29,  1.57s/it]  9%|▉         | 215/2332 [06:48<54:10,  1.54s/it]  9%|▉         | 216/2332 [06:49<53:00,  1.50s/it]  9%|▉         | 217/2332 [06:51<52:13,  1.48s/it]  9%|▉         | 218/2332 [06:52<51:45,  1.47s/it]  9%|▉         | 219/2332 [06:54<51:30,  1.46s/it]  9%|▉         | 220/2332 [06:55<51:22,  1.46s/it]  9%|▉         | 221/2332 [06:57<51:57,  1.48s/it] 10%|▉         | 222/2332 [06:58<51:39,  1.47s/it] 10%|▉         | 223/2332 [07:00<51:23,  1.46s/it] 10%|▉         | 224/2332 [07:01<51:06,  1.45s/it] 10%|▉         | 225/2332 [07:03<51:00,  1.45s/it] 10%|▉         | 226/2332 [07:04<50:44,  1.45s/it] 10%|▉         | 227/2332 [07:05<50:39,  1.44s/it] 10%|▉         | 228/2332 [07:07<51:47,  1.48s/it] 10%|▉         | 229/2332 [07:08<51:25,  1.47s/it] 10%|▉         | 230/2332 [07:10<51:07,  1.46s/it] 10%|▉         | 231/2332 [07:11<51:48,  1.48s/it] 10%|▉         | 232/2332 [07:13<51:24,  1.47s/it] 10%|▉         | 233/2332 [07:14<51:13,  1.46s/it] 10%|█         | 234/2332 [07:16<50:59,  1.46s/it] 10%|█         | 235/2332 [07:17<50:46,  1.45s/it] 10%|█         | 236/2332 [07:19<50:44,  1.45s/it] 10%|█         | 237/2332 [07:20<50:31,  1.45s/it] 10%|█         | 238/2332 [07:21<50:29,  1.45s/it] 10%|█         | 239/2332 [07:23<50:22,  1.44s/it] 10%|█         | 240/2332 [07:24<50:16,  1.44s/it] 10%|█         | 241/2332 [07:26<50:21,  1.44s/it] 10%|█         | 242/2332 [07:27<50:22,  1.45s/it] 10%|█         | 243/2332 [07:29<50:20,  1.45s/it] 10%|█         | 244/2332 [07:30<50:14,  1.44s/it] 11%|█         | 245/2332 [07:32<50:19,  1.45s/it] 11%|█         | 246/2332 [07:33<51:07,  1.47s/it] 11%|█         | 247/2332 [07:35<50:45,  1.46s/it] 11%|█         | 248/2332 [07:36<50:39,  1.46s/it] 11%|█         | 249/2332 [07:37<50:33,  1.46s/it] 11%|█         | 250/2332 [07:39<50:28,  1.45s/it] 11%|█         | 251/2332 [07:40<50:13,  1.45s/it] 11%|█         | 252/2332 [07:42<50:00,  1.44s/it] 11%|█         | 253/2332 [07:43<51:11,  1.48s/it] 11%|█         | 254/2332 [07:45<50:55,  1.47s/it] 11%|█         | 255/2332 [07:46<50:37,  1.46s/it] 11%|█         | 256/2332 [07:48<50:31,  1.46s/it] 11%|█         | 257/2332 [07:49<51:10,  1.48s/it] 11%|█         | 258/2332 [07:51<50:42,  1.47s/it] 11%|█         | 259/2332 [07:52<50:34,  1.46s/it] 11%|█         | 260/2332 [07:54<50:20,  1.46s/it] 11%|█         | 261/2332 [07:55<50:02,  1.45s/it] 11%|█         | 262/2332 [07:56<50:01,  1.45s/it] 11%|█▏        | 263/2332 [07:58<49:53,  1.45s/it] 11%|█▏        | 264/2332 [07:59<49:55,  1.45s/it] 11%|█▏        | 265/2332 [08:01<49:56,  1.45s/it] 11%|█▏        | 266/2332 [08:02<49:59,  1.45s/it] 11%|█▏        | 267/2332 [08:04<49:55,  1.45s/it] 11%|█▏        | 268/2332 [08:05<49:52,  1.45s/it] 12%|█▏        | 269/2332 [08:07<49:42,  1.45s/it] 12%|█▏        | 270/2332 [08:08<49:45,  1.45s/it] 12%|█▏        | 271/2332 [08:09<49:35,  1.44s/it] 12%|█▏        | 272/2332 [08:11<50:15,  1.46s/it] 12%|█▏        | 273/2332 [08:12<50:07,  1.46s/it] 12%|█▏        | 274/2332 [08:14<50:01,  1.46s/it] 12%|█▏        | 275/2332 [08:15<49:47,  1.45s/it] 12%|█▏        | 276/2332 [08:17<49:42,  1.45s/it] 12%|█▏        | 277/2332 [08:18<49:32,  1.45s/it] 12%|█▏        | 278/2332 [08:20<50:51,  1.49s/it] 12%|█▏        | 279/2332 [08:21<50:29,  1.48s/it] 12%|█▏        | 280/2332 [08:23<50:14,  1.47s/it] 12%|█▏        | 281/2332 [08:24<50:02,  1.46s/it] 12%|█▏        | 282/2332 [08:26<50:31,  1.48s/it] 12%|█▏        | 283/2332 [08:27<50:10,  1.47s/it] 12%|█▏        | 284/2332 [08:29<49:51,  1.46s/it] 12%|█▏        | 285/2332 [08:30<49:44,  1.46s/it] 12%|█▏        | 286/2332 [08:31<49:40,  1.46s/it] 12%|█▏        | 287/2332 [08:33<49:38,  1.46s/it] 12%|█▏        | 288/2332 [08:34<49:35,  1.46s/it] 12%|█▏        | 289/2332 [08:36<49:28,  1.45s/it] 12%|█▏        | 290/2332 [08:37<49:27,  1.45s/it] 12%|█▏        | 291/2332 [08:39<49:27,  1.45s/it] 13%|█▎        | 292/2332 [08:40<49:19,  1.45s/it] 13%|█▎        | 293/2332 [08:42<49:14,  1.45s/it] 13%|█▎        | 294/2332 [08:43<49:14,  1.45s/it] 13%|█▎        | 295/2332 [08:44<49:14,  1.45s/it] 13%|█▎        | 296/2332 [08:46<49:12,  1.45s/it] 13%|█▎        | 297/2332 [08:47<49:13,  1.45s/it] 13%|█▎        | 298/2332 [08:49<49:57,  1.47s/it] 13%|█▎        | 299/2332 [08:50<49:48,  1.47s/it] 13%|█▎        | 300/2332 [08:52<49:34,  1.46s/it][32m[2023-11-10 11:28:43,935] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id. If tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:28:44,259] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:28:44,259] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:28:44,259] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:28:44,259] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:28:44,259] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:12,  2.80it/s][A
  8%|▊         | 3/36 [00:01<00:18,  1.81it/s][A
 11%|█         | 4/36 [00:02<00:19,  1.64it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.55it/s][A
 17%|█▋        | 6/36 [00:03<00:20,  1.49it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.47it/s][A
 22%|██▏       | 8/36 [00:05<00:20,  1.40it/s][A
 25%|██▌       | 9/36 [00:05<00:19,  1.40it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.39it/s][A
 31%|███       | 11/36 [00:07<00:17,  1.40it/s][A
 33%|███▎      | 12/36 [00:08<00:17,  1.40it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.41it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.41it/s][A
 42%|████▏     | 15/36 [00:10<00:14,  1.41it/s][A
 44%|████▍     | 16/36 [00:10<00:14,  1.41it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.41it/s][A
 50%|█████     | 18/36 [00:12<00:12,  1.40it/s][A
 53%|█████▎    | 19/36 [00:12<00:12,  1.40it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.41it/s][A
 58%|█████▊    | 21/36 [00:14<00:10,  1.41it/s][A
 61%|██████    | 22/36 [00:15<00:09,  1.41it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.41it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.42it/s][A
 69%|██████▉   | 25/36 [00:17<00:07,  1.42it/s][A
 72%|███████▏  | 26/36 [00:18<00:07,  1.36it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.38it/s][A
 78%|███████▊  | 28/36 [00:19<00:05,  1.39it/s][A
 81%|████████  | 29/36 [00:20<00:05,  1.40it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.40it/s][A
 86%|████████▌ | 31/36 [00:21<00:03,  1.54it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.83it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.12it/s][A
 94%|█████████▍| 34/36 [00:22<00:00,  2.38it/s][A
 97%|█████████▋| 35/36 [00:22<00:00,  2.61it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.19it/s][A                                                  
                                               [Aeval_anls: 49.320439274872854, epoch: 0.5146
 13%|█▎        | 300/2332 [09:18<49:34,  1.46s/it]
100%|██████████| 36/36 [00:23<00:00,  3.19it/s][A
                                               [A[32m[2023-11-10 11:29:10,350] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-300[0m
[32m[2023-11-10 11:29:10,359] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-300/config.json[0m
[32m[2023-11-10 11:29:12,881] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-300/model_state.pdparams[0m
[32m[2023-11-10 11:29:12,881] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-300/tokenizer_config.json[0m
[32m[2023-11-10 11:29:12,882] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-300/special_tokens_map.json[0m
[32m[2023-11-10 11:29:17,877] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-100] due to args.save_total_limit[0m
 13%|█▎        | 301/2332 [09:28<6:37:50, 11.75s/it] 13%|█▎        | 302/2332 [09:29<4:52:56,  8.66s/it] 13%|█▎        | 303/2332 [09:31<3:40:46,  6.53s/it] 13%|█▎        | 304/2332 [09:32<2:49:01,  5.00s/it] 13%|█▎        | 305/2332 [09:33<2:12:59,  3.94s/it] 13%|█▎        | 306/2332 [09:35<1:48:19,  3.21s/it] 13%|█▎        | 307/2332 [09:36<1:30:17,  2.68s/it] 13%|█▎        | 308/2332 [09:38<1:17:44,  2.30s/it] 13%|█▎        | 309/2332 [09:39<1:09:05,  2.05s/it] 13%|█▎        | 310/2332 [09:41<1:02:48,  1.86s/it] 13%|█▎        | 311/2332 [09:42<58:30,  1.74s/it]   13%|█▎        | 312/2332 [09:44<55:27,  1.65s/it] 13%|█▎        | 313/2332 [09:45<53:26,  1.59s/it] 13%|█▎        | 314/2332 [09:47<52:05,  1.55s/it] 14%|█▎        | 315/2332 [09:48<50:56,  1.52s/it] 14%|█▎        | 316/2332 [09:49<50:10,  1.49s/it] 14%|█▎        | 317/2332 [09:51<49:35,  1.48s/it] 14%|█▎        | 318/2332 [09:52<49:09,  1.46s/it] 14%|█▎        | 319/2332 [09:54<48:55,  1.46s/it] 14%|█▎        | 320/2332 [09:55<48:44,  1.45s/it] 14%|█▍        | 321/2332 [09:57<49:23,  1.47s/it] 14%|█▍        | 322/2332 [09:58<49:02,  1.46s/it] 14%|█▍        | 323/2332 [10:00<48:45,  1.46s/it] 14%|█▍        | 324/2332 [10:01<48:35,  1.45s/it] 14%|█▍        | 325/2332 [10:02<48:21,  1.45s/it] 14%|█▍        | 326/2332 [10:04<48:30,  1.45s/it] 14%|█▍        | 327/2332 [10:05<48:25,  1.45s/it] 14%|█▍        | 328/2332 [10:07<49:31,  1.48s/it] 14%|█▍        | 329/2332 [10:08<49:13,  1.47s/it] 14%|█▍        | 330/2332 [10:10<48:58,  1.47s/it] 14%|█▍        | 331/2332 [10:11<49:33,  1.49s/it] 14%|█▍        | 332/2332 [10:13<49:09,  1.47s/it] 14%|█▍        | 333/2332 [10:14<48:48,  1.46s/it] 14%|█▍        | 334/2332 [10:16<48:37,  1.46s/it] 14%|█▍        | 335/2332 [10:17<48:33,  1.46s/it] 14%|█▍        | 336/2332 [10:19<48:27,  1.46s/it] 14%|█▍        | 337/2332 [10:20<48:23,  1.46s/it] 14%|█▍        | 338/2332 [10:21<48:18,  1.45s/it] 15%|█▍        | 339/2332 [10:23<48:06,  1.45s/it] 15%|█▍        | 340/2332 [10:24<48:03,  1.45s/it] 15%|█▍        | 341/2332 [10:26<47:58,  1.45s/it] 15%|█▍        | 342/2332 [10:27<47:59,  1.45s/it] 15%|█▍        | 343/2332 [10:29<47:56,  1.45s/it] 15%|█▍        | 344/2332 [10:30<47:48,  1.44s/it] 15%|█▍        | 345/2332 [10:32<47:43,  1.44s/it] 15%|█▍        | 346/2332 [10:33<48:34,  1.47s/it] 15%|█▍        | 347/2332 [10:35<48:17,  1.46s/it] 15%|█▍        | 348/2332 [10:36<48:07,  1.46s/it] 15%|█▍        | 349/2332 [10:37<48:03,  1.45s/it] 15%|█▌        | 350/2332 [10:39<48:03,  1.45s/it] 15%|█▌        | 351/2332 [10:40<47:52,  1.45s/it] 15%|█▌        | 352/2332 [10:42<47:49,  1.45s/it] 15%|█▌        | 353/2332 [10:43<48:50,  1.48s/it] 15%|█▌        | 354/2332 [10:45<48:33,  1.47s/it] 15%|█▌        | 355/2332 [10:46<48:15,  1.46s/it] 15%|█▌        | 356/2332 [10:48<47:53,  1.45s/it] 15%|█▌        | 357/2332 [10:49<48:33,  1.48s/it] 15%|█▌        | 358/2332 [10:51<48:06,  1.46s/it] 15%|█▌        | 359/2332 [10:52<47:59,  1.46s/it] 15%|█▌        | 360/2332 [10:54<47:48,  1.45s/it] 15%|█▌        | 361/2332 [10:55<47:46,  1.45s/it] 16%|█▌        | 362/2332 [10:56<47:43,  1.45s/it] 16%|█▌        | 363/2332 [10:58<47:42,  1.45s/it] 16%|█▌        | 364/2332 [10:59<47:29,  1.45s/it] 16%|█▌        | 365/2332 [11:01<47:33,  1.45s/it] 16%|█▌        | 366/2332 [11:02<47:26,  1.45s/it] 16%|█▌        | 367/2332 [11:04<47:21,  1.45s/it] 16%|█▌        | 368/2332 [11:05<47:24,  1.45s/it] 16%|█▌        | 369/2332 [11:07<47:28,  1.45s/it] 16%|█▌        | 370/2332 [11:08<47:24,  1.45s/it] 16%|█▌        | 371/2332 [11:09<47:22,  1.45s/it] 16%|█▌        | 372/2332 [11:11<48:05,  1.47s/it] 16%|█▌        | 373/2332 [11:12<47:47,  1.46s/it] 16%|█▌        | 374/2332 [11:14<47:40,  1.46s/it] 16%|█▌        | 375/2332 [11:15<47:28,  1.46s/it] 16%|█▌        | 376/2332 [11:17<47:19,  1.45s/it] 16%|█▌        | 377/2332 [11:18<47:20,  1.45s/it] 16%|█▌        | 378/2332 [11:20<48:27,  1.49s/it] 16%|█▋        | 379/2332 [11:21<48:07,  1.48s/it] 16%|█▋        | 380/2332 [11:23<47:46,  1.47s/it] 16%|█▋        | 381/2332 [11:24<47:39,  1.47s/it] 16%|█▋        | 382/2332 [11:26<48:04,  1.48s/it] 16%|█▋        | 383/2332 [11:27<47:44,  1.47s/it] 16%|█▋        | 384/2332 [11:29<47:36,  1.47s/it] 17%|█▋        | 385/2332 [11:30<47:26,  1.46s/it] 17%|█▋        | 386/2332 [11:31<47:18,  1.46s/it] 17%|█▋        | 387/2332 [11:33<47:03,  1.45s/it] 17%|█▋        | 388/2332 [11:34<47:02,  1.45s/it] 17%|█▋        | 389/2332 [11:36<46:52,  1.45s/it] 17%|█▋        | 390/2332 [11:37<46:54,  1.45s/it] 17%|█▋        | 391/2332 [11:39<46:53,  1.45s/it] 17%|█▋        | 392/2332 [11:40<46:55,  1.45s/it] 17%|█▋        | 393/2332 [11:42<46:41,  1.44s/it] 17%|█▋        | 394/2332 [11:43<46:33,  1.44s/it] 17%|█▋        | 395/2332 [11:44<46:27,  1.44s/it] 17%|█▋        | 396/2332 [11:46<46:26,  1.44s/it] 17%|█▋        | 397/2332 [11:47<46:28,  1.44s/it] 17%|█▋        | 398/2332 [11:49<47:12,  1.46s/it] 17%|█▋        | 399/2332 [11:50<46:59,  1.46s/it] 17%|█▋        | 400/2332 [11:52<46:50,  1.45s/it][32m[2023-11-10 11:31:43,866] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id. If tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:31:44,235] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:31:44,236] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:31:44,236] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:31:44,236] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:31:44,236] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:12,  2.82it/s][A
  8%|▊         | 3/36 [00:01<00:18,  1.80it/s][A
 11%|█         | 4/36 [00:02<00:19,  1.63it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.55it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.50it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.48it/s][A
 22%|██▏       | 8/36 [00:05<00:19,  1.40it/s][A
 25%|██▌       | 9/36 [00:05<00:19,  1.40it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.40it/s][A
 31%|███       | 11/36 [00:07<00:17,  1.41it/s][A
 33%|███▎      | 12/36 [00:07<00:17,  1.41it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.41it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.41it/s][A
 42%|████▏     | 15/36 [00:10<00:14,  1.41it/s][A
 44%|████▍     | 16/36 [00:10<00:14,  1.42it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.42it/s][A
 50%|█████     | 18/36 [00:12<00:12,  1.42it/s][A
 53%|█████▎    | 19/36 [00:12<00:11,  1.42it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.42it/s][A
 58%|█████▊    | 21/36 [00:14<00:10,  1.42it/s][A
 61%|██████    | 22/36 [00:15<00:09,  1.42it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.42it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.41it/s][A
 69%|██████▉   | 25/36 [00:17<00:07,  1.41it/s][A
 72%|███████▏  | 26/36 [00:17<00:07,  1.41it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.36it/s][A
 78%|███████▊  | 28/36 [00:19<00:05,  1.37it/s][A
 81%|████████  | 29/36 [00:20<00:05,  1.39it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.39it/s][A
 86%|████████▌ | 31/36 [00:21<00:03,  1.52it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.82it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.12it/s][A
 94%|█████████▍| 34/36 [00:22<00:00,  2.38it/s][A
 97%|█████████▋| 35/36 [00:22<00:00,  2.61it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.18it/s][A                                                  
                                               [Aeval_anls: 51.526410124573886, epoch: 0.6861
 17%|█▋        | 400/2332 [12:18<46:50,  1.45s/it]
100%|██████████| 36/36 [00:23<00:00,  3.18it/s][A
                                               [A[32m[2023-11-10 11:32:10,262] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-400[0m
[32m[2023-11-10 11:32:10,271] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-400/config.json[0m
[32m[2023-11-10 11:32:12,853] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-400/model_state.pdparams[0m
[32m[2023-11-10 11:32:12,854] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-400/tokenizer_config.json[0m
[32m[2023-11-10 11:32:12,854] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-400/special_tokens_map.json[0m
[32m[2023-11-10 11:32:17,969] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-200] due to args.save_total_limit[0m
 17%|█▋        | 401/2332 [12:28<6:19:54, 11.80s/it] 17%|█▋        | 402/2332 [12:29<4:39:48,  8.70s/it] 17%|█▋        | 403/2332 [12:31<3:30:44,  6.55s/it] 17%|█▋        | 404/2332 [12:32<2:41:22,  5.02s/it] 17%|█▋        | 405/2332 [12:34<2:06:42,  3.95s/it] 17%|█▋        | 406/2332 [12:35<1:43:16,  3.22s/it] 17%|█▋        | 407/2332 [12:37<1:26:14,  2.69s/it] 17%|█▋        | 408/2332 [12:38<1:14:15,  2.32s/it] 18%|█▊        | 409/2332 [12:39<1:05:47,  2.05s/it] 18%|█▊        | 410/2332 [12:41<59:59,  1.87s/it]   18%|█▊        | 411/2332 [12:42<55:48,  1.74s/it] 18%|█▊        | 412/2332 [12:44<52:53,  1.65s/it] 18%|█▊        | 413/2332 [12:45<50:56,  1.59s/it] 18%|█▊        | 414/2332 [12:47<49:27,  1.55s/it] 18%|█▊        | 415/2332 [12:48<48:31,  1.52s/it] 18%|█▊        | 416/2332 [12:50<47:45,  1.50s/it] 18%|█▊        | 417/2332 [12:51<47:14,  1.48s/it] 18%|█▊        | 418/2332 [12:52<46:58,  1.47s/it] 18%|█▊        | 419/2332 [12:54<46:45,  1.47s/it] 18%|█▊        | 420/2332 [12:55<46:35,  1.46s/it] 18%|█▊        | 421/2332 [12:57<46:16,  1.45s/it] 18%|█▊        | 422/2332 [12:58<46:05,  1.45s/it] 18%|█▊        | 423/2332 [13:00<46:07,  1.45s/it] 18%|█▊        | 424/2332 [13:01<46:42,  1.47s/it] 18%|█▊        | 425/2332 [13:03<46:31,  1.46s/it] 18%|█▊        | 426/2332 [13:04<46:21,  1.46s/it] 18%|█▊        | 427/2332 [13:06<46:12,  1.46s/it] 18%|█▊        | 428/2332 [13:07<47:14,  1.49s/it] 18%|█▊        | 429/2332 [13:09<46:50,  1.48s/it] 18%|█▊        | 430/2332 [13:10<46:37,  1.47s/it] 18%|█▊        | 431/2332 [13:12<47:07,  1.49s/it] 19%|█▊        | 432/2332 [13:13<46:45,  1.48s/it] 19%|█▊        | 433/2332 [13:14<46:23,  1.47s/it] 19%|█▊        | 434/2332 [13:16<46:12,  1.46s/it] 19%|█▊        | 435/2332 [13:17<46:01,  1.46s/it] 19%|█▊        | 436/2332 [13:19<45:53,  1.45s/it] 19%|█▊        | 437/2332 [13:20<45:51,  1.45s/it] 19%|█▉        | 438/2332 [13:22<45:47,  1.45s/it] 19%|█▉        | 439/2332 [13:23<45:42,  1.45s/it] 19%|█▉        | 440/2332 [13:25<45:37,  1.45s/it] 19%|█▉        | 441/2332 [13:26<45:35,  1.45s/it] 19%|█▉        | 442/2332 [13:27<45:29,  1.44s/it] 19%|█▉        | 443/2332 [13:29<45:34,  1.45s/it] 19%|█▉        | 444/2332 [13:30<45:35,  1.45s/it] 19%|█▉        | 445/2332 [13:32<45:36,  1.45s/it] 19%|█▉        | 446/2332 [13:33<45:33,  1.45s/it] 19%|█▉        | 447/2332 [13:35<45:26,  1.45s/it] 19%|█▉        | 448/2332 [13:36<45:28,  1.45s/it] 19%|█▉        | 449/2332 [13:38<45:26,  1.45s/it] 19%|█▉        | 450/2332 [13:39<46:09,  1.47s/it] 19%|█▉        | 451/2332 [13:41<45:49,  1.46s/it] 19%|█▉        | 452/2332 [13:42<45:28,  1.45s/it] 19%|█▉        | 453/2332 [13:44<46:31,  1.49s/it] 19%|█▉        | 454/2332 [13:45<46:12,  1.48s/it] 20%|█▉        | 455/2332 [13:46<45:55,  1.47s/it] 20%|█▉        | 456/2332 [13:48<45:37,  1.46s/it] 20%|█▉        | 457/2332 [13:49<46:13,  1.48s/it] 20%|█▉        | 458/2332 [13:51<45:47,  1.47s/it] 20%|█▉        | 459/2332 [13:52<45:40,  1.46s/it] 20%|█▉        | 460/2332 [13:54<45:34,  1.46s/it] 20%|█▉        | 461/2332 [13:55<45:25,  1.46s/it] 20%|█▉        | 462/2332 [13:57<45:15,  1.45s/it] 20%|█▉        | 463/2332 [13:58<45:13,  1.45s/it] 20%|█▉        | 464/2332 [14:00<45:01,  1.45s/it] 20%|█▉        | 465/2332 [14:01<45:03,  1.45s/it] 20%|█▉        | 466/2332 [14:02<44:56,  1.44s/it] 20%|██        | 467/2332 [14:04<44:55,  1.45s/it] 20%|██        | 468/2332 [14:05<44:50,  1.44s/it] 20%|██        | 469/2332 [14:07<44:54,  1.45s/it] 20%|██        | 470/2332 [14:08<44:47,  1.44s/it] 20%|██        | 471/2332 [14:10<44:50,  1.45s/it] 20%|██        | 472/2332 [14:11<44:48,  1.45s/it] 20%|██        | 473/2332 [14:13<44:44,  1.44s/it] 20%|██        | 474/2332 [14:14<44:47,  1.45s/it] 20%|██        | 475/2332 [14:16<45:24,  1.47s/it] 20%|██        | 476/2332 [14:17<45:15,  1.46s/it] 20%|██        | 477/2332 [14:18<45:06,  1.46s/it] 20%|██        | 478/2332 [14:20<45:58,  1.49s/it] 21%|██        | 479/2332 [14:21<45:37,  1.48s/it] 21%|██        | 480/2332 [14:23<45:24,  1.47s/it] 21%|██        | 481/2332 [14:24<45:08,  1.46s/it] 21%|██        | 482/2332 [14:26<45:39,  1.48s/it] 21%|██        | 483/2332 [14:27<45:18,  1.47s/it] 21%|██        | 484/2332 [14:29<45:08,  1.47s/it] 21%|██        | 485/2332 [14:30<45:00,  1.46s/it] 21%|██        | 486/2332 [14:32<44:53,  1.46s/it] 21%|██        | 487/2332 [14:33<44:49,  1.46s/it] 21%|██        | 488/2332 [14:35<44:43,  1.46s/it] 21%|██        | 489/2332 [14:36<44:41,  1.45s/it] 21%|██        | 490/2332 [14:37<44:38,  1.45s/it] 21%|██        | 491/2332 [14:39<44:37,  1.45s/it] 21%|██        | 492/2332 [14:40<44:36,  1.45s/it] 21%|██        | 493/2332 [14:42<44:30,  1.45s/it] 21%|██        | 494/2332 [14:43<44:33,  1.45s/it] 21%|██        | 495/2332 [14:45<44:28,  1.45s/it] 21%|██▏       | 496/2332 [14:46<44:25,  1.45s/it] 21%|██▏       | 497/2332 [14:48<44:24,  1.45s/it] 21%|██▏       | 498/2332 [14:49<44:25,  1.45s/it] 21%|██▏       | 499/2332 [14:51<44:16,  1.45s/it] 21%|██▏       | 500/2332 [14:52<45:03,  1.48s/it]                                                  loss: 2.54346558, learning_rate: 1.653e-05, global_step: 500, interval_runtime: 892.6242, interval_samples_per_second: 13.443507642963324, interval_steps_per_second: 0.5601461517901385, epoch: 0.8576
 21%|██▏       | 500/2332 [14:52<45:03,  1.48s/it][32m[2023-11-10 11:34:44,236] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id. If tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:34:44,607] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:34:44,607] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:34:44,607] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:34:44,607] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:34:44,607] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:12,  2.83it/s][A
  8%|▊         | 3/36 [00:01<00:18,  1.81it/s][A
 11%|█         | 4/36 [00:02<00:19,  1.64it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.56it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.51it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.47it/s][A
 22%|██▏       | 8/36 [00:05<00:20,  1.39it/s][A
 25%|██▌       | 9/36 [00:05<00:19,  1.39it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.40it/s][A
 31%|███       | 11/36 [00:07<00:17,  1.40it/s][A
 33%|███▎      | 12/36 [00:08<00:17,  1.41it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.40it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.40it/s][A
 42%|████▏     | 15/36 [00:10<00:14,  1.41it/s][A
 44%|████▍     | 16/36 [00:10<00:14,  1.41it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.41it/s][A
 50%|█████     | 18/36 [00:12<00:12,  1.41it/s][A
 53%|█████▎    | 19/36 [00:12<00:12,  1.42it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.42it/s][A
 58%|█████▊    | 21/36 [00:14<00:10,  1.42it/s][A
 61%|██████    | 22/36 [00:15<00:09,  1.42it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.42it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.42it/s][A
 69%|██████▉   | 25/36 [00:17<00:07,  1.42it/s][A
 72%|███████▏  | 26/36 [00:17<00:07,  1.42it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.41it/s][A
 78%|███████▊  | 28/36 [00:19<00:05,  1.41it/s][A
 81%|████████  | 29/36 [00:20<00:05,  1.36it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.38it/s][A
 86%|████████▌ | 31/36 [00:21<00:03,  1.52it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.81it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.10it/s][A
 94%|█████████▍| 34/36 [00:22<00:00,  2.37it/s][A
 97%|█████████▋| 35/36 [00:22<00:00,  2.60it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.17it/s][A                                                  
                                               [Aeval_anls: 50.31312251037534, epoch: 0.8576
 21%|██▏       | 500/2332 [15:19<45:03,  1.48s/it]
100%|██████████| 36/36 [00:23<00:00,  3.17it/s][A
                                               [A[32m[2023-11-10 11:35:10,687] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-500[0m
[32m[2023-11-10 11:35:10,696] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-500/config.json[0m
[32m[2023-11-10 11:35:13,278] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-500/model_state.pdparams[0m
[32m[2023-11-10 11:35:13,279] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-500/tokenizer_config.json[0m
[32m[2023-11-10 11:35:13,279] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-500/special_tokens_map.json[0m
[32m[2023-11-10 11:35:18,394] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-300] due to args.save_total_limit[0m
 21%|██▏       | 501/2332 [15:28<6:01:21, 11.84s/it] 22%|██▏       | 502/2332 [15:30<4:26:00,  8.72s/it] 22%|██▏       | 503/2332 [15:31<3:20:19,  6.57s/it] 22%|██▏       | 504/2332 [15:33<2:33:23,  5.03s/it] 22%|██▏       | 505/2332 [15:34<2:00:29,  3.96s/it] 22%|██▏       | 506/2332 [15:36<1:38:10,  3.23s/it] 22%|██▏       | 507/2332 [15:37<1:21:45,  2.69s/it] 22%|██▏       | 508/2332 [15:38<1:10:27,  2.32s/it] 22%|██▏       | 509/2332 [15:40<1:02:22,  2.05s/it] 22%|██▏       | 510/2332 [15:41<56:47,  1.87s/it]   22%|██▏       | 511/2332 [15:43<52:48,  1.74s/it] 22%|██▏       | 512/2332 [15:44<50:03,  1.65s/it] 22%|██▏       | 513/2332 [15:46<48:09,  1.59s/it] 22%|██▏       | 514/2332 [15:47<46:52,  1.55s/it] 22%|██▏       | 515/2332 [15:48<45:50,  1.51s/it] 22%|██▏       | 516/2332 [15:50<45:16,  1.50s/it] 22%|██▏       | 517/2332 [15:51<44:49,  1.48s/it] 22%|██▏       | 518/2332 [15:53<44:32,  1.47s/it] 22%|██▏       | 519/2332 [15:54<44:22,  1.47s/it] 22%|██▏       | 520/2332 [15:56<44:13,  1.46s/it] 22%|██▏       | 521/2332 [15:57<44:01,  1.46s/it] 22%|██▏       | 522/2332 [15:59<43:49,  1.45s/it] 22%|██▏       | 523/2332 [16:00<43:46,  1.45s/it] 22%|██▏       | 524/2332 [16:02<43:44,  1.45s/it] 23%|██▎       | 525/2332 [16:03<43:44,  1.45s/it] 23%|██▎       | 526/2332 [16:04<43:42,  1.45s/it] 23%|██▎       | 527/2332 [16:06<43:31,  1.45s/it] 23%|██▎       | 528/2332 [16:07<44:28,  1.48s/it] 23%|██▎       | 529/2332 [16:09<44:04,  1.47s/it] 23%|██▎       | 530/2332 [16:10<43:53,  1.46s/it] 23%|██▎       | 531/2332 [16:12<44:26,  1.48s/it] 23%|██▎       | 532/2332 [16:13<44:05,  1.47s/it] 23%|██▎       | 533/2332 [16:15<43:48,  1.46s/it] 23%|██▎       | 534/2332 [16:16<43:43,  1.46s/it] 23%|██▎       | 535/2332 [16:18<43:35,  1.46s/it] 23%|██▎       | 536/2332 [16:19<43:30,  1.45s/it] 23%|██▎       | 537/2332 [16:21<43:21,  1.45s/it] 23%|██▎       | 538/2332 [16:22<43:20,  1.45s/it] 23%|██▎       | 539/2332 [16:23<43:09,  1.44s/it] 23%|██▎       | 540/2332 [16:25<43:06,  1.44s/it] 23%|██▎       | 541/2332 [16:26<43:02,  1.44s/it] 23%|██▎       | 542/2332 [16:28<42:53,  1.44s/it] 23%|██▎       | 543/2332 [16:29<42:58,  1.44s/it] 23%|██▎       | 544/2332 [16:31<42:56,  1.44s/it] 23%|██▎       | 545/2332 [16:32<42:55,  1.44s/it] 23%|██▎       | 546/2332 [16:33<42:59,  1.44s/it] 23%|██▎       | 547/2332 [16:35<42:55,  1.44s/it] 23%|██▎       | 548/2332 [16:36<42:56,  1.44s/it] 24%|██▎       | 549/2332 [16:38<43:00,  1.45s/it] 24%|██▎       | 550/2332 [16:39<43:01,  1.45s/it] 24%|██▎       | 551/2332 [16:41<42:53,  1.45s/it] 24%|██▎       | 552/2332 [16:42<42:50,  1.44s/it] 24%|██▎       | 553/2332 [16:44<43:51,  1.48s/it] 24%|██▍       | 554/2332 [16:45<43:32,  1.47s/it] 24%|██▍       | 555/2332 [16:47<43:10,  1.46s/it] 24%|██▍       | 556/2332 [16:48<43:00,  1.45s/it] 24%|██▍       | 557/2332 [16:50<43:33,  1.47s/it] 24%|██▍       | 558/2332 [16:51<43:12,  1.46s/it] 24%|██▍       | 559/2332 [16:53<43:48,  1.48s/it] 24%|██▍       | 560/2332 [16:54<43:26,  1.47s/it] 24%|██▍       | 561/2332 [16:55<43:09,  1.46s/it] 24%|██▍       | 562/2332 [16:57<42:59,  1.46s/it] 24%|██▍       | 563/2332 [16:58<42:54,  1.46s/it] 24%|██▍       | 564/2332 [17:00<42:47,  1.45s/it] 24%|██▍       | 565/2332 [17:01<42:38,  1.45s/it] 24%|██▍       | 566/2332 [17:03<42:33,  1.45s/it] 24%|██▍       | 567/2332 [17:04<42:29,  1.44s/it] 24%|██▍       | 568/2332 [17:06<42:34,  1.45s/it] 24%|██▍       | 569/2332 [17:07<42:36,  1.45s/it] 24%|██▍       | 570/2332 [17:08<42:29,  1.45s/it] 24%|██▍       | 571/2332 [17:10<42:25,  1.45s/it] 25%|██▍       | 572/2332 [17:11<42:29,  1.45s/it] 25%|██▍       | 573/2332 [17:13<42:29,  1.45s/it] 25%|██▍       | 574/2332 [17:14<42:31,  1.45s/it] 25%|██▍       | 575/2332 [17:16<42:25,  1.45s/it] 25%|██▍       | 576/2332 [17:17<42:19,  1.45s/it] 25%|██▍       | 577/2332 [17:19<42:14,  1.44s/it] 25%|██▍       | 578/2332 [17:20<40:24,  1.38s/it] 25%|██▍       | 579/2332 [17:21<37:39,  1.29s/it] 25%|██▍       | 580/2332 [17:22<35:40,  1.22s/it] 25%|██▍       | 581/2332 [17:23<34:21,  1.18s/it] 25%|██▍       | 582/2332 [17:24<33:27,  1.15s/it] 25%|██▌       | 583/2332 [17:25<29:43,  1.02s/it] 25%|██▌       | 584/2332 [17:28<53:09,  1.82s/it] 25%|██▌       | 585/2332 [17:30<49:52,  1.71s/it] 25%|██▌       | 586/2332 [17:31<47:24,  1.63s/it] 25%|██▌       | 587/2332 [17:33<45:50,  1.58s/it] 25%|██▌       | 588/2332 [17:34<45:23,  1.56s/it] 25%|██▌       | 589/2332 [17:36<44:26,  1.53s/it] 25%|██▌       | 590/2332 [17:37<43:40,  1.50s/it] 25%|██▌       | 591/2332 [17:39<43:07,  1.49s/it] 25%|██▌       | 592/2332 [17:40<42:45,  1.47s/it] 25%|██▌       | 593/2332 [17:42<42:29,  1.47s/it] 25%|██▌       | 594/2332 [17:43<42:16,  1.46s/it] 26%|██▌       | 595/2332 [17:44<42:10,  1.46s/it] 26%|██▌       | 596/2332 [17:46<41:58,  1.45s/it] 26%|██▌       | 597/2332 [17:47<41:56,  1.45s/it] 26%|██▌       | 598/2332 [17:49<41:50,  1.45s/it] 26%|██▌       | 599/2332 [17:50<41:43,  1.44s/it] 26%|██▌       | 600/2332 [17:52<41:41,  1.44s/it][32m[2023-11-10 11:37:43,821] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id. If tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:37:44,315] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:37:44,315] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:37:44,315] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:37:44,315] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:37:44,316] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:12,  2.79it/s][A
  8%|▊         | 3/36 [00:01<00:16,  1.98it/s][A
 11%|█         | 4/36 [00:02<00:18,  1.73it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.61it/s][A
 17%|█▋        | 6/36 [00:03<00:20,  1.45it/s][A
 19%|█▉        | 7/36 [00:04<00:20,  1.43it/s][A
 22%|██▏       | 8/36 [00:05<00:19,  1.42it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.42it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.42it/s][A
 31%|███       | 11/36 [00:07<00:17,  1.42it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.42it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.37it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.38it/s][A
 42%|████▏     | 15/36 [00:10<00:15,  1.39it/s][A
 44%|████▍     | 16/36 [00:10<00:14,  1.40it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.39it/s][A
 50%|█████     | 18/36 [00:12<00:12,  1.40it/s][A
 53%|█████▎    | 19/36 [00:12<00:12,  1.40it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.36it/s][A
 58%|█████▊    | 21/36 [00:14<00:10,  1.37it/s][A
 61%|██████    | 22/36 [00:15<00:10,  1.39it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.40it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.40it/s][A
 69%|██████▉   | 25/36 [00:17<00:07,  1.40it/s][A
 72%|███████▏  | 26/36 [00:18<00:07,  1.40it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.41it/s][A
 78%|███████▊  | 28/36 [00:19<00:05,  1.41it/s][A
 81%|████████  | 29/36 [00:20<00:04,  1.41it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.41it/s][A
 86%|████████▌ | 31/36 [00:21<00:03,  1.55it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.84it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.13it/s][A
 94%|█████████▍| 34/36 [00:22<00:00,  2.40it/s][A
 97%|█████████▋| 35/36 [00:22<00:00,  2.62it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.19it/s][A                                                  
                                               [Aeval_anls: 53.15961955159746, epoch: 1.0292
 26%|██▌       | 600/2332 [18:18<41:41,  1.44s/it]
100%|██████████| 36/36 [00:23<00:00,  3.19it/s][A
                                               [A[32m[2023-11-10 11:38:10,207] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-600[0m
[32m[2023-11-10 11:38:10,224] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-600/config.json[0m
[32m[2023-11-10 11:38:12,783] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-600/model_state.pdparams[0m
[32m[2023-11-10 11:38:12,783] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-600/tokenizer_config.json[0m
[32m[2023-11-10 11:38:12,783] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-600/special_tokens_map.json[0m
[32m[2023-11-10 11:38:17,834] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-400] due to args.save_total_limit[0m
 26%|██▌       | 601/2332 [18:28<5:39:23, 11.76s/it] 26%|██▌       | 602/2332 [18:29<4:09:56,  8.67s/it] 26%|██▌       | 603/2332 [18:30<3:07:18,  6.50s/it] 26%|██▌       | 604/2332 [18:32<2:23:33,  4.98s/it] 26%|██▌       | 605/2332 [18:33<1:53:55,  3.96s/it] 26%|██▌       | 606/2332 [18:35<1:32:08,  3.20s/it] 26%|██▌       | 607/2332 [18:36<1:16:49,  2.67s/it] 26%|██▌       | 608/2332 [18:38<1:06:06,  2.30s/it] 26%|██▌       | 609/2332 [18:39<58:34,  2.04s/it]   26%|██▌       | 610/2332 [18:41<53:28,  1.86s/it] 26%|██▌       | 611/2332 [18:42<49:52,  1.74s/it] 26%|██▌       | 612/2332 [18:44<47:11,  1.65s/it] 26%|██▋       | 613/2332 [18:45<46:14,  1.61s/it] 26%|██▋       | 614/2332 [18:47<44:47,  1.56s/it] 26%|██▋       | 615/2332 [18:48<43:43,  1.53s/it] 26%|██▋       | 616/2332 [18:49<43:42,  1.53s/it] 26%|██▋       | 617/2332 [18:51<42:55,  1.50s/it] 27%|██▋       | 618/2332 [18:52<42:23,  1.48s/it] 27%|██▋       | 619/2332 [18:54<42:03,  1.47s/it] 27%|██▋       | 620/2332 [18:55<41:49,  1.47s/it] 27%|██▋       | 621/2332 [18:57<41:34,  1.46s/it] 27%|██▋       | 622/2332 [18:58<41:24,  1.45s/it] 27%|██▋       | 623/2332 [19:00<41:17,  1.45s/it] 27%|██▋       | 624/2332 [19:01<41:17,  1.45s/it] 27%|██▋       | 625/2332 [19:02<41:16,  1.45s/it] 27%|██▋       | 626/2332 [19:04<41:14,  1.45s/it] 27%|██▋       | 627/2332 [19:05<41:13,  1.45s/it] 27%|██▋       | 628/2332 [19:07<41:12,  1.45s/it] 27%|██▋       | 629/2332 [19:08<41:13,  1.45s/it] 27%|██▋       | 630/2332 [19:10<42:11,  1.49s/it] 27%|██▋       | 631/2332 [19:11<41:53,  1.48s/it] 27%|██▋       | 632/2332 [19:13<41:38,  1.47s/it] 27%|██▋       | 633/2332 [19:14<41:18,  1.46s/it] 27%|██▋       | 634/2332 [19:16<41:14,  1.46s/it] 27%|██▋       | 635/2332 [19:17<41:11,  1.46s/it] 27%|██▋       | 636/2332 [19:19<41:04,  1.45s/it] 27%|██▋       | 637/2332 [19:20<40:58,  1.45s/it] 27%|██▋       | 638/2332 [19:21<40:54,  1.45s/it] 27%|██▋       | 639/2332 [19:23<40:54,  1.45s/it] 27%|██▋       | 640/2332 [19:24<40:52,  1.45s/it] 27%|██▋       | 641/2332 [19:26<40:42,  1.44s/it] 28%|██▊       | 642/2332 [19:27<40:38,  1.44s/it] 28%|██▊       | 643/2332 [19:29<40:42,  1.45s/it] 28%|██▊       | 644/2332 [19:30<40:47,  1.45s/it] 28%|██▊       | 645/2332 [19:32<41:19,  1.47s/it] 28%|██▊       | 646/2332 [19:33<41:42,  1.48s/it] 28%|██▊       | 647/2332 [19:35<41:22,  1.47s/it] 28%|██▊       | 648/2332 [19:36<41:11,  1.47s/it] 28%|██▊       | 649/2332 [19:38<41:02,  1.46s/it] 28%|██▊       | 650/2332 [19:39<40:53,  1.46s/it] 28%|██▊       | 651/2332 [19:40<40:48,  1.46s/it] 28%|██▊       | 652/2332 [19:42<40:45,  1.46s/it] 28%|██▊       | 653/2332 [19:43<40:38,  1.45s/it] 28%|██▊       | 654/2332 [19:45<40:34,  1.45s/it] 28%|██▊       | 655/2332 [19:46<40:29,  1.45s/it] 28%|██▊       | 656/2332 [19:48<41:22,  1.48s/it] 28%|██▊       | 657/2332 [19:49<41:06,  1.47s/it] 28%|██▊       | 658/2332 [19:51<40:52,  1.46s/it] 28%|██▊       | 659/2332 [19:52<40:38,  1.46s/it] 28%|██▊       | 660/2332 [19:54<40:31,  1.45s/it] 28%|██▊       | 661/2332 [19:55<40:26,  1.45s/it] 28%|██▊       | 662/2332 [19:56<40:26,  1.45s/it] 28%|██▊       | 663/2332 [19:58<40:24,  1.45s/it] 28%|██▊       | 664/2332 [19:59<40:22,  1.45s/it] 29%|██▊       | 665/2332 [20:01<40:16,  1.45s/it] 29%|██▊       | 666/2332 [20:02<40:08,  1.45s/it] 29%|██▊       | 667/2332 [20:04<40:12,  1.45s/it] 29%|██▊       | 668/2332 [20:05<40:07,  1.45s/it] 29%|██▊       | 669/2332 [20:07<40:04,  1.45s/it] 29%|██▊       | 670/2332 [20:08<40:05,  1.45s/it] 29%|██▉       | 671/2332 [20:09<40:03,  1.45s/it] 29%|██▉       | 672/2332 [20:11<40:06,  1.45s/it] 29%|██▉       | 673/2332 [20:12<40:48,  1.48s/it] 29%|██▉       | 674/2332 [20:14<40:33,  1.47s/it] 29%|██▉       | 675/2332 [20:15<40:23,  1.46s/it] 29%|██▉       | 676/2332 [20:17<40:51,  1.48s/it] 29%|██▉       | 677/2332 [20:18<40:35,  1.47s/it] 29%|██▉       | 678/2332 [20:20<40:22,  1.46s/it] 29%|██▉       | 679/2332 [20:21<40:14,  1.46s/it] 29%|██▉       | 680/2332 [20:23<40:07,  1.46s/it] 29%|██▉       | 681/2332 [20:24<40:54,  1.49s/it] 29%|██▉       | 682/2332 [20:26<40:36,  1.48s/it] 29%|██▉       | 683/2332 [20:27<40:23,  1.47s/it] 29%|██▉       | 684/2332 [20:29<40:13,  1.46s/it] 29%|██▉       | 685/2332 [20:30<40:06,  1.46s/it] 29%|██▉       | 686/2332 [20:32<40:01,  1.46s/it] 29%|██▉       | 687/2332 [20:33<39:54,  1.46s/it] 30%|██▉       | 688/2332 [20:34<39:52,  1.46s/it] 30%|██▉       | 689/2332 [20:36<39:48,  1.45s/it] 30%|██▉       | 690/2332 [20:37<39:46,  1.45s/it] 30%|██▉       | 691/2332 [20:39<39:47,  1.46s/it] 30%|██▉       | 692/2332 [20:40<39:46,  1.46s/it] 30%|██▉       | 693/2332 [20:42<39:35,  1.45s/it] 30%|██▉       | 694/2332 [20:43<39:35,  1.45s/it] 30%|██▉       | 695/2332 [20:45<39:29,  1.45s/it] 30%|██▉       | 696/2332 [20:46<39:26,  1.45s/it] 30%|██▉       | 697/2332 [20:47<39:27,  1.45s/it] 30%|██▉       | 698/2332 [20:49<39:16,  1.44s/it] 30%|██▉       | 699/2332 [20:50<39:20,  1.45s/it] 30%|███       | 700/2332 [20:52<39:15,  1.44s/it][32m[2023-11-10 11:40:43,887] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id. If tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:40:44,389] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:40:44,390] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:40:44,390] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:40:44,390] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:40:44,390] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.84it/s][A
  8%|▊         | 3/36 [00:01<00:16,  2.00it/s][A
 11%|█         | 4/36 [00:02<00:18,  1.74it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.61it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.54it/s][A
 19%|█▉        | 7/36 [00:04<00:20,  1.42it/s][A
 22%|██▏       | 8/36 [00:05<00:20,  1.34it/s][A
 25%|██▌       | 9/36 [00:05<00:19,  1.37it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.38it/s][A
 31%|███       | 11/36 [00:07<00:17,  1.39it/s][A
 33%|███▎      | 12/36 [00:08<00:17,  1.39it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.40it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.40it/s][A
 42%|████▏     | 15/36 [00:10<00:15,  1.40it/s][A
 44%|████▍     | 16/36 [00:10<00:14,  1.40it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.41it/s][A
 50%|█████     | 18/36 [00:12<00:12,  1.41it/s][A
 53%|█████▎    | 19/36 [00:12<00:12,  1.40it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.41it/s][A
 58%|█████▊    | 21/36 [00:14<00:10,  1.41it/s][A
 61%|██████    | 22/36 [00:15<00:09,  1.41it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.42it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.42it/s][A
 69%|██████▉   | 25/36 [00:17<00:07,  1.42it/s][A
 72%|███████▏  | 26/36 [00:17<00:07,  1.41it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.41it/s][A
 78%|███████▊  | 28/36 [00:19<00:05,  1.41it/s][A
 81%|████████  | 29/36 [00:20<00:04,  1.41it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.42it/s][A
 86%|████████▌ | 31/36 [00:21<00:03,  1.55it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.85it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.14it/s][A
 94%|█████████▍| 34/36 [00:22<00:00,  2.40it/s][A
 97%|█████████▋| 35/36 [00:22<00:00,  2.62it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.20it/s][A                                                  
                                               [Aeval_anls: 57.90610656448577, epoch: 1.2007
 30%|███       | 700/2332 [21:18<39:15,  1.44s/it]
100%|██████████| 36/36 [00:23<00:00,  3.20it/s][A
                                               [A[32m[2023-11-10 11:41:10,259] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-700[0m
[32m[2023-11-10 11:41:10,268] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-700/config.json[0m
[32m[2023-11-10 11:41:12,851] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-700/model_state.pdparams[0m
[32m[2023-11-10 11:41:12,851] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-700/tokenizer_config.json[0m
[32m[2023-11-10 11:41:12,851] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-700/special_tokens_map.json[0m
[32m[2023-11-10 11:41:17,975] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-500] due to args.save_total_limit[0m
 30%|███       | 701/2332 [21:28<5:20:18, 11.78s/it] 30%|███       | 702/2332 [21:29<3:56:24,  8.70s/it] 30%|███       | 703/2332 [21:31<2:57:07,  6.52s/it] 30%|███       | 704/2332 [21:32<2:15:42,  5.00s/it] 30%|███       | 705/2332 [21:34<1:46:41,  3.93s/it] 30%|███       | 706/2332 [21:35<1:26:16,  3.18s/it] 30%|███       | 707/2332 [21:36<1:12:06,  2.66s/it] 30%|███       | 708/2332 [21:38<1:03:08,  2.33s/it] 30%|███       | 709/2332 [21:39<55:50,  2.06s/it]   30%|███       | 710/2332 [21:41<50:48,  1.88s/it] 30%|███       | 711/2332 [21:42<47:12,  1.75s/it] 31%|███       | 712/2332 [21:44<44:45,  1.66s/it] 31%|███       | 713/2332 [21:45<43:05,  1.60s/it] 31%|███       | 714/2332 [21:47<41:52,  1.55s/it] 31%|███       | 715/2332 [21:48<40:50,  1.52s/it] 31%|███       | 716/2332 [21:50<40:14,  1.49s/it] 31%|███       | 717/2332 [21:51<39:51,  1.48s/it] 31%|███       | 718/2332 [21:52<39:36,  1.47s/it] 31%|███       | 719/2332 [21:54<39:25,  1.47s/it] 31%|███       | 720/2332 [21:55<39:04,  1.45s/it] 31%|███       | 721/2332 [21:57<38:54,  1.45s/it] 31%|███       | 722/2332 [21:58<38:54,  1.45s/it] 31%|███       | 723/2332 [22:00<38:51,  1.45s/it] 31%|███       | 724/2332 [22:01<38:42,  1.44s/it] 31%|███       | 725/2332 [22:03<38:43,  1.45s/it] 31%|███       | 726/2332 [22:04<38:37,  1.44s/it] 31%|███       | 727/2332 [22:05<39:19,  1.47s/it] 31%|███       | 728/2332 [22:07<39:07,  1.46s/it] 31%|███▏      | 729/2332 [22:08<38:57,  1.46s/it] 31%|███▏      | 730/2332 [22:10<38:49,  1.45s/it] 31%|███▏      | 731/2332 [22:11<38:45,  1.45s/it] 31%|███▏      | 732/2332 [22:13<38:36,  1.45s/it] 31%|███▏      | 733/2332 [22:14<39:31,  1.48s/it] 31%|███▏      | 734/2332 [22:16<39:52,  1.50s/it] 32%|███▏      | 735/2332 [22:17<39:23,  1.48s/it] 32%|███▏      | 736/2332 [22:19<39:10,  1.47s/it] 32%|███▏      | 737/2332 [22:20<38:53,  1.46s/it] 32%|███▏      | 738/2332 [22:22<38:40,  1.46s/it] 32%|███▏      | 739/2332 [22:23<38:39,  1.46s/it] 32%|███▏      | 740/2332 [22:24<38:33,  1.45s/it] 32%|███▏      | 741/2332 [22:26<38:33,  1.45s/it] 32%|███▏      | 742/2332 [22:27<38:31,  1.45s/it] 32%|███▏      | 743/2332 [22:29<38:26,  1.45s/it] 32%|███▏      | 744/2332 [22:30<38:20,  1.45s/it] 32%|███▏      | 745/2332 [22:32<38:18,  1.45s/it] 32%|███▏      | 746/2332 [22:33<38:15,  1.45s/it] 32%|███▏      | 747/2332 [22:35<38:12,  1.45s/it] 32%|███▏      | 748/2332 [22:36<38:10,  1.45s/it] 32%|███▏      | 749/2332 [22:38<38:12,  1.45s/it] 32%|███▏      | 750/2332 [22:39<38:04,  1.44s/it] 32%|███▏      | 751/2332 [22:40<38:04,  1.45s/it] 32%|███▏      | 752/2332 [22:42<37:57,  1.44s/it] 32%|███▏      | 753/2332 [22:43<38:33,  1.47s/it] 32%|███▏      | 754/2332 [22:45<38:21,  1.46s/it] 32%|███▏      | 755/2332 [22:46<38:08,  1.45s/it] 32%|███▏      | 756/2332 [22:48<38:03,  1.45s/it] 32%|███▏      | 757/2332 [22:49<37:57,  1.45s/it] 33%|███▎      | 758/2332 [22:51<38:48,  1.48s/it] 33%|███▎      | 759/2332 [22:52<38:33,  1.47s/it] 33%|███▎      | 760/2332 [22:54<38:11,  1.46s/it] 33%|███▎      | 761/2332 [22:55<38:03,  1.45s/it] 33%|███▎      | 762/2332 [22:56<37:57,  1.45s/it] 33%|███▎      | 763/2332 [22:58<37:47,  1.45s/it] 33%|███▎      | 764/2332 [22:59<38:25,  1.47s/it] 33%|███▎      | 765/2332 [23:01<38:13,  1.46s/it] 33%|███▎      | 766/2332 [23:02<38:02,  1.46s/it] 33%|███▎      | 767/2332 [23:04<37:55,  1.45s/it] 33%|███▎      | 768/2332 [23:05<37:49,  1.45s/it] 33%|███▎      | 769/2332 [23:07<37:45,  1.45s/it] 33%|███▎      | 770/2332 [23:08<37:41,  1.45s/it] 33%|███▎      | 771/2332 [23:10<37:42,  1.45s/it] 33%|███▎      | 772/2332 [23:11<37:41,  1.45s/it] 33%|███▎      | 773/2332 [23:12<37:40,  1.45s/it] 33%|███▎      | 774/2332 [23:14<37:39,  1.45s/it] 33%|███▎      | 775/2332 [23:15<37:40,  1.45s/it] 33%|███▎      | 776/2332 [23:17<37:37,  1.45s/it] 33%|███▎      | 777/2332 [23:18<37:38,  1.45s/it] 33%|███▎      | 778/2332 [23:20<37:36,  1.45s/it] 33%|███▎      | 779/2332 [23:21<38:06,  1.47s/it] 33%|███▎      | 780/2332 [23:23<37:52,  1.46s/it] 33%|███▎      | 781/2332 [23:24<37:46,  1.46s/it] 34%|███▎      | 782/2332 [23:26<37:39,  1.46s/it] 34%|███▎      | 783/2332 [23:27<38:29,  1.49s/it] 34%|███▎      | 784/2332 [23:29<38:05,  1.48s/it] 34%|███▎      | 785/2332 [23:30<37:50,  1.47s/it] 34%|███▎      | 786/2332 [23:31<37:39,  1.46s/it] 34%|███▎      | 787/2332 [23:33<37:33,  1.46s/it] 34%|███▍      | 788/2332 [23:34<37:31,  1.46s/it] 34%|███▍      | 789/2332 [23:36<37:24,  1.45s/it] 34%|███▍      | 790/2332 [23:37<37:24,  1.46s/it] 34%|███▍      | 791/2332 [23:39<37:17,  1.45s/it] 34%|███▍      | 792/2332 [23:40<37:08,  1.45s/it] 34%|███▍      | 793/2332 [23:42<37:09,  1.45s/it] 34%|███▍      | 794/2332 [23:43<37:41,  1.47s/it] 34%|███▍      | 795/2332 [23:45<37:26,  1.46s/it] 34%|███▍      | 796/2332 [23:46<37:18,  1.46s/it] 34%|███▍      | 797/2332 [23:47<37:07,  1.45s/it] 34%|███▍      | 798/2332 [23:49<37:00,  1.45s/it] 34%|███▍      | 799/2332 [23:50<37:00,  1.45s/it] 34%|███▍      | 800/2332 [23:52<37:02,  1.45s/it][32m[2023-11-10 11:43:43,919] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id. If tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:43:44,366] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:43:44,366] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:43:44,366] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:43:44,366] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:43:44,366] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:12,  2.80it/s][A
  8%|▊         | 3/36 [00:01<00:16,  1.99it/s][A
 11%|█         | 4/36 [00:02<00:19,  1.64it/s][A
 14%|█▍        | 5/36 [00:02<00:20,  1.54it/s][A
 17%|█▋        | 6/36 [00:03<00:20,  1.50it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.47it/s][A
 22%|██▏       | 8/36 [00:05<00:19,  1.44it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.43it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.43it/s][A
 31%|███       | 11/36 [00:07<00:17,  1.43it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.42it/s][A
 36%|███▌      | 13/36 [00:08<00:17,  1.35it/s][A
 39%|███▉      | 14/36 [00:09<00:16,  1.37it/s][A
 42%|████▏     | 15/36 [00:10<00:15,  1.38it/s][A
 44%|████▍     | 16/36 [00:10<00:14,  1.39it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.40it/s][A
 50%|█████     | 18/36 [00:12<00:12,  1.41it/s][A
 53%|█████▎    | 19/36 [00:12<00:12,  1.41it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.41it/s][A
 58%|█████▊    | 21/36 [00:14<00:10,  1.41it/s][A
 61%|██████    | 22/36 [00:15<00:10,  1.37it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.37it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.39it/s][A
 69%|██████▉   | 25/36 [00:17<00:07,  1.40it/s][A
 72%|███████▏  | 26/36 [00:18<00:07,  1.40it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.40it/s][A
 78%|███████▊  | 28/36 [00:19<00:05,  1.40it/s][A
 81%|████████  | 29/36 [00:20<00:04,  1.41it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.40it/s][A
 86%|████████▌ | 31/36 [00:21<00:03,  1.54it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.84it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.12it/s][A
 94%|█████████▍| 34/36 [00:22<00:00,  2.39it/s][A
 97%|█████████▋| 35/36 [00:22<00:00,  2.61it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.18it/s][A                                                  
                                               [Aeval_anls: 58.983842536342145, epoch: 1.3722
 34%|███▍      | 800/2332 [24:18<37:02,  1.45s/it]
100%|██████████| 36/36 [00:23<00:00,  3.18it/s][A
                                               [A[32m[2023-11-10 11:44:10,317] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-800[0m
[32m[2023-11-10 11:44:10,326] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-800/config.json[0m
[32m[2023-11-10 11:44:12,854] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-800/model_state.pdparams[0m
[32m[2023-11-10 11:44:12,855] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-800/tokenizer_config.json[0m
[32m[2023-11-10 11:44:12,855] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-800/special_tokens_map.json[0m
[32m[2023-11-10 11:44:17,873] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-600] due to args.save_total_limit[0m
 34%|███▍      | 801/2332 [24:28<4:59:48, 11.75s/it] 34%|███▍      | 802/2332 [24:29<3:40:44,  8.66s/it] 34%|███▍      | 803/2332 [24:30<2:45:24,  6.49s/it] 34%|███▍      | 804/2332 [24:32<2:06:37,  4.97s/it] 35%|███▍      | 805/2332 [24:33<1:40:07,  3.93s/it] 35%|███▍      | 806/2332 [24:35<1:20:59,  3.18s/it] 35%|███▍      | 807/2332 [24:36<1:07:42,  2.66s/it] 35%|███▍      | 808/2332 [24:38<58:18,  2.30s/it]   35%|███▍      | 809/2332 [24:39<51:47,  2.04s/it] 35%|███▍      | 810/2332 [24:41<47:09,  1.86s/it] 35%|███▍      | 811/2332 [24:42<44:01,  1.74s/it] 35%|███▍      | 812/2332 [24:44<41:49,  1.65s/it] 35%|███▍      | 813/2332 [24:45<41:13,  1.63s/it] 35%|███▍      | 814/2332 [24:47<39:51,  1.58s/it] 35%|███▍      | 815/2332 [24:48<38:42,  1.53s/it] 35%|███▍      | 816/2332 [24:49<37:56,  1.50s/it] 35%|███▌      | 817/2332 [24:51<38:07,  1.51s/it] 35%|███▌      | 818/2332 [24:52<37:32,  1.49s/it] 35%|███▌      | 819/2332 [24:54<37:15,  1.48s/it] 35%|███▌      | 820/2332 [24:55<37:00,  1.47s/it] 35%|███▌      | 821/2332 [24:57<36:44,  1.46s/it] 35%|███▌      | 822/2332 [24:58<36:40,  1.46s/it] 35%|███▌      | 823/2332 [25:00<36:33,  1.45s/it] 35%|███▌      | 824/2332 [25:01<36:25,  1.45s/it] 35%|███▌      | 825/2332 [25:02<36:26,  1.45s/it] 35%|███▌      | 826/2332 [25:04<36:21,  1.45s/it] 35%|███▌      | 827/2332 [25:05<36:15,  1.45s/it] 36%|███▌      | 828/2332 [25:07<36:10,  1.44s/it] 36%|███▌      | 829/2332 [25:08<36:12,  1.45s/it] 36%|███▌      | 830/2332 [25:10<36:12,  1.45s/it] 36%|███▌      | 831/2332 [25:11<36:08,  1.44s/it] 36%|███▌      | 832/2332 [25:13<36:09,  1.45s/it] 36%|███▌      | 833/2332 [25:14<36:08,  1.45s/it] 36%|███▌      | 834/2332 [25:15<36:01,  1.44s/it] 36%|███▌      | 835/2332 [25:17<35:56,  1.44s/it] 36%|███▌      | 836/2332 [25:18<36:28,  1.46s/it] 36%|███▌      | 837/2332 [25:20<36:18,  1.46s/it] 36%|███▌      | 838/2332 [25:21<36:08,  1.45s/it] 36%|███▌      | 839/2332 [25:23<36:00,  1.45s/it] 36%|███▌      | 840/2332 [25:24<36:01,  1.45s/it] 36%|███▌      | 841/2332 [25:26<35:57,  1.45s/it] 36%|███▌      | 842/2332 [25:27<36:32,  1.47s/it] 36%|███▌      | 843/2332 [25:29<36:21,  1.46s/it] 36%|███▌      | 844/2332 [25:30<36:12,  1.46s/it] 36%|███▌      | 845/2332 [25:32<36:53,  1.49s/it] 36%|███▋      | 846/2332 [25:33<36:38,  1.48s/it] 36%|███▋      | 847/2332 [25:35<36:20,  1.47s/it] 36%|███▋      | 848/2332 [25:36<36:13,  1.46s/it] 36%|███▋      | 849/2332 [25:37<36:01,  1.46s/it] 36%|███▋      | 850/2332 [25:39<35:52,  1.45s/it] 36%|███▋      | 851/2332 [25:40<35:44,  1.45s/it] 37%|███▋      | 852/2332 [25:42<35:45,  1.45s/it] 37%|███▋      | 853/2332 [25:43<35:45,  1.45s/it] 37%|███▋      | 854/2332 [25:45<35:41,  1.45s/it] 37%|███▋      | 855/2332 [25:46<35:42,  1.45s/it] 37%|███▋      | 856/2332 [25:48<35:41,  1.45s/it] 37%|███▋      | 857/2332 [25:49<35:38,  1.45s/it] 37%|███▋      | 858/2332 [25:50<35:33,  1.45s/it] 37%|███▋      | 859/2332 [25:52<35:33,  1.45s/it] 37%|███▋      | 860/2332 [25:53<35:34,  1.45s/it] 37%|███▋      | 861/2332 [25:55<35:33,  1.45s/it] 37%|███▋      | 862/2332 [25:56<35:33,  1.45s/it] 37%|███▋      | 863/2332 [25:58<35:30,  1.45s/it] 37%|███▋      | 864/2332 [25:59<35:24,  1.45s/it] 37%|███▋      | 865/2332 [26:01<35:22,  1.45s/it] 37%|███▋      | 866/2332 [26:02<35:16,  1.44s/it] 37%|███▋      | 867/2332 [26:04<35:50,  1.47s/it] 37%|███▋      | 868/2332 [26:05<36:11,  1.48s/it] 37%|███▋      | 869/2332 [26:07<35:46,  1.47s/it] 37%|███▋      | 870/2332 [26:08<35:31,  1.46s/it] 37%|███▋      | 871/2332 [26:09<35:27,  1.46s/it] 37%|███▋      | 872/2332 [26:11<35:25,  1.46s/it] 37%|███▋      | 873/2332 [26:12<36:11,  1.49s/it] 37%|███▋      | 874/2332 [26:14<35:49,  1.47s/it] 38%|███▊      | 875/2332 [26:15<35:38,  1.47s/it] 38%|███▊      | 876/2332 [26:17<35:28,  1.46s/it] 38%|███▊      | 877/2332 [26:18<35:18,  1.46s/it] 38%|███▊      | 878/2332 [26:20<35:10,  1.45s/it] 38%|███▊      | 879/2332 [26:21<35:07,  1.45s/it] 38%|███▊      | 880/2332 [26:23<35:04,  1.45s/it] 38%|███▊      | 881/2332 [26:24<35:05,  1.45s/it] 38%|███▊      | 882/2332 [26:25<35:02,  1.45s/it] 38%|███▊      | 883/2332 [26:27<35:05,  1.45s/it] 38%|███▊      | 884/2332 [26:28<35:00,  1.45s/it] 38%|███▊      | 885/2332 [26:30<35:01,  1.45s/it] 38%|███▊      | 886/2332 [26:31<35:00,  1.45s/it] 38%|███▊      | 887/2332 [26:33<34:58,  1.45s/it] 38%|███▊      | 888/2332 [26:34<34:57,  1.45s/it] 38%|███▊      | 889/2332 [26:36<34:55,  1.45s/it] 38%|███▊      | 890/2332 [26:37<34:52,  1.45s/it] 38%|███▊      | 891/2332 [26:39<34:54,  1.45s/it] 38%|███▊      | 892/2332 [26:40<34:52,  1.45s/it] 38%|███▊      | 893/2332 [26:41<35:12,  1.47s/it] 38%|███▊      | 894/2332 [26:43<34:55,  1.46s/it] 38%|███▊      | 895/2332 [26:44<34:48,  1.45s/it] 38%|███▊      | 896/2332 [26:46<35:18,  1.48s/it] 38%|███▊      | 897/2332 [26:47<35:06,  1.47s/it] 39%|███▊      | 898/2332 [26:49<34:53,  1.46s/it] 39%|███▊      | 899/2332 [26:50<34:48,  1.46s/it] 39%|███▊      | 900/2332 [26:52<34:45,  1.46s/it][32m[2023-11-10 11:46:43,783] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id. If tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:46:44,111] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:46:44,112] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:46:44,112] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:46:44,112] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:46:44,112] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:12,  2.80it/s][A
  8%|▊         | 3/36 [00:01<00:16,  1.96it/s][A
 11%|█         | 4/36 [00:02<00:18,  1.71it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.60it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.53it/s][A
 19%|█▉        | 7/36 [00:04<00:20,  1.40it/s][A
 22%|██▏       | 8/36 [00:05<00:19,  1.41it/s][A
 25%|██▌       | 9/36 [00:05<00:19,  1.40it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.40it/s][A
 31%|███       | 11/36 [00:07<00:17,  1.41it/s][A
 33%|███▎      | 12/36 [00:07<00:17,  1.41it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.40it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.41it/s][A
 42%|████▏     | 15/36 [00:10<00:15,  1.40it/s][A
 44%|████▍     | 16/36 [00:10<00:14,  1.40it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.41it/s][A
 50%|█████     | 18/36 [00:12<00:12,  1.41it/s][A
 53%|█████▎    | 19/36 [00:12<00:12,  1.41it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.41it/s][A
 58%|█████▊    | 21/36 [00:14<00:10,  1.41it/s][A
 61%|██████    | 22/36 [00:15<00:09,  1.41it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.41it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.36it/s][A
 69%|██████▉   | 25/36 [00:17<00:08,  1.37it/s][A
 72%|███████▏  | 26/36 [00:17<00:07,  1.38it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.34it/s][A
 78%|███████▊  | 28/36 [00:19<00:05,  1.36it/s][A
 81%|████████  | 29/36 [00:20<00:05,  1.38it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.39it/s][A
 86%|████████▌ | 31/36 [00:21<00:03,  1.53it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.83it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.12it/s][A
 94%|█████████▍| 34/36 [00:22<00:00,  2.38it/s][A
 97%|█████████▋| 35/36 [00:22<00:00,  2.60it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.18it/s][A                                                  
                                               [Aeval_anls: 59.30772357902208, epoch: 1.5437
 39%|███▊      | 900/2332 [27:18<34:45,  1.46s/it]
100%|██████████| 36/36 [00:23<00:00,  3.18it/s][A
                                               [A[32m[2023-11-10 11:47:10,258] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-900[0m
[32m[2023-11-10 11:47:10,267] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-900/config.json[0m
[32m[2023-11-10 11:47:12,797] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-900/model_state.pdparams[0m
[32m[2023-11-10 11:47:12,797] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-900/tokenizer_config.json[0m
[32m[2023-11-10 11:47:12,798] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-900/special_tokens_map.json[0m
[32m[2023-11-10 11:47:17,817] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-700] due to args.save_total_limit[0m
 39%|███▊      | 901/2332 [27:28<4:41:02, 11.78s/it] 39%|███▊      | 902/2332 [27:29<3:26:53,  8.68s/it] 39%|███▊      | 903/2332 [27:30<2:35:01,  6.51s/it] 39%|███▉      | 904/2332 [27:32<1:58:47,  4.99s/it] 39%|███▉      | 905/2332 [27:33<1:33:28,  3.93s/it] 39%|███▉      | 906/2332 [27:35<1:15:45,  3.19s/it] 39%|███▉      | 907/2332 [27:36<1:03:18,  2.67s/it] 39%|███▉      | 908/2332 [27:38<54:38,  2.30s/it]   39%|███▉      | 909/2332 [27:39<48:31,  2.05s/it] 39%|███▉      | 910/2332 [27:41<44:14,  1.87s/it] 39%|███▉      | 911/2332 [27:42<41:15,  1.74s/it] 39%|███▉      | 912/2332 [27:43<39:03,  1.65s/it] 39%|███▉      | 913/2332 [27:45<37:39,  1.59s/it] 39%|███▉      | 914/2332 [27:46<36:34,  1.55s/it] 39%|███▉      | 915/2332 [27:48<35:46,  1.51s/it] 39%|███▉      | 916/2332 [27:49<35:15,  1.49s/it] 39%|███▉      | 917/2332 [27:51<34:50,  1.48s/it] 39%|███▉      | 918/2332 [27:52<34:38,  1.47s/it] 39%|███▉      | 919/2332 [27:54<34:30,  1.47s/it] 39%|███▉      | 920/2332 [27:55<34:23,  1.46s/it] 39%|███▉      | 921/2332 [27:57<34:18,  1.46s/it] 40%|███▉      | 922/2332 [27:58<34:10,  1.45s/it] 40%|███▉      | 923/2332 [27:59<34:42,  1.48s/it] 40%|███▉      | 924/2332 [28:01<34:29,  1.47s/it] 40%|███▉      | 925/2332 [28:02<34:23,  1.47s/it] 40%|███▉      | 926/2332 [28:04<34:53,  1.49s/it] 40%|███▉      | 927/2332 [28:05<34:32,  1.48s/it] 40%|███▉      | 928/2332 [28:07<34:21,  1.47s/it] 40%|███▉      | 929/2332 [28:08<34:08,  1.46s/it] 40%|███▉      | 930/2332 [28:10<34:00,  1.46s/it] 40%|███▉      | 931/2332 [28:11<33:54,  1.45s/it] 40%|███▉      | 932/2332 [28:13<33:50,  1.45s/it] 40%|████      | 933/2332 [28:14<33:47,  1.45s/it] 40%|████      | 934/2332 [28:16<34:33,  1.48s/it] 40%|████      | 935/2332 [28:17<34:15,  1.47s/it] 40%|████      | 936/2332 [28:19<34:03,  1.46s/it] 40%|████      | 937/2332 [28:20<33:58,  1.46s/it] 40%|████      | 938/2332 [28:21<33:51,  1.46s/it] 40%|████      | 939/2332 [28:23<33:41,  1.45s/it] 40%|████      | 940/2332 [28:24<33:40,  1.45s/it] 40%|████      | 941/2332 [28:26<33:40,  1.45s/it] 40%|████      | 942/2332 [28:27<33:36,  1.45s/it] 40%|████      | 943/2332 [28:29<33:38,  1.45s/it] 40%|████      | 944/2332 [28:30<33:30,  1.45s/it] 41%|████      | 945/2332 [28:32<33:29,  1.45s/it] 41%|████      | 946/2332 [28:33<33:23,  1.45s/it] 41%|████      | 947/2332 [28:34<33:26,  1.45s/it] 41%|████      | 948/2332 [28:36<33:26,  1.45s/it] 41%|████      | 949/2332 [28:37<33:27,  1.45s/it] 41%|████      | 950/2332 [28:39<33:24,  1.45s/it] 41%|████      | 951/2332 [28:40<33:24,  1.45s/it] 41%|████      | 952/2332 [28:42<33:23,  1.45s/it] 41%|████      | 953/2332 [28:43<33:22,  1.45s/it] 41%|████      | 954/2332 [28:45<33:51,  1.47s/it] 41%|████      | 955/2332 [28:46<33:36,  1.46s/it] 41%|████      | 956/2332 [28:48<33:32,  1.46s/it] 41%|████      | 957/2332 [28:49<33:22,  1.46s/it] 41%|████      | 958/2332 [28:51<33:44,  1.47s/it] 41%|████      | 959/2332 [28:52<33:34,  1.47s/it] 41%|████      | 960/2332 [28:53<33:28,  1.46s/it] 41%|████      | 961/2332 [28:55<33:20,  1.46s/it] 41%|████▏     | 962/2332 [28:56<33:08,  1.45s/it] 41%|████▏     | 963/2332 [28:58<33:08,  1.45s/it] 41%|████▏     | 964/2332 [28:59<33:54,  1.49s/it] 41%|████▏     | 965/2332 [29:01<33:37,  1.48s/it] 41%|████▏     | 966/2332 [29:02<33:20,  1.46s/it] 41%|████▏     | 967/2332 [29:04<33:11,  1.46s/it] 42%|████▏     | 968/2332 [29:05<33:09,  1.46s/it] 42%|████▏     | 969/2332 [29:07<33:01,  1.45s/it] 42%|████▏     | 970/2332 [29:08<33:00,  1.45s/it] 42%|████▏     | 971/2332 [29:09<32:51,  1.45s/it] 42%|████▏     | 972/2332 [29:11<32:51,  1.45s/it] 42%|████▏     | 973/2332 [29:12<32:51,  1.45s/it] 42%|████▏     | 974/2332 [29:14<32:54,  1.45s/it] 42%|████▏     | 975/2332 [29:15<32:50,  1.45s/it] 42%|████▏     | 976/2332 [29:17<32:47,  1.45s/it] 42%|████▏     | 977/2332 [29:18<32:47,  1.45s/it] 42%|████▏     | 978/2332 [29:20<32:46,  1.45s/it] 42%|████▏     | 979/2332 [29:21<32:43,  1.45s/it] 42%|████▏     | 980/2332 [29:23<32:42,  1.45s/it] 42%|████▏     | 981/2332 [29:24<32:41,  1.45s/it] 42%|████▏     | 982/2332 [29:25<32:40,  1.45s/it] 42%|████▏     | 983/2332 [29:27<32:38,  1.45s/it] 42%|████▏     | 984/2332 [29:28<32:37,  1.45s/it] 42%|████▏     | 985/2332 [29:30<32:35,  1.45s/it] 42%|████▏     | 986/2332 [29:31<33:08,  1.48s/it] 42%|████▏     | 987/2332 [29:33<32:54,  1.47s/it] 42%|████▏     | 988/2332 [29:34<32:46,  1.46s/it] 42%|████▏     | 989/2332 [29:36<32:37,  1.46s/it] 42%|████▏     | 990/2332 [29:37<32:31,  1.45s/it] 42%|████▏     | 991/2332 [29:39<32:28,  1.45s/it] 43%|████▎     | 992/2332 [29:40<32:27,  1.45s/it] 43%|████▎     | 993/2332 [29:41<32:18,  1.45s/it] 43%|████▎     | 994/2332 [29:43<33:00,  1.48s/it] 43%|████▎     | 995/2332 [29:44<32:48,  1.47s/it] 43%|████▎     | 996/2332 [29:46<32:40,  1.47s/it] 43%|████▎     | 997/2332 [29:47<32:32,  1.46s/it] 43%|████▎     | 998/2332 [29:49<32:25,  1.46s/it] 43%|████▎     | 999/2332 [29:50<32:21,  1.46s/it] 43%|████▎     | 1000/2332 [29:52<32:16,  1.45s/it]                                                   loss: 1.283755, learning_rate: 1.202e-05, global_step: 1000, interval_runtime: 899.6704, interval_samples_per_second: 13.338217390281583, interval_steps_per_second: 0.5557590579283993, epoch: 1.7153
 43%|████▎     | 1000/2332 [29:52<32:16,  1.45s/it][32m[2023-11-10 11:49:43,907] [    INFO][0m - The following columns in the evaluation set  don't have a corresponding argument in `ErnieLayoutForQuestionAnswering.forward` and have been ignored: tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id. If tokens, question_id, start_labels, end_labels, token_is_max_context, token_to_orig_map, questions, id are not expected by `ErnieLayoutForQuestionAnswering.forward`,  you can safely ignore this message.[0m
[32m[2023-11-10 11:49:44,414] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2023-11-10 11:49:44,414] [    INFO][0m -   Num examples = 852[0m
[32m[2023-11-10 11:49:44,414] [    INFO][0m -   Total prediction steps = 36[0m
[32m[2023-11-10 11:49:44,414] [    INFO][0m -   Pre device batch size = 6[0m
[32m[2023-11-10 11:49:44,414] [    INFO][0m -   Total Batch size = 24[0m

  0%|          | 0/36 [00:00<?, ?it/s][A
  6%|▌         | 2/36 [00:00<00:11,  2.85it/s][A
  8%|▊         | 3/36 [00:01<00:16,  2.00it/s][A
 11%|█         | 4/36 [00:02<00:18,  1.74it/s][A
 14%|█▍        | 5/36 [00:02<00:19,  1.61it/s][A
 17%|█▋        | 6/36 [00:03<00:19,  1.54it/s][A
 19%|█▉        | 7/36 [00:04<00:19,  1.50it/s][A
 22%|██▏       | 8/36 [00:04<00:19,  1.47it/s][A
 25%|██▌       | 9/36 [00:05<00:18,  1.45it/s][A
 28%|██▊       | 10/36 [00:06<00:18,  1.44it/s][A
 31%|███       | 11/36 [00:07<00:17,  1.43it/s][A
 33%|███▎      | 12/36 [00:07<00:16,  1.43it/s][A
 36%|███▌      | 13/36 [00:08<00:16,  1.42it/s][A
 39%|███▉      | 14/36 [00:09<00:15,  1.42it/s][A
 42%|████▏     | 15/36 [00:09<00:14,  1.41it/s][A
 44%|████▍     | 16/36 [00:10<00:14,  1.41it/s][A
 47%|████▋     | 17/36 [00:11<00:13,  1.41it/s][A
 50%|█████     | 18/36 [00:12<00:13,  1.37it/s][A
 53%|█████▎    | 19/36 [00:12<00:12,  1.37it/s][A
 56%|█████▌    | 20/36 [00:13<00:11,  1.34it/s][A
 58%|█████▊    | 21/36 [00:14<00:11,  1.35it/s][A
 61%|██████    | 22/36 [00:15<00:10,  1.31it/s][A
 64%|██████▍   | 23/36 [00:15<00:09,  1.34it/s][A
 67%|██████▋   | 24/36 [00:16<00:08,  1.36it/s][A
 69%|██████▉   | 25/36 [00:17<00:07,  1.38it/s][A
 72%|███████▏  | 26/36 [00:17<00:07,  1.39it/s][A
 75%|███████▌  | 27/36 [00:18<00:06,  1.39it/s][A
 78%|███████▊  | 28/36 [00:19<00:05,  1.40it/s][A
 81%|████████  | 29/36 [00:20<00:04,  1.40it/s][A
 83%|████████▎ | 30/36 [00:20<00:04,  1.41it/s][A
 86%|████████▌ | 31/36 [00:21<00:03,  1.54it/s][A
 89%|████████▉ | 32/36 [00:21<00:02,  1.84it/s][A
 92%|█████████▏| 33/36 [00:21<00:01,  2.13it/s][A
 94%|█████████▍| 34/36 [00:22<00:00,  2.39it/s][A
 97%|█████████▋| 35/36 [00:22<00:00,  2.62it/s][A
100%|██████████| 36/36 [00:22<00:00,  3.20it/s][A                                                   
                                               [Aeval_anls: 60.51299094577889, epoch: 1.7153
 43%|████▎     | 1000/2332 [30:18<32:16,  1.45s/it]
100%|██████████| 36/36 [00:23<00:00,  3.20it/s][A
                                               [A[32m[2023-11-10 11:50:10,309] [    INFO][0m - Saving model checkpoint to ./models/fidelity_save_100/checkpoint-1000[0m
[32m[2023-11-10 11:50:10,318] [    INFO][0m - Configuration saved in ./models/fidelity_save_100/checkpoint-1000/config.json[0m
[32m[2023-11-10 11:50:12,898] [    INFO][0m - Model weights saved in ./models/fidelity_save_100/checkpoint-1000/model_state.pdparams[0m
[32m[2023-11-10 11:50:12,899] [    INFO][0m - tokenizer config file saved in ./models/fidelity_save_100/checkpoint-1000/tokenizer_config.json[0m
[32m[2023-11-10 11:50:12,899] [    INFO][0m - Special tokens file saved in ./models/fidelity_save_100/checkpoint-1000/special_tokens_map.json[0m
[32m[2023-11-10 11:50:18,010] [    INFO][0m - Deleting older checkpoint [models/fidelity_save_100/checkpoint-800] due to args.save_total_limit[0m
 43%|████▎     | 1001/2332 [30:28<4:22:04, 11.81s/it] 43%|████▎     | 1002/2332 [30:29<3:12:56,  8.70s/it] 43%|████▎     | 1003/2332 [30:31<2:24:29,  6.52s/it] 43%|████▎     | 1004/2332 [30:32<1:50:36,  5.00s/it]